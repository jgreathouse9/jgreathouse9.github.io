---
title: 'Synthetic Controls for Marketing Experiments'
date: 2025-09-20
categories: [Experiments, Econometrics]
---

# The Difficulties of Market Experimentation

[Suppose the government of Curaçao](https://www.travelpulse.com/news/destinations/curacao-takes-proactive-steps-toward-sustainability-amid-global-overtourism) wishes to implement a new ["Green Stay" initiative](https://www.businesstravelnewseurope.com/Accommodation/HRS-Green-Stay-updates-are-set-to-increase-hotel-participation) that encourages hotels to adopt sustainability measures such as reducing water usage, improving waste management, and shifting toward renewable energy. From a methodological standpoint, the most rigorous way to measure the impact of such a program would be through a randomized controlled trial, where some neighborhoods are randomly assigned to implement the policy while others serve as controls. Randomization ensures that, in expectation, treated and control groups are balanced, and that differences in outcomes can be attributed to the policy. It also provides a clear framework for statistical inference and lends credibility to the findings.

Despite these advantages, there are serious obstacles to conducting a true RCT in this context. Ethically, assigning certain neighborhoods to receive the benefits while denying them to others may be perceived as unfair, particularly if the policy boosts reputation, attracts eco-conscious travelers, or leads to financial advantages. Guests might also experience different standards unknowingly, raising questions about fairness and transparency.

Politically, implementing randomization across hundreds of neighborhoods would require coordination among the government, hotel associations, and local businesses, all of whom may resist being “experimented on.” Because tourism is central to the island’s economy, public perception would be sensitive, and randomized assignment could easily be framed as a risk to one of Curaçao’s most important industries.

The logistical burden is equally daunting: monitoring and enforcing different sustainability requirements neighborhood by neighborhood would be complex and costly, with significant overhead required for compliance and enforcement. In practice, coordinating hundreds of independent hotels is unlikely to succeed. In other words, while an RCT is desirable in theory, ethical, political, and feasibility barriers make it impractical in the context of Curaçao’s hotel sector.

Given these challenges, a more practical approach is to consider the market at a localized level, grouping neighborhoods into clusters that share similar characteristics, such as geographic location, customer demographics, or historical patterns of tourist activity. By focusing on clusters rather than individual units, we mitigate ethical concerns of treating some neighborhoods differently than others, since selection occurs within naturally similar groups rather than arbitrarily across the whole market.

Cluster analysis also reduces logistical complexity, because interventions can be coordinated at the cluster level rather than needing to monitor and enforce policies across hundreds of disparate hotels. Within each cluster, we can select treated neighborhoods that closely resemble the local market and control neighborhoods that reflect the cluster’s underlying dynamics. This ensures that observed effects of the intervention are not driven by unusual outliers or idiosyncratic behaviors, but rather capture the typical response within a given market segment.

But how would we select which neighborhoods to treat and which to use as controls? Even within a single cluster, we cannot treat everybody for practical reasons. To address this, we can use synthetic control methods, which construct “synthetic neighborhoods” that mimic the characteristics of treated clusters, allowing analysts to experiment with different treatment assignments and evaluate their likely impact rigorously.

## Notation and Synthetic Control Designs

In our Curaçao Green Stay example, the unit of treatment is the neighborhood, not individual hotels. That is, entire neighborhoods may adopt sustainability measures, while others remain untreated. Let $\mathcal{J} = \{1, \dots, J\}$ denote neighborhoods observed over $T_0$ pre-treatment periods $\mathcal{T}_0 = \{1, \dots, T_0\}$, nested within $K$ clusters $\mathcal{K} = \{1, \dots, K\}$ that group neighborhoods with similar characteristics (e.g., location, tourist demographics, historical occupancy). Let $I_k \subseteq \mathcal{J}$ denote neighborhoods in cluster $k$, with $j \in I_{k(j)}$ indicating cluster membership. In our case, we have 21 units of interest, where some will be treated and others will not. We have 128 pre-treatment periods, and 28 post periods. Observed outcomes, like occupancy rates or energy use, are collected in $\mathbf{Y} \in \mathbb{R}^{J \times T_0}$, with $y_{jt}$ representing the outcome for neighborhood $j$ at time $t$. Potential outcomes are $y_{jt}^I$ if the neighborhood participates in Green Stay and $y_{jt}^N$ if not. The cluster-weighted average treatment effect is

$$
\tau_t = \sum_{j=1}^J f_j \, (y_{jt}^I - y_{jt}^N), \quad t > T_0,
$$

where $f_j \ge 0$ and $\sum_{j=1}^J f_j = 1$. Predictor vectors $\mathbf{x}_j \in \mathbb{R}^r$ capture pre-treatment characteristics of neighborhoods, such as the average energy use, hotel occupancy, or typical guest demographics, with cluster mean

$$
\mathbf{\bar{x}}_k = \frac{\sum_{j \in I_k} f_j \mathbf{x}_j}{\sum_{j \in I_k} f_j}.
$$

Distances are defined as

$$
D_{1,j,k} = \|\mathbf{x}_j - \mathbf{\bar{x}}_k\|_2^2, \quad
D_{2,j,j',k} = \|\mathbf{x}_j - \mathbf{x}_{j'}\|_2^2.
$$

We define synthetic treated and control neighborhoods via weights $w_j \ge 0$ and $v_{i,j} \ge 0$, and neighborhoods are either treated or not via $z_j \in \{0,1\}$. Optional costs $\mathbf{c}$ and cluster budgets $B_k$ constrain feasible assignments. The feasible set is

$$
\begin{aligned}
\mathcal{F} = \Big\{ (w,v) \,\Big| \;
& w_j, v_j \ge 0, \quad \sum_{j \in I_k} w_j = \sum_{j \in I_k} v_j = 1, \\
& w_j \le z_j, \; v_j \le 1 - z_j, \quad \sum_{j \in I_k} z_j \in [m_{\text{min},k}, m_{\text{max},k}], \\
& \sum_{j \in I_k} c_j w_j \le B_k, \quad \sum_{k=1}^K \sum_{j \in I_k} c_j w_j \le B_{\text{total}}, \\
& \sum_{k=1}^K z_{j,k} \le 1
\Big\}.
\end{aligned}
$$

Intuitively, in the Curaçao context:

1. **Non-negativity and normalization ($w_j, v_j \ge 0$, $\sum w_j = \sum v_j = 1$):**  
   Synthetic neighborhoods are weighted averages of real neighborhoods. Negative weights would imply “subtracting” a neighborhood’s influence. Summing to one ensures each synthetic neighborhood is a proper mixture of real neighborhoods.

2. **Treatment assignment ($z_j \in \{0,1\}, w_j \le z_j, v_j \le 1 - z_j$):**  
   Each neighborhood is either treated or not. A treated neighborhood contributes to the synthetic treated unit but not the control unit, and vice versa, avoiding “double-counting.”

3. **Cardinality constraints ($\sum z_j \in [m_{\text{min},k}, m_{\text{max},k}]$):**  
   Ensures at least one neighborhood is treated per cluster to observe effects, but not every neighborhood, respecting ethical, financial, or logistical limits.

4. **Budget constraints ($\sum c_j w_j \le B_k$, $\sum c_j w_j \le B_{\text{total}}$):**  
   Neighborhoods may have different costs. Cluster- and global-level budgets ensure the program remains financially feasible.

5. **Cluster exclusivity ($\sum_k z_{j,k} \le 1$):**  
   Each neighborhood can only be treated in one cluster to avoid overlapping treatments that could confound the effect.

The clustered synthetic control estimator is obtained by solving

$$
\min_{(\mathbf{w},\mathbf{v}) \in \mathcal{F}} \; \mathcal{L}(\mathbf{w},\mathbf{v}),
$$

where $\mathcal{F}$ is the feasible set defined by nonnegativity, within–cluster normalization, exclusivity, and any budget constraints. Each cluster $k \in \{1,\dots,K\}$ has member indices $I_k$, cluster mean $\mathbf{\bar{x}}_k$, and outcomes $\mathbf{X}_{I_k}$. 

The optimization problem chooses synthetic treated and control neighborhoods by minimizing a loss function subject to feasibility constraints. At its core, the method constructs weighted averages of units to be treated and untreated, such that both averages resemble the cluster as a whole and each other.

The base estimator is the most straightforward. For each cluster $k$, it tries to make the synthetic treated and synthetic control neighborhoods look like the cluster mean:

$$
\mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v}) = \sum_{k=1}^K \Big( \mathbf{f}_{I_k}^\top \mathbf{1} \Big) \Big[
\|\bar{\mathbf{x}}_k - \mathbf{X}_{I_k}^\top \mathbf{w}_{I_k}\|_2^2
+ \|\bar{\mathbf{x}}_k - \mathbf{X}_{I_k}^\top \mathbf{v}_{I_k}\|_2^2
\Big].
$$

This guarantees that both sides of the experiment are anchored to the same benchmark—the average of their cluster.

The weakly penalized version extends this idea by also encouraging the treated and control groups to resemble each other directly:

$$
\mathcal{L}_{\text{Weak}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \beta \|\mathbf{X}_{I_k}^\top \mathbf{w}_{I_k} - \mathbf{X}_{I_k}^\top \mathbf{v}_{I_k}\|_2^2.
$$

In practice, this prevents the optimization from drifting toward two very different synthetic groups that both happen to match the cluster mean.

The penalized estimator takes things further by adding distance-based penalties. These ensure that the selected neighborhoods are not only good statistical matches but also geographically or demographically close to the cluster mean:

$$
\mathcal{L}_{\text{Penalized}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \lambda_1 \mathbf{w}_{I_k}^\top D_{1,k} \mathbf{w}_{I_k}
+ \lambda_2 \mathbf{v}_{I_k}^\top D_{1,k} \mathbf{v}_{I_k}.
$$

Here, the matrices $D_{1,k}$ encode distances, so neighborhoods that are far from the cluster center are penalized more heavily.

Finally, the unit-level estimator introduces the most granular controls. It not only enforces closeness to cluster means but also requires that each treated neighborhood has a tight match with its synthetic control, and that pairs of neighborhoods within the cluster do not diverge too much:

$$
\mathcal{L}_{\text{Unit}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \xi \, \big(\mathbf{w}_{I_k}^\top \, \mathrm{diag}(\|\mathbf{X}_{I_k} - \mathbf{X}_{I_k} \mathbf{V}_k\|_F^2)\big)
+ \lambda_{2,\text{unit}} \, \mathrm{tr}(\mathbf{W}_{I_k}^\top D_{2,k} \mathbf{V}_{I_k}).
$$

This is the most restrictive design. It is best suited for cases where heterogeneity across neighborhoods matters, and analysts want each treated area to have a carefully matched control counterpart.

Together, these estimators form a hierarchy. The base version establishes a simple anchor; the weak estimator prevents divergence; the penalized estimator incorporates geographic or demographic realism; and the unit-level estimator enforces fine-grained one-to-one balance. Analysts can experiment across this ladder, choosing the design that best matches the goals and constraints of their study.


## Intuition

What’s particularly interesting about this setup is that we’re not necessarily estimating treatment effects yet. Instead, the synthetic control framework is acting as a decision-making tool: it tells us which neighborhoods to treat and why, based on the trade-offs encoded in the objective function. Analysts can tweak the penalties, change clustering schemes, or adjust budget and cardinality constraints to see how the proposed treatment assignments change.

For example, increasing $\beta$ encourages the treated neighborhoods to be closer to the synthetic controls, effectively prioritizing representativeness of the treated units. Adjusting $\lambda_1$ and $\lambda_2$ emphasizes fidelity to cluster averages, penalizing outlier neighborhoods from being selected. Unit-level penalties $\xi$ and $\lambda_{2,j}$ allow analysts to ensure micro-level balance, focusing on individual neighborhoods with unique characteristics.

Because the framework is fully flexible, it becomes a sandbox for experimental design: you can simulate different cost and budget allocations, understand which neighborhoods are likely to be chosen under various priorities, and make informed decisions before the business/government does any treatment. In other words, this lets analysts answer: "Given our objectives and constraints, which neighborhoods should be treated, and why?" before any policy is implemented, making the framework extremely valuable for planning ethical, practical, and representative market experiments.


## Inference

### Unit- and Cluster-Level Effects

Let $\hat{\tau}_{jt}$ denote the unit-level treatment effect for neighborhood $j$ at time $t$:  

$$
\hat{\tau}_{jt} = y_{jt}^{\mathcal{N}_1} - y_{jt}^{\mathcal{N}_0}, \quad j \in \mathcal{J}, \; t \in \mathcal{T}_2.
$$

The cluster-weighted effect for cluster $k$ is  

$$
\hat{\tau}_{kt} = \sum_{j \in I_k} f_j \, \hat{\tau}_{jt}, \quad k \in \mathcal{K}, \; t \in \mathcal{T}_2,
$$

and the overall weighted effect is  

$$
\hat{\tau}_t = \sum_{k=1}^K \hat{\tau}_{kt} = \sum_{k=1}^K \sum_{j \in I_k} f_j \, \hat{\tau}_{jt}, \quad t \in \mathcal{T}_2.
$$

### Placebo Effects

Placebo effects are computed over pre-treatment "blank" periods $\mathcal{B} \subseteq \mathcal{T}_1$:  

Unit-level:  

$$
\hat{\tau}_{j,t}^{\mathcal{B}} = y_{jt}^{\mathcal{N}_1} - y_{jt}^{\mathcal{N}_0}, \quad j \in \mathcal{J}, \; t \in \mathcal{B}.
$$

Cluster-level: 

$$
\hat{\tau}_{k,t}^{\mathcal{B}} = \sum_{j \in I_k} f_j \, \hat{\tau}_{j,t}^{\mathcal{B}}, \quad k \in \mathcal{K}, \; t \in \mathcal{B}.
$$

Overall we have:  

$$
\hat{\tau}_t^{\mathcal{B}} = \sum_{k=1}^K \hat{\tau}_{k,t}^{\mathcal{B}} = \sum_{j \in \mathcal{J}} f_j \, \hat{\tau}_{j,t}^{\mathcal{B}}, \quad t \in \mathcal{B}.
$$

### Post-Treatment Effects

Weighted effects in the post-treatment period $\mathcal{T}_2$:  

Unit-level: 

$$
\hat{\tau}_{jt} = y_{jt}^{\mathcal{N}_1} - y_{jt}^{\mathcal{N}_0}, \quad j \in \mathcal{J}, \; t \in \mathcal{T}_2.
$$

Cluster-level:

$$
\hat{\tau}_{k,t} = \sum_{j \in I_k} f_j \, \hat{\tau}_{jt}, \quad k \in \mathcal{K}, \; t \in \mathcal{T}_2.
$$

Overall we have:  

$$
\hat{\tau}_t = \sum_{k=1}^K \hat{\tau}_{k,t} = \sum_{j \in \mathcal{J}} f_j \, \hat{\tau}_{jt}, \quad t \in \mathcal{T}_2.
$$

### Confidence Intervals (Split-Conformal)

For post-treatment period $t \in \mathcal{T}_2$, the $(1-\alpha)$ confidence interval is  

$$
q_{1-\alpha} = \text{Quantile}_{1-\alpha}\big(|\hat{\tau}_t^{\mathcal{B}}|\big), \quad t \in \mathcal{B},
$$

$$
\text{CI}_t = \big[\hat{\tau}_t - q_{1-\alpha}, \hat{\tau}_t + q_{1-\alpha}\big].
$$

Cluster-level confidence intervals:  

$$
q_{k,1-\alpha} = \text{Quantile}_{1-\alpha}\big(|\hat{\tau}_{k,t}^{\mathcal{B}}|\big), \quad t \in \mathcal{B},
$$

$$
\text{CI}_{k,t} = \big[\hat{\tau}_{k,t} - q_{k,1-\alpha}, \hat{\tau}_{k,t} + q_{k,1-\alpha}\big].
$$

### Global Permutation Test

Observed test statistic:  

$$
S_\text{obs} = \frac{1}{|\mathcal{T}_2|} \sum_{t \in \mathcal{T}_2} |\hat{\tau}_t|.
$$

Permutation distribution: randomly sample $|\mathcal{T}_2|$-sized subsets from $\{\hat{\tau}_t^{\mathcal{B}} \cup \hat{\tau}_t : t \in \mathcal{T}_2\}$. Denote permuted statistics $S_\text{perm}^{(b)}, b = 1,\dots,B$. Global p-value:  

$$
p_\text{global} = \frac{1}{B} \sum_{b=1}^B \mathbf{1}\{S_\text{perm}^{(b)} \ge S_\text{obs}\}.
$$

Cluster-level permutation tests are analogous, yielding $p_{k, \text{global}}$ for each cluster.

# A Simple Example

To illustrate, suppose we have 128 pre-treatment periods across two clusters of neighborhoods in Curaçao. The non-Willemstad cluster includes Barber, Lagún, Oostpunt, Santa Rosa, Sint Willibrordus, Soto, Spaanse Water, Tera Corá, and Westpunt. The plot below gives our results from the sythethic experiment.

```{python}

#| echo: false

from mlsynth import MAREX
import pandas as pd
from mlsynth.utils.exputils import plot_cluster_full


df = pd.read_csv("https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/Curacao_EXP_922.csv")
df_pre = df[df["time"] <= 120].copy()
#df_pre["Region"]=0

config = {"df": df,
          "unitid": "town",
          "time": "time",
          "outcome": "Y_obs",
          "T0": 128,
          "m_min": 2,
          "m_max": 3,
          "cluster": "Region",
          "program_type": "MIQP", "blank_periods": 28}

design = MAREX(config).fit()


plot_cluster_full(df,design)

```

We see that the above gives good pretreatment fit. We can do other things too. In the below I treat Curacao as a single region instead of two.

```{python}

#| echo: false

from mlsynth import MAREX
import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.utils.exputils import plot_cluster_full


df = pd.read_csv("https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/Curacao_EXP_922.csv")
df_pre = df[df["time"] <= 120].copy()
df["Region"]=0

config = {"df": df,
          "unitid": "town",
          "time": "time",
          "outcome": "Y_obs",
          "T0": 128,
          "m_min": 1,
          "m_max": 5,
          "cluster": "Region",
          "program_type": "MIQP", "blank_periods": 28}

design = MAREX(config).fit()

plot_cluster_full(df,design)
```

What this shows is that the synthetic design is robust: whether we divide Curaçao into clusters or treat it as a single unit, whether we enforce integer treatment assignments or relax them, the method generally produces stable synthetic matches. Notice that we have not yet discussed the ATT/ATE or the uncertainty of estimated treatment effects. This is because the framework above is focused on experimental design. The main goal is to decide *who should be treated* and *why* before any real-world intervention. In other words, we can be confident in the design itself—who is treated, who is not, and why—before estimating treatment effects or reporting uncertainty.

Contrast this with a traditional RCT. In an RCT, randomization occurs once in the field. Designing an RCT is costly and requires coordination across multiple actors. We cannot experiment with different randomization schemes beforehand, since the quality of the randomization can only be assessed *after* it occurs. Once units are assigned, the study must proceed, with all associated costs, risks, and constraints.

By contrast, the synthetic control design can be iterated rapidly on a laptop. We can explore different cluster groupings, penalties, cost constraints, or solver formulations and observe how stable the resulting design is before committing any resources. Crucially, in settings with few treatable units, synthetic control designs can achieve balance analogous to an RCT, even when only a handful of units are treated. This allows interventions to be targeted strategically, saving resources while maintaining credible comparisons.

This flexibility makes the synthetic experimentation method a powerful planning tool. It enables researchers and practitioners to prototype experiments cheaply and safely, ensuring that the data support a credible design before any real-world action. Large-scale RCTs remain the gold standard, but they are expensive and logistically demanding. Synthetic design exercises open the door to an iterative, exploratory phase of experimental planning—helping ensure that when we do invest in a full study, it is targeted, efficient, and built on a solid foundation.
