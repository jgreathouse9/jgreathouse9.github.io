---
title: "Synthetic Interventions for Policy Analysis"
date: 2025-04-16
categories: [Causal Inference, Econometrics]
---

In standard synthetic control methods (SCM), we aim to estimate the outcome for a treated unit if it had not been treated at all. In fact, this is what most data scientsts who do causal inference are interested in, that is to say the counterfactual under no treatment. But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, a natural question would be: *How would Store A have performed if it had adopted Program B instead of Program A?* In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus. Observing health outcomes such as obesity rates and soda sales across these cities, we might ask: *What if City 1 had imposed a soda tax instead of banning large sodas?* How would these metrics have evolved in this scenario? [Plenty](https://doi.org/10.1177/0022243720936230) [of](https://doi.org/10.1186/s12889-025-22526-5) [academic papers](https://jamanetwork.com/journals/jama-health-forum/fullarticle/2813506) have addressed this idea before, but only to assess the counerfactual scenario of no tax at all. A natrual question, then, is how we could know the impact of another policy, had it been applied to a unit that actually did another policy. Or, How a unit that did no policy would have evolved had it done *any* policy. Here, the standard SCM does not offer a satisfying answer. This is where the [Synthetic Interventions](https://doi.org/10.48550/arXiv.2006.07691) estimator comes from. SI estimates how a treated unit would have performed under an alternative intervention it did not actually receive, using outcomes from units that did receive that intervention. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI *substantially expands* the range of counterfactual questions that can be credibly answered. 

Before we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like [this](https://arxiv.org/pdf/2009.09987) circulate that claimed to be able to estimate things like "what would NYC's COVID rate look like had it locked down earlier than it actually did". I had never heard of tensors, or really even matrices at the time, so I would always ask "wow, how can we even do this?" So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others.

# Notation

Scalars are denoted by lowercase italic letters such as $g$. Vectors are denoted by bold lowercase letters such as $\mathbf{v} \in \mathbb{R}^n$. Matrices are denoted by bold uppercase letters such as $\mathbf{A} \in \mathbb{R}^{m \times n}$. Sets are denoted by calligraphic letters such as $\mathcal{N}$ and $\mathcal{T}$. The cardinality of a finite set $\mathcal{A}$ is denoted $|\mathcal{A}|$. Let $\mathcal{N} = \{1, 2, \dots, N\}$ index the units and $\mathcal{T} = \{1, 2, \dots, T\}$ index time periods. Let $\mathcal{D} = \{0, 1, \dots, D\}$ index treatment statuses, with $0$ denoting the untreated condition. Each unit $j \in \mathcal{N}$ is assigned a treatment $d_j \in \mathcal{D}$. 

Define $T_0 < T$ as the final pre-treatment period and partition $\mathcal{T}$ into pre-treatment periods $\mathcal{T}_{\text{pre}} = \{1, 2, \dots, T_0\}$ and post-treatment periods $\mathcal{T}_{\text{post}} = \{T_0 + 1, \dots, T\}$. Let $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{T_0}$ denote the pre-treatment outcome vector for unit $j$ and $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{T - T_0}$ denote its post-treatment outcomes. For a group of units assigned to treatment $d$, define $\mathbf{Y}_d^{\text{pre}} \in \mathbb{R}^{T_0 \times N_d}$ as the matrix stacking the pre-treatment outcomes of units $j \in \mathcal{N}_d$, and $\mathbf{Y}_d^{\text{post}} \in \mathbb{R}^{T - T_0 \times N_d}$ as the corresponding post-treatment outcome matrix. Let $\mathcal{N}_d = \{ j \in \mathcal{N} \mid d_j = d \}$ denote the set of units exposed to treatment $d$. At each $(t, j)$ pair, we observe only $y_{t j} = y^{\ast}_{t j d_j}$, the realized outcome under the assigned treatment. Our goal is to estimate the counterfactual post-treatment trajectory $\{ y^{\ast}_{t 1 d} \}_{t \in \mathcal{T}_{\text{post}}}$ for a focal unit $j = 1$ that actually received treatment $d_1$, where $d \neq d_1$ represents an alternative treatment.

# Econometric Model

[The paper](https://doi.org/10.48550/arXiv.2006.07691) assumes that potential outcomes admit a low-rank latent factor structure. Formally, each potential outcome $y_{t j d}^{\ast}$ for unit $j$ at time $t$ under treatment $d$ is generated by
$$
y_{t j d}^{\ast} = \mathbf{l}_t^\top \mathbf{f}_j^{(d)} + \varepsilon_{t j d},
$$
where $\mathbf{l}_t \in \mathbb{R}^r$ is a vector of time-specific latent factors, $\mathbf{f}_j^{(d)} \in \mathbb{R}^r$ is a vector of unit- and treatment-specific loadings, and $\varepsilon_{t j d}$ is an idiosyncratic error term with mean zero. The dimension $r$ is assumed to be small relative to $T$ and $N$.

The interactive structure implies that the tensor of potential outcomes across $(t, j, d)$ admits a low-rank decomposition. The principal components of the pre-treatment donor matrix $\mathbf{Y}_d^{\text{pre}}$ consistently estimate the column space of $\{ \mathbf{l}_t \}$ up to rotation, under standard conditions. Consequently, by projecting pre-treatment outcomes onto this estimated subspace and regressing the treated unit onto the donors, we can recover consistent estimates of the treated unit’s counterfactual trajectory under alternative treatments. Consequentually, SI relies on assumptions similar to SCMs. First, the tensor of potential outcomes admits a low-rank decomposition. Practically, this means the number of latent factors $r$ is small relative to the number of time periods and units (which is usually true in the standard econ application). We also presume the latent factor structure is stable across treatments and over time. The factors $\mathbf{l}_t$ and loadings $\mathbf{f}_j^{(d)}$ do not change discontinuously due to treatment assignment. Third, the donor units assigned to the alternative treatment $d$ span the relevant factor space needed to approximate the treated unit’s pre-treatment outcomes. Fourth, the idiosyncratic error terms $\varepsilon_{t j d}$ are mean-zero and satisfy weak regularity conditions such as independence across units or bounded variance. Finally, there is sufficient pre-treatment data to accurately estimate the principal components of the donor matrix. As we can see, all of this follow normal, vanilla SCM methods (more or less), so nothing too crazy is happening here.

# Recalling PCR

The estimation (unlike the theoretical justification) proceeds entirely through matrices, and does not use any tensor decomposition. In practice, SI simply applies PCR to the pre-treatment matrix of units assigned to the alternative treatment $d$. To estimate the counterfactual trajectory under an alternative treatment $d \neq d_1$, we apply singular value decomposition SVD to the pre-treatment donor matrix $\mathbf{Y}_d^{\text{pre}} \in \mathbb{R}^{T_0 \times N_d}$. This yields the decomposition
$$
\mathbf{Y}_d^{\text{pre}} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top,
$$
where $\mathbf{U} \in \mathbb{R}^{T_0 \times T_0}$ contains the left singular vectors, $\mathbf{\Sigma} \in \mathbb{R}^{T_0 \times N_d}$ is the diagonal matrix of singular values, and $\mathbf{V} \in \mathbb{R}^{N_d \times N_d}$ contains the right singular vectors. To obtain a low-dimensional representation, we truncate the decomposition at rank $r \leq T_0$ via Universal Singular Value Thresholding. Let $\mathbf{U}_r \in \mathbb{R}^{T_0 \times r}$ denote the first $r$ columns of $\mathbf{U}$, and let $\mathbf{\Sigma}_r \in \mathbb{R}^{r \times r}$ denote the leading $r$ singular values on the diagonal. Define the reduced-rank basis
$$
\tilde{\mathbf{Y}}_d^{\text{pre}} = \mathbf{U}_r \mathbf{\Sigma}_r.
$$

We then project the treated unit's pre-treatment outcomes $\mathbf{y}_1^{\text{pre}} \in \mathbb{R}^{T_0}$ onto the donor space by solving the regression problem
$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \mathbb{R}^{N_d}}{\operatorname*{argmin}} \left\| \mathbf{y}_1^{\text{pre}} - \mathbf{Y}_d^{\text{pre}} \mathbf{w} \right\|_2^2.
$$

This yields a weight vector $\mathbf{w}^{\ast}$ over the donor units in $\mathcal{N}_d$, constrained to lie in the subspace spanned by the leading principal components. The estimated counterfactual trajectory for the unit of interest under treatment $d$ is then constructed by applying these weights to the observed post-treatment outcomes:
$$
\hat{\mathbf{y}}_1^{\text{post}} = \mathbf{Y}_d^{\text{post}} \mathbf{w}^\ast.
$$

This quantity serves as an estimate for $\{y^{\ast}_{t 1 d}\}_{t \in \mathcal{T}_{\text{post}}}$, the counterfactual outcomes that unit 1 would have experienced had it received treatment $d$ instead of $d_1$.

