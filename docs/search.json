[
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, many datasets we work with come in pretty csv files that are clean. And while that’s great… oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible code/script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we’d need to ask AAA and pay thousands of dollars for an extended time series… but now we don’t need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for the scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA’s website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is “https://gasprices.aaa.com/?state=MA”. For Florida, the URL is “https://gasprices.aaa.com/?state=FL”. See the pattern? There’s a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python’s requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we’ve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year’s worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I’ve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "actins.html",
    "href": "actins.html",
    "title": "Causal Inference Runs the World: Actionable Insights, Econometrics Style",
    "section": "",
    "text": "Man, I hate industry jargon. Don’t get me wrong, I’m not against all jargon. I’ve been in academia ten years, and I’ve spent maybe 6 of those being tormented by econometrics. We have our own jargon too, which doesn’t always jive very well with industry- I say “causal inference”, marketers will say “incrementality testing”. I say “experiment”, others say “A/B testing”. I say “practical significance”, others say “value”. There’s something about the data science industry lingo that irritates me. “Ambiguity” (I’ll do a post on this one, one day). “Self starter”. “Business acumen”. And the favorite of the day, “actionable insights.” This post is dedicated to showing you how I think about actionable insights. I approach this from the perspective of somebody who cares a lot about causal inference. I would like to focus on how we can use causal methods from econometrics to deliver actionable insights… But I’m getting ahead of myself.\n“Actionable insights”. It’s what we hear about all the time, even if it isn’t phrased exactly like that. Job descriptions and data science pages will go on and on about how we’re meant to deliver them as econometricians, policy analysts, and data scientists. Our livelihoods are essentially staked to producing them. But, what are actionable insights anyways? Some pages say ““Actionable insights” describes contextualised data analysis. It’s a piece of information that can actually be put into action — supported by the analysis, context and communication tools required to get the job done.” I agree with this. This leads us to one question then: how do we know which insights are actionable? And, more pressingly, how do we prodcue these in the first place?\n\n\nRecently I saw a post on LinkedIn. It went something like this:\n\nData Science Intern: “This is a chart of our App download trends.” Client: “So?”\n\n\nJunior Data Scientist: “The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear.” Client: “Great! Why?”\n\n\nPrincipal Data Scientist: “The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear due to our new pricing strategy. We should roll out Pricing Strategy in more markets since it’ll likely increase revenue by Big Amount.” Client: “That’s great! We’ll get right to it.”\n\nIn context, the post was about connecting your results to the next steps of actions businesses/clients should take, saying that context is key to making business recommendations. And that’s great, we should all be doing that. But I find the hypothetical Principal DS’s answer to be quite wanting. I do not understand why we are meant to see the Principal DS’s remarks as any more wise than the Junior DS. Why?\nThe Principal DS was implicitly making a counterfactual claim in this example. They were explaining to the client that their Policy W had X impact on Outcome Y, an even went as far as to say that if we kept doing Policy W, we may see gains elsewhere in the future. And this may be true. But how can we tell? For all the talk we have about showing impact (instead of telling what you did descriptively), surely we can do better than this. How? Causal inference.\nHere is my issue with the post: the second you add the phrase “Impact X happened given our new pricing strategy/policy/other intervention”, we’re now very far aflung from Descriptiville, stuck in Counterfactual Land. In this domain, the laws work differently. You will never get by if the most you can do is simply getting Tableau/BI to make us a chart. Here you see, we need to have some estimate of how sales (or whatever metric we’re meant to care about) would’ve evolved ABSENT whatever the new policy was. Put differently, you can’t just show a bar chart and say “You should do X cuz I made this chart and have verbally attached an explanation to why we see a number”. It requires A LOT more work than that."
  },
  {
    "objectID": "actins.html#a-motivating-example",
    "href": "actins.html#a-motivating-example",
    "title": "Causal Inference Runs the World: Actionable Insights, Econometrics Style",
    "section": "",
    "text": "Recently I saw a post on LinkedIn. It went something like this:\n\nData Science Intern: “This is a chart of our App download trends.” Client: “So?”\n\n\nJunior Data Scientist: “The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear.” Client: “Great! Why?”\n\n\nPrincipal Data Scientist: “The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear due to our new pricing strategy. We should roll out Pricing Strategy in more markets since it’ll likely increase revenue by Big Amount.” Client: “That’s great! We’ll get right to it.”\n\nIn context, the post was about connecting your results to the next steps of actions businesses/clients should take, saying that context is key to making business recommendations. And that’s great, we should all be doing that. But I find the hypothetical Principal DS’s answer to be quite wanting. I do not understand why we are meant to see the Principal DS’s remarks as any more wise than the Junior DS. Why?\nThe Principal DS was implicitly making a counterfactual claim in this example. They were explaining to the client that their Policy W had X impact on Outcome Y, an even went as far as to say that if we kept doing Policy W, we may see gains elsewhere in the future. And this may be true. But how can we tell? For all the talk we have about showing impact (instead of telling what you did descriptively), surely we can do better than this. How? Causal inference.\nHere is my issue with the post: the second you add the phrase “Impact X happened given our new pricing strategy/policy/other intervention”, we’re now very far aflung from Descriptiville, stuck in Counterfactual Land. In this domain, the laws work differently. You will never get by if the most you can do is simply getting Tableau/BI to make us a chart. Here you see, we need to have some estimate of how sales (or whatever metric we’re meant to care about) would’ve evolved ABSENT whatever the new policy was. Put differently, you can’t just show a bar chart and say “You should do X cuz I made this chart and have verbally attached an explanation to why we see a number”. It requires A LOT more work than that."
  },
  {
    "objectID": "actins.html#defining-the-problem",
    "href": "actins.html#defining-the-problem",
    "title": "Causal Inference Runs the World: Actionable Insights, Econometrics Style",
    "section": "Defining the Problem",
    "text": "Defining the Problem\nSuppose we work for Uber. Uber introduced Uber Green in September of 2020, as an initiative that is meant to (among other things) incentivize drivers to use electric cars/low emissions vehicles. Suppose our supervisor tasks us with evaluating whether this policy in fact affected the number of drivers who use electric cars, as was the intended goal. Given this situation, we must roll out this intervention someplace first in order to see how it may work in other markets (say, Sayulita, Mexico), and we must generate a counterfactual. Or, the number of drivers who would have used electric cars but for Uber Green’s introduction. In order to accomplish this task, I will use synthetic control based methodologies to estimate the impact. Of course, the goal here is to compute the average treatment effect on the treated, or the average of the differences between our treated unit and the out-of-sample predictions (post intervention period). This allows us to have a summary statistic of the causal effect, as an averge or as a total."
  },
  {
    "objectID": "actins.html#solving-the-problem",
    "href": "actins.html#solving-the-problem",
    "title": "Causal Inference Runs the World: Actionable Insights, Econometrics Style",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\nLet \\(\\mathcal{N}\\) denote the set of cities indexed by \\(j\\), where \\(N \\coloneqq |\\mathcal{N}|\\) represents the total number of markets. Sayulita, the treated city, is indexed by \\(j = 1\\), while the set of control cities is denoted by \\(\\mathcal{N}_0 \\coloneqq \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0 \\coloneqq |\\mathcal{N}_0|\\). Time periods are indexed by \\(t\\), with pre-intervention periods \\(\\mathcal{T}_1 \\coloneqq \\{1, 2, \\dots, T_0\\}\\) and post-intervention periods \\(\\mathcal{T}_2 \\coloneqq \\{T_0+1, \\dots, T\\}\\), where \\(T_0\\) is the last period before the intervention.\nFor each market \\(j\\), let \\(\\mathbf{y}_j \\coloneqq [y_{jt}, \\dots, y_{jT}]^\\top \\in \\mathbb{R}^{T}\\) represent the vector of new drivers who use electric cars, where \\(y_{jt}\\) denotes the number of weekly new drivers in market \\(j\\) at time \\(t\\). Let \\(\\mathbf{Y}_0 \\coloneqq (\\mathbf{y}_j)_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\) be the matrix of control markets that did not do this intervention at this time. To estimate the counterfactual new driver supply in Sayulita, we construct a synthetic control by taking some weighted average of the control markets based on their pre-intervention trends, \\(\\hat{\\mathbf{y}}_1(0) = \\mathbf{Y}_0 \\mathbf{w}^\\top\\), where \\(\\mathbf{w} \\in \\mathbb{R}^{N_0}\\) is a vector of weights assigned to the control cities. These weights are chosen to minimize some loss function. The treatment effect at time \\(t\\) is then estimated as the difference between the observed and counterfactual outcomes, \\(\\widehat{\\Delta}_t \\coloneqq y_{1t} - \\hat{y}_{1t}(0)\\). Our result of interest is the average of these treatment effects over the post-intervention period:\n\\[\n\\widehat{ATT} \\coloneqq \\mathbb{E}_2[\\widehat{\\Delta}_t] = \\frac{1}{T_2} \\sum_{t \\in \\mathcal{T}_2} \\widehat{\\Delta}_t.\n\\]\nIf the introduction of Uber Green in Sayulita leads to an increase in the number of new drivers who use electric vehicles, we expect \\(\\widehat{\\Delta}_t\\), we can then comment on how this program may be applied to other areas.[^1]. The observed new drivers who use electric cars in Sayulita follows a factor model:\n\\[\n\\mathbf{y}_1 = \\mathbf{\\Gamma} \\mathbf{F} + \\boldsymbol{\\nu}_1 + \\boldsymbol{\\delta} \\mathbb{1}(t \\geq T_0),\n\\]\nwhere \\(\\mathbf{\\Gamma} \\in \\mathbb{R}^{1 \\times k}\\) represents the factor loadings, \\(\\mathbf{F} \\in \\mathbb{R}^{k \\times T}\\) is the matrix of latent common factors, and the factors evolve as:\n\\[\n\\mathbf{F}_t = \\rho \\mathbf{F}_{t-1} + \\boldsymbol{\\eta}_t, \\quad \\boldsymbol{\\eta}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_k),\n\\]\nwhere \\(\\rho\\) is the autocorrelation parameter and \\(\\boldsymbol{\\eta}_t\\) is a noise term. The idiosyncratic error term, \\(\\boldsymbol{\\nu}_1\\), is assumed to follow a normal distribution with zero mean and variance \\(\\sigma^2\\), i.e., \\(\\boldsymbol{\\nu}_1 \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\\). The treatment effect vector \\(\\boldsymbol{\\delta} \\coloneqq [\\delta_{T_0+1}, \\dots, \\delta_T]^\\top\\) represents the change in driver supply due to the Uber Green intervention, which is assumed to affect Sayulita starting at time \\(T_0+1\\).\n\nLet’s begin by plotting our outcome. The plot depicts the weekly new Uber drivers who have switched to electric vehicles. Sayulita is our target unit in black, the average of the control group is the thin red line, and the thin grey line are our donors that didn’t enact the intervention.\n\n\n\n\n\n\n\n\n\nA few things are apparent: for one, the parallel trends assumption doesn’t apply here across the 75 pre-treatment periods. The mean of controls doesn’t mirror the trend of Sayulita. So, we must use something a little more sophisticated than the standard Difference-in-Differences method. Furthermore, these donors have pretty noisy outcome series. Fortunately, we can exploit the low rank structure of our control group and use that to learn the weights which reconstructs the pre-intervention time series for our treated unit. I’ve written more about this here. Basically, we denoise the data via PCA or functional analysis, and cluster over the functional representation of control units or their right singular values, and extract the donor pool from this low-rank representation. We can then use principal component regression or robust principal component regression to learn our unit weights.\nWhen we do this, we obtain these predictions, of course using my mlsynth to fit the pre-intervention period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nRSC\nRPCASC\n\n\n\n\n0\nPre-Treatment RMSE\n1.263\n2.285\n\n\n1\nATT\n14.875\n17.312\n\n\n2\nPercent ATT\n22.473\n27.154\n\n\n\n\n\n\n\nWe can see that both the Robust PCA Synthetic Control and the Robust Synthetic Control/Principal Component Regression methods fit Sayulita quite well in the pre-intervention period. They also have very similar ATTs, suggesting that Uber Green increased new electric vehicle use amongst its drivers by anywhere from 22.473 to 26.425 percent. The normal ATTs are also pretty close to the what I simulated, an ATT of 15. Next, I’ll simulate the ATT using Forward Difference-in-Differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nFDID\nDID\n\n\n\n\n0\nATT\n15.527\n14.957\n\n\n1\nPre-Treatment RMSE\n2.315\n5.572\n\n\n2\nPercent ATT\n23.691\n22.624\n\n\n\n\n\n\n\nHere we plot the results of the standard Difference-in-Differences design and the Forward DID method. As we can see, parallel pre-intervention trends doesn’t hold at all. The selected parallel trends made by FDID is a lot more sensible in this instance, fitting as well as the synthetic control methods as above.\nJust to go really crazy, I decided to combine my estimates into a convex average. Let \\(K\\) denote the number of counterfactual models, in our case 3. Define \\(\\mathbf{A} \\in \\mathbb{R}^{T \\times K}\\) as the matrix of counterfactual vectors, where each column corresponds to a model and each row represents a time period. The optimization problem is formulated as:\n\\[\n\\min_{\\mathbf{w} \\in \\Delta_K} \\left\\| \\mathbf{y}_1 - \\mathbf{A} \\mathbf{w} \\right\\|_2\n\\]\nwhere \\(\\mathbf{w}\\) belongs to the simplex:\n\\[\n\\mathbf{w} \\in \\Delta_K = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^K \\mid \\left\\| \\mathbf{w} \\right\\|_1 = 1 \\right\\},\n\\]\nor the set spanned by the convex hull of the differing model predictions.\n\n\n\n\n\n\n\n\n\nThe ATT we compute is 14.902. Just as a very technical note, we see that the RMSE here with the model averaged estimator is lower than any other model I’ve estimated so far. This is because of Jensen’s inequality. Jensen’s inequality states that for a convex function \\(f\\) and a set of inputs, the function’s value at the weighted average is less than or equal to the weighted average of the function’s inputs:\n\\[\nf\\left(\\sum_{k=1}^{K} w_k A_k\\right) \\leq \\sum_{k=1}^{K} w_k f(A_k).\n\\]\nIn our case, we consider the function of the MSE. Due to the quadratic term, \\(f(x) = x^2\\), we have a convex function. Applying Jensen’s inequality to our loss function, we have\n\\[\n\\text{MSE}(\\mathbf{w}^\\top \\mathbf{A}) = \\mathbb{E} \\left[ (\\mathbf{y}_1 - \\mathbf{w}^\\top \\mathbf{A})^2 \\right] \\leq \\sum_{k=1}^{K} w_k \\mathbb{E} \\left[ (\\mathbf{y}_1 - A_k)^2 \\right],\n\\]\nwhich essentially guarantees that our model averaged pre-treatment MSE must be lower than the MSE of one of these models by themselves."
  },
  {
    "objectID": "actins.html#communicating-actionable-insights",
    "href": "actins.html#communicating-actionable-insights",
    "title": "Causal Inference Runs the World: Actionable Insights, Econometrics Style",
    "section": "Communicating Actionable Insights",
    "text": "Communicating Actionable Insights\nWhy is this approach superior than simply showing a chart/bar graph? With these results, we can communicate findings easier than before. Instead of simply speculating, we can say, for example,\n\nWe simulated the effect of Uber Green four times. Our best estimates suggest that the percentage of new drivers who use electric cars increased between 22.473 and 26.4 percent in the weeks following the Uber Green program being introduced (I also here might use the Bayesian prediction intervals from RSC to comment on uncertainty). When we combined our best three models together, Uber Green added 15 more electric car drivers on average per week. The program added around 488 more electric car drivers than what we would’ve seen otherwise. We would need to roll out the program in new areas for more confirmation, but the evidence suggests Uber Green increases electric car usage.\n\nSee how I’m not simply speculating with a bar chart? The value added here is that I’m simulating the universe where what did happen, did not happen, and then I’m tentatively suggesting that we implement the program elsewhere based off my findings, not a simple analysis of trends. The potential outcomes framework, when implemented judiciously, directly suggests what should or should not be done based off the treatment effects and uncertainty.\nThis is even more evident when we consider the actual presentation of my results. I’m actually showing the client the effect of the program, not just telling them. I am presenting them an explicit picture of how the world would look if they didn’t do their policy or intervention. This is the great thing about synthetic control methods (and other methods like Difference-in-Differences event studies) ; they’re very visual designs. They can be explained to people who have 0 training with and are easy to follow. The treatment effect of Uber Green here pops right out at you, mkaing the choice (more drivers being better than less drivers) obvious. What’s more, the assumptions of my model can can (sometimes) explicitly be defended, instead of sort of arguing from intuition.\nWhile I can’t predict how the treatment would behave in other markets (well I lied, I kinda can), the econometric approach of testing for causal impacts presents a far more compelling picture than me simply saying “Hey this bar chart shows an effect and I’m gonna speculate the effect is cuz of this policy.” This is why I say causal inference runs the world: assuming we expect our policies to impact people and increase value for our clients, doing rigourous impact analysis is key to assess whether we are actual in fact doing that."
  },
  {
    "objectID": "clustsc.html",
    "href": "clustsc.html",
    "title": "On Clustering for Synthetic Controls",
    "section": "",
    "text": "Picking a donor pool is very important with the synthetic control method. The vanilla method suffers from interpolation biases in the case when the donor pool is not suitable for the target unit. This is farily common with applied SCM in real life, as oftentimes we may have a high-dimensional donor pool and not know what the right control units would be a priori. The standard advice given in this instance is to limit our control group to units that are already similar to the target unit. But how do we do this? One way is to use clustering methods to select the control group. Consider the classic Basque Country example below, where the Basque Country undergoes a wave of terrorism in the mid-1970s which is thought to impact their GDP per Capita relative to other areas of Spain.\nHere, we plot the Basque versus the average of its controls as well as the individual donor outcome vectors themselves. We can see that the Basque Country is one of the wealthiest areas of Spain, up there with Madrid, Cataluna, and the Balearic Islands. We have other donors too, which by comparison are less wealthy. The key, then, is to ask which donors we should select for the synthetic control algorithm to consider in the first place, by exploiting pre-intervention similarities between the treated unit and control group. With “better” pre-policy donors, there is a higher chance that our out-of-sample predictions would be closer to the actual counterfactual."
  },
  {
    "objectID": "clustsc.html#plotting-our-selected-donors",
    "href": "clustsc.html#plotting-our-selected-donors",
    "title": "On Clustering for Synthetic Controls",
    "section": "Plotting Our Selected Donors",
    "text": "Plotting Our Selected Donors\n\n\n\n\n\n\n\n\n\nHere we plot the Basque Country versus its selected donors. We can see that these donors (Cataluna, the very closest grey line in Euclidean distance to the Basque Country), Madrid, and the Balearic Islands are much more similar to the Basque Country than the other 13 control units. Both fPCA-clustering implemented in Mani’s dissertation and the clustering over the right singular vectors choose the same donor pool when the pretreatment period extends up to 1975."
  },
  {
    "objectID": "clustsc.html#takeaways-for-practitioners",
    "href": "clustsc.html#takeaways-for-practitioners",
    "title": "On Clustering for Synthetic Controls",
    "section": "Takeaways for Practitioners",
    "text": "Takeaways for Practitioners\nThe meaning of these results are quite simple: donor selection matters for SCM studies. In fact, simple machine-learning donor selection methods can oftentimes give the same or similar answers to classical studies which oftentimes used covariates (in the original SCM paper, Abadie and his advisor used 13 covariates to construct the counterfactual, returning Cataluna and Madrid as the weighted donors). I say oftentimes because these results are predicated on both assumptions and tuning parameters. We assume, for example, some low rank approximation exists that can fit the pre-intervention time series of the treated unit. The tuning parameters matter too– the lambda parameters control the sparisty of our results, for example. I use a simple herustic to tune it, but it may be more reasonable to use methods such as cross-validation to select the number of clusters or the values lambda should take, but this would demand simulation evidence.\nEither way, the main benefit for policy analysts is that these two methods offer ways to select a donor poor for synthetic control methods, on top of not needing to collect auxilary covariates that the original paper used to obtain very similar counterfactual predictions, and more work should be done on these estimators to see when and why they’d agree."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "\nJared Greathouse\n",
    "section": "",
    "text": "Jared Greathouse\n\n\nEconometrician. Consultant.\n\n\n👋 Welcome\nI am Jared Amani Greathouse. I offer consulting services in data science and econometrics, especially with synthetic control methods.\n\n\n📄 Intake Form\nTo offer me a project:\n\nDownload the Request Intake Form (DOCX). If the form does not downlaod, copy the link and put it in your browser.\nWhen it is filled out, email it to me with a short description of the project.\nFees begin at at $250/hour, unless otherwise agreed upon.\n\n\n\n📬 Contact\nSchool Email: jgreathouse3@student.gsu.edu (this is the best way to reach me)\nPersonal Email: j.greathouse200@gmail.com\nWebsite: jgreathouse9.github.io"
  },
  {
    "objectID": "fdid.html",
    "href": "fdid.html",
    "title": "Applying Forward DID to Construction and Tourism Policy",
    "section": "",
    "text": "Causal inference is critical to economics, marketing, policy, and other sectors of industry. Frequently, policies or natural events occur that may affect metrics we care about. In order to maximize our decision making capabilities, understanding the effects of these events we care about is critical so that businseses and governments may plan future business decisions better or know if a policy intervention achieves its intended aims. In absence of A/B tests (randomized controlled trials, which is quite popular amongst marketing firms/other areas of tech), business scientists and policy analysts frequently resort to constructing counterfactuals to infer treatment effecs. This is because conducting proper experiments is difficult, costly, and/or unethical, especially with most of the events we are concerned with which impact millions of people.\nDifference-in-Differences (DID) is one of the most popular methods for quasi-experimental designs/treatment effect analysis. DID is simple to compute, and is valid even in settings where we have one treated unit and a single control unit. The key identifying assumption of DID is parallel trends (PTA), or that the trend of the treated group would be constant with respect to the control group had a given intervention or policy never took place. Additionally, PTA posits no-anticipation of the intervention. Various restatements of PTA are common in the econometrics literature, especially under staggered adoption where DID is frequently applied to use cases researchers care about. However, sometimes DID is used even in settings of a single treated unit. And in these settings, PTA may be less likely to hold. This blog post goes over the Forward DID method with an application to the construction/tourism industry."
  },
  {
    "objectID": "fdid.html#business-use-cases",
    "href": "fdid.html#business-use-cases",
    "title": "Applying Forward DID to Construction and Tourism Policy",
    "section": "Business Use Cases",
    "text": "Business Use Cases\nWhen might these be useful in business science such as construction or in tourism? Well for one, lots of states have passed laws regarding heat safety for workers which restrict local areas from passing laws which would provide workers with water, shade, and rest in the hot summer months. We can use these techniques to see how such laws/policies affect labor or safety. We can use causal inference to estimate the impact of events that are meant to affect the demand for tourism or other KPIs the tourism industry cares about. For construction, the physical building of construction units could be affected by these kind of policies, impacting metrics like put-in-place value or project stress indices. On the supply side, we may quantify the effects of policies such as tariffs on the costs of materials. With proper causal infernece, firms and policymakers may plan more effectively, knowing whether to pursue current policies or not, and take action with scientifically based analysis."
  },
  {
    "objectID": "fscm.html",
    "href": "fscm.html",
    "title": "Forward Selected Synthetic Control",
    "section": "",
    "text": "Interpolation bias is a known issue with synthetic control models (SCMs) For valid counterfactual prediction, the donor units, or the set of units that were never exposed to an intervention, should be as similar as possible to the treated unit in the pre-treatment periods. Selecting an appropriate donor pool is therefore critical, for practitioners. However, this can be challenging in settings with many potential controls, potentially many more control units than pre-treatment periods. Practically, researchers may wish to use this method when they have a high-dimensional donor pool and may be unsure as to which donors to include to reduce the impact of interpolation biases. To this end, this blog post introduces users the Forward Selected SCM. This applies Forward Selection (FS) to choose the donor pool for a SCM before estimating out-of-sample predictions."
  },
  {
    "objectID": "fscm.html#notation",
    "href": "fscm.html#notation",
    "title": "Forward Selected Synthetic Control",
    "section": "Notation",
    "text": "Notation\nLet \\(\\mathbb{R}\\) denote the set of real numbers. A calligraphic letter, such as \\(\\mathcal{S}\\), represents a discrete set with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) represent indices for a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Let \\(j = 1\\) be the treated unit, with the set of controls being \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0\\). The pre-treatment period consists of the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\},\\) where \\(T_0\\) is the final period before treatment. Similarly, the post-treatment period is given by \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}.\\) The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), where a generic outcome vector for a given unit in the dataset is \\(\\mathbf{y}_j \\in \\mathbb{R}^T\\), where \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^{T}\\). The outcome vector for the treated unit specifically is \\(\\mathbf{y}_1\\). The donor matrix, similarly, is defined as \\(\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\), where each column indexes a donor unit and each row is indexed to a time period.\nSCM estimates the counterfactual outcome for the treated unit by solving the program\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} \\|_2^2 \\: \\forall t \\in \\mathcal{T}_1.\n\\]\nWe seek the weight vector, \\(\\mathbf{w}\\), that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. For our purposes, the space of SC weights is the \\(N_0\\)-dimensional probability simplex \\(\\Delta^{N_0} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{N_0} : \\|\\mathbf{w}\\|_1 = 1 \\right\\}.\\) Practically this means that the post-intervention predictions will never be greater than the maximum outcome of the donor pool or lower than the minimum outcome of the donor pool."
  },
  {
    "objectID": "fscm.html#step-1",
    "href": "fscm.html#step-1",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 1",
    "text": "Step 1\nFS proceeds over \\(K \\in \\mathbb{N}\\) iterations, builds a sequence of tuples, \\(\\mathbb{T} = \\{(\\mathcal{S}_K, \\text{MSE}_K) \\}_{K=1}^{N_0}\\). The tuple contains two elements: the selected donor set for the \\(K\\)-th iteration and its corresponding \\(\\text{MSE}_K\\) (or the pre-treatment mean squared error). We begin by minimizing the SCM objective function as above, cycling through each donor unit vector one at a time instead of using the full control group. We denote these as submodels, which returns \\(N_0\\) one unit SCM models. We choose the single donor unit (the nearest neighbor in this specific case) that minimizes the MSE among all the \\(N_0\\) submodels. Our first tuple, then, is built with this single donor unit and the model’s corresponding MSE\n\\[\n\\mathcal{S}_1 = \\{j^\\ast\\}, \\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0}{\\operatorname*{argmin}} \\ \\text{MSE}(\\{j\\}).\n\\]"
  },
  {
    "objectID": "fscm.html#step-2",
    "href": "fscm.html#step-2",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 2",
    "text": "Step 2\nFor \\(K=2\\), we now estimate \\(N_0-1\\) two-unit SCMs. We include the originally selected donor along with the remaining controls, one remaining donor at a time. As above, the first and second elements of the second tuple, respectively, are\n\\[\n\\mathcal{S}_2 = \\mathcal{S}_1 \\cup \\{j^\\ast\\},\\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}_{K-1}}{\\operatorname*{argmin}} \\ \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j\\}).\n\\]\nNow, we have two tuples and two MSEs to choose from."
  },
  {
    "objectID": "fscm.html#generalizing",
    "href": "fscm.html#generalizing",
    "title": "Forward Selected Synthetic Control",
    "section": "Generalizing",
    "text": "Generalizing\nThis process generalizes, continuing for the rest of the donor pool. The general form for this algorithm, then, is\n\\[\n(\\mathcal{S}_K, \\text{MSE}_K) = (\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\}, \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\})).\n\\]\nThe algorithm continues until \\(S_K=N_0\\), when there are no more donors to add. FSCM chooses the tuple with the lowest \\(\\text{MSE}\\):\n\\[\n\\mathcal{S}^{\\ast} = \\underset{(\\mathcal{S}_K, \\text{MSE}_K) \\in \\mathbb{T}}{\\operatorname*{argmin}} \\ \\text{MSE}_K.\n\\]\nas the optimal donor set, \\(\\mathcal{S}^{\\ast}\\). Note that even within \\(\\mathcal{S}^{\\ast}\\) (as we will see below), some donors may receive zero weight in the final solution. The selected donors are just the units selected for inclusion in the donor pool in the first place, they are no guarantee of the unit having positive weight. This is in contrast to methods such as Forward Difference-in-Differences or the FS panel data method. Both of these designs are available in mlsynth too, in the FDID class and PDA class with the method of fs (the default). The main difference here is that FDID can never overfit because it estimates only one parameter, whereas (in theory) FSCM and fsPDA can overfit if they end up including too many parameters in the regression model. Unclear how likely this is, since as we see below, teh FS method reduces the full donor pool to just under half of the originally selected donor units."
  },
  {
    "objectID": "fscm.html#fscm-in-mlsynth",
    "href": "fscm.html#fscm-in-mlsynth",
    "title": "Forward Selected Synthetic Control",
    "section": "FSCM in mlsynth",
    "text": "FSCM in mlsynth\nNow I will give an example of how to use FSCM for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nAnd then we load the Proposition 99 dataset and fit the model in the ususal mlsynth fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.\n\nimport pandas as pd # To work with panel data\n\nfrom IPython.display import display, Markdown # To create the table\n\nfrom mlsynth.mlsynth import FSCM # The method of interest\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\n# Feel free to change \"smoking\" with \"basque\" above in the URL\n\ndata = pd.read_csv(url)\n\n# Our method inputs\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": \"red\"}\n\narco = FSCM(config).fit()\n\n\n\n\n\n\n\n\nAfter estimation, we can get the weights into a table like\n\nweights_dict = arco['Weights'][0]\ndf = pd.DataFrame(list(weights_dict.items()), columns=['State', 'Weight'])\ndisplay(Markdown(df.to_markdown(index=False)))\n\n\n\n\nState\nWeight\n\n\n\n\nMontana\n0.232\n\n\nAlabama\n-0\n\n\nColorado\n0.015\n\n\nConnecticut\n0.109\n\n\nGeorgia\n-0\n\n\nIdaho\n0\n\n\nIllinois\n0\n\n\nNevada\n0.205\n\n\nNew Hampshire\n0.045\n\n\nNew Mexico\n0\n\n\nNorth Carolina\n0\n\n\nNorth Dakota\n-0\n\n\nOklahoma\n-0\n\n\nUtah\n0.394\n\n\nVermont\n-0\n\n\nWest Virginia\n0\n\n\nWyoming\n0\n\n\n\n\n\nThese are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the SC, with only 6 being assigned positive weight. The ATT of Prop 99 as estimated by FSCM is -19.51 and the pre-treatment Root Mean Squared Error for FSCM is 1.66. I compared these results to the same results we get in Stata, which includes the covariates that Abadie, Diamond, and Hainmuller originally adjusted for as well as customizes the period over which to minimize the MSE. The ATT using the original method is -19.0018, and the RMSE for the pre-treatment period is 1.76. The corresponding weights using the full donor pool are 0.334 for Utah, 0.235 for Nevada, 0.2020 for Montana, Colorado 0.161, and Connecticut 0.068. So as we can see, the ATTs are very similar, and the pre-treatment prediction errors are pretty much the same. When we estimate this in Stata (omitting the auxilary covariate predictors and estimate synth2 cigsale cigsale(1988) cigsale(1980) cigsale(1975) , trunit(3) trperiod(1989) xperiod(1980(1)1988) nested fig, we get a RMSE of 4.33 and an ATT of -22.88. Furthermore, with this specification, the weights are no longer a sparse vector.\nThe point of this article is very simple. The original SCM works well, however it can be very sensitive to the inclusion of covariates, which covariates are included, what their lags are, and so on and so forth. Furthermore, there is also an issue of covariate selection in settings where we have multiple covariates that can potentially inform our selection of the donor pool. Furthermore, collecting a rich list of covariates may also not be possible in some settings. In such situations, especially without some pre-existing grount truth donor pool, analysts may apply the FSCM algorithm to guard against interpolation biases.\nAt least with the California example (and West Germany and Basque datasets, which I also tested), we can sometimes get comparable results to the baseline estimates which used multiple covariates for acceptable results (in all three of the standard test cases, FSCM actually get lower MSE than the original applications). In the Proposition 99 example, we select some of the same donor units, get a slightly better MSE and a very similar ATT without needing to fit to the covariates originally specified in the JASA paper.\nThe promise of machine-learning methods in this space is to automate away donor/predictor selection to some acceptable degree. The key thing of interest (for me, from an econometric theory perspective anyways) is which methods are best suited for this task, when do they perform well, and why. For example, it might be useful to derive bias bounds for this estimator to quantify how much the MSE should improve by compared to the original SCM and Forward DID, as has been done with clustering based methods, for example.\nA final caveat: in the original paper, Giovanni uses cross-validation to estimate this model, and he also employs the same covariates. I have not done the cross validation yet on my end, but I will very soon. As ususal, email me with questions or comments."
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we’re scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it’s just “scrape”, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that’s the language I use, but I’m certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don’t add them and then the job ends (this is what happens if I try to run the action after it’s ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I’m updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I’m scraping (unless you’re a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I’m home to personally oversee it. The value here is that I’ve manually gotten my computer to do a specific task every single day, the correct way (assuming you’ve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA’s site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it’s worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you’re a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we’ve automated our tasks correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Analysis, Data Science, and Causal Inference",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nArtificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nJan 24, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nOn Clustering for Synthetic Controls\n\n\n\nCausal Inference\n\nMachine Learning\n\n\n\n\n\n\nFeb 3, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nApplying Forward DID to Construction and Tourism Policy\n\n\n\nCausal Inference\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nFeb 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nCausal Inference Runs the World: Actionable Insights, Econometrics Style\n\n\n\nEconometrics\n\nCausal Inference\n\nData Science\n\n\n\n\n\n\nMar 3, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Regressing Control Method for Python\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 6, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sccare.html",
    "href": "sccare.html",
    "title": "Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?",
    "section": "",
    "text": "The Importance of Theory\nMany people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant’Anna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.\nHowever, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like mlsynth or geolift on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantyl, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory. And this fact is usually obvious to me from the problems they run into. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective. This is much more crucial because if you do not know when your methods are expected to work well, you are sort of blindly applying calculus and algebra without much thought as it how it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.\nThis is kind of a broader problem in data science in many respects. Ostensibly, there are myths about SCM. One is “the myth that SCM/[Augmented] SCM[s] don’t have assumptions like parallel trends”. I do not know who believes this myth, and I am inclined to think that widespread belief of this myth is itself a myth. After all, even your most applied economists you know will tell you “yeah, SCM has assumptions”. They may not be able to articulate them as formally, but everybody in my experience knows SCM has assumptions that need to be met (whether they verify them is of course another matter entirely. If you know anybody who truly holds these beliefs, direct them to me so I can chat with them).\nEither way, even there are not many people who confidently say/think “SCM has no assumptions”, a much better argument can be made that even if people do not literally think this… this is certainly the way many researchers act in practice. And that is the point of this post. My main point is that knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain.\n\n\nBasic SCM\nPeople often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this has been revised substantially in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or latent variable model) that often serves as a motivating data generating process. The linear factor model takes the form:\n\\[\nY_{it}(0) = \\boldsymbol{\\lambda}_t^\\top \\boldsymbol{\\mu}_i + \\varepsilon_{it}.\n\\]\nWhat does this mean? It simply means that our outcome are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units. These factors may indeed affect each unit differently… but they are common across units. SCM fundamentally is about matching our treated unit’s common factors to the common factors that we believe are embedded in the donor set (see conditons 1-6 here, such that unobservable heteorgeneity goes away and we have a good enough match between our donor pool an the target unit on both observed and unobserved factors. In other words, this is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual using control units that behave similarly, regardless of which units those happen to be, by exploiting the correlations between the donors and treated unit.\nA more flexible idea is the fine-grained potential outcomes model. This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit’s population. The practical implication of this is that when we apply these panel data models to real life settings, we are allowed to use different donor typs on the condition that they help us learn about the trajector of the target unit in the pre-intervention period. Because SCM does not care about what kind of donors you use, so long as they are informative donors.\n\n\nApplication\nUnconvinced? We can replicate some results to show this. We know the classic example of Prop 99, where California’s anti-tobacco program was compared to 38 donor states to predict the counterfactual per capita cigarette consumption. But California is a ridiculously large economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units? It turns our that we can do just that. Here, I compare California to the donor divisions of the United States, where the outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level and California’s remains as is.\n\n\n\n\n\n\n\n\n\nHere is TSSC estimator, where we adjust for an intercept.\n\n\n\n\n\n\n\n\n\nHere is \\(\\ell_2\\)-PDA estimator.\n\n\n\n\n\n\n\n\n\nHere is the predictions of the FDID estimator.\n\n\nTakeaway\nThe qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California’s pre-treatment trends, and so long as we are doing that, we may generally use as many or as little relevant donors as we like. The issue is not that donors in general do not matter. I wrote mlsynth precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit. The key issue though is that the “right donors” do not necessarily need to be the same type of unit as the kind you are using. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.\nNote that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy, or you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you too can apply the theoretical implications of econometrics to your own case.\n\n\n\n\n\n\n\n\n\n\nArtificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nJan 24, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nOn Clustering for Synthetic Controls\n\n\n\nCausal Inference\n\nMachine Learning\n\n\n\n\n\n\nFeb 3, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nApplying Forward DID to Construction and Tourism Policy\n\n\n\nCausal Inference\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nFeb 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nCausal Inference Runs the World: Actionable Insights, Econometrics Style\n\n\n\nEconometrics\n\nCausal Inference\n\nData Science\n\n\n\n\n\n\nMar 3, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Regressing Control Method for Python\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 6, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scdense.html",
    "href": "scdense.html",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "",
    "text": "Plenty of posts have been done in the last decade on the synthetic control method and related approaches. It is a flavor of artificial counterfactual estimator for the panel data setting. Folks from Microsoft, Databricks, Uber, Amazon, Netflix, Gainwell Technologies, and elsewhere have written about/covered it, detailing its implementation, use cases, and econometric theory.\nMany (not all) of these cover the standard SCM, developed originally to study terrorism in the Basque Country and conduct comparative case studies more broadly. Standard SCM tends to favor, under certain technical conditions, a sparse set of control units being assigned weights. These weights aim to reconstruct the factor loadings/observed values of the treated unit, pre-intervention. Sparsity, or the true coefficient vector being mostly 0, has appealing properties; for example, a sparse vector allows us to interpret the synthetic control a lot easier, facilitating the estimation of leave-one-out placebos and other sensitivity checks. In some cases though, the sparsity notion is unfounded.\nThe the \\(\\ell_2\\) panel data approach is an econometric methodology developed by Zhentao Shi and Yishu Wang. The \\(\\ell_2\\)-PDA is a form of synthetic control estimation that accommodates sparse or dense data generation processes. A dense DGP is when the true vector of coefficients is mostly not zero. Sometimes, a sparse model is too simplistic, especially in settings where we have very many predictors. This is especially true when we have lots of multicollinearity among our predictors, which may be very plausible in settings with a lot of control units. The LASSO and the convex hull SCM (for different reasons and in different cases) generally struggle with this, whereas the Ridge or \\(\\ell_2\\)-PDA accomodate multicollinearity as a feature. In this post, I demonstrate the method as implemented in my library mlsynth. The Python code for these results may be found here."
  },
  {
    "objectID": "scdense.html#a-review-of-synthetic-controls",
    "href": "scdense.html#a-review-of-synthetic-controls",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "A Review of Synthetic Controls",
    "text": "A Review of Synthetic Controls\nLet \\(\\mathbb{R}\\) denote the set of real numbers. Denote the natural numbers and unit interval respectively as \\[\n\\mathbb{N} \\coloneqq \\{1, 2, 3, \\dots\\}, \\quad \\mathbb{I} \\coloneqq \\{ w \\in \\mathbb{R} : 0 \\leq w \\leq 1 \\}.\n\\] Let a caligraphic letter, say \\(\\mathcal A\\), denote a descrete set whose cardinality is \\(A=|\\mathcal{A}|\\). The sup norm of a vector \\(\\mathbf{y} \\in \\mathbb{R}^N\\) is defined as the maximum absolute value of its components, \\(\\|\\mathbf{y}\\|_\\infty = \\max_{j = 1, \\ldots, N} |y_j|\\). The floor function of a real number \\(x \\in \\mathbb{R}\\), denoted as \\(\\lfloor x \\rfloor\\), returns \\(\\lfloor x \\rfloor = \\max \\{k \\in \\mathbb{N} : k \\leq x\\}\\). Let \\(t \\in \\mathbb{N}\\) and \\(i \\in \\mathbb{N}\\), represent the indices for \\(T\\) time periods and \\(N\\) units. The pre-treatment period consists of consecutive time periods \\(\\mathcal{T}_1 = \\{1, 2, \\ldots, T_0\\}\\) (cardinality \\(T_1\\)), while the post-treatment period is given by \\(\\mathcal{T}_2 = \\{T_0 + 1, \\ldots, T\\}\\) (cardinality \\(T_2\\)). The treated unit is indexed by \\(i = 1\\), while the remaining set of units, \\(\\mathcal{N}_0 \\coloneqq \\{2, \\ldots, N_0 + 1\\}\\) (cardinality \\(N_0\\)), forms the control group. Each outcome for all units is denoted by \\(y_{it}\\). Denote the outcome vector for the treated unit as \\(\\mathbf{y}_1 \\coloneqq \\begin{bmatrix} y_{11} & y_{12} & \\cdots & y_{1T} \\end{bmatrix}^\\top \\in \\mathbb{R}^T\\), where each entry corresponds to the outcome of the treated unit at time \\(t\\). The donor pool matrix, similarly, is defined as\n\\[\n\\mathbf{Y}_0 = \\begin{bmatrix}\n    y_{21} & y_{22} & \\cdots & y_{2T} \\\\\n    y_{31} & y_{32} & \\cdots & y_{3T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    y_{(N_0+1)1} & y_{(N_0+1)2} & \\cdots & y_{(N_0+1)T}\n  \\end{bmatrix} \\in \\mathbb{R}^{N_0 \\times T}.\n\\]\nThe key challenge of causal inference is that we only observe the factual/realized outcome, expressed as:\n\\[\ny_{it} = y_{it}^1 d_{it} + \\left(1 - d_{it}\\right)y_{it}^0.\n\\] Outcomes in this framework are a function of treatment status, \\(d_{it} \\in \\{0, 1\\}\\). While \\(d_{it} = 1\\), a unit is treated and while \\(d_{it} = 0\\) a unit is untreated. Thus, \\(y_{it}^1\\) is the potential outcome under treatment, \\(y_{it}^0\\) is the counterfactual (or potential) outcome under no treatment. The objective is to estimate the counterfactual outcomes \\(y_{1t}^0\\). SCMs and panel data approaches are weighting based estimators. In this setup, some weight vector \\[\n\\mathbf{w} \\coloneqq \\begin{bmatrix} w_2 & w_3 & \\cdots & w_{N_0+1} \\end{bmatrix}^\\top, \\: \\text{where } w_i \\: \\forall \\: i \\in \\mathcal{N}_0.\n\\] is assigned across the \\(N_0\\) control units to approximate the treated unit’s pre-intervention outcome. SCM solves for \\(\\mathbf{w}\\) via the optimization:\n\\[\n\\underset{\\mathbf{w} \\in \\mathbb{I}^{N_0}}{\\operatorname*{argmin}} \\lVert \\mathbf{y}_1 - \\mathbf{w}^\\top \\mathbf{Y}_0 \\rVert_2^2,  \\: \\forall \\: t \\in \\mathcal{T}_{1}, \\: \\text{subject to } \\lVert \\mathbf{w} \\rVert_1 = 1.\n\\]\nHere, \\(\\mathbf{w}^\\top \\mathbf{Y}_0\\) represents the dot product of predictions, also known as a weighted average. The constraint of the convex hull disallows extrapolation of any form. SCMs are predicated on good pretreatment fit, such that \\(\\mathbf{w}^\\top \\mathbf{Y}_0 \\approx \\mathbf{y}_1\\). Speaking generally, the better pre-treatment fit we have over a long time series, the better the out-of-sample predictions will be for the synthetic control."
  },
  {
    "objectID": "scdense.html#selecting-tau",
    "href": "scdense.html#selecting-tau",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "Selecting \\(\\tau\\)",
    "text": "Selecting \\(\\tau\\)\nCritical to \\(\\ell_2\\)-PDA is the selection of tau. The reason for this is because while \\(\\tau \\to 0\\), the estimator converges to OLS, which we know will overfit to the pre-intervention data in most cases unless the data are exactly low-rank and fully observed. Too high a value of \\(\\tau\\) results in drastic underfitting (trust me) of the pre-treatment data. The way mlsynth tunes tau is by cross-validation over the pre-treatment period. mlsynth invokes a log-space over the interval \\(\\tau \\in \\left[ 10^{-4}, \\tau_{\\text{init}} \\right]\\), where \\(\\tau_{\\text{init}} = \\|\\boldsymbol{\\eta}\\|_\\infty\\). These correspond to the values of tau we supply. We first create a training and validation period, \\(\\mathcal{T}_1 = \\mathcal{T}_1^{\\text{train}} \\cup \\mathcal{T}_1^{\\text{val}}, \\quad\n\\mathcal{T}_1^{\\text{train}} \\cap \\mathcal{T}_1^{\\text{val}} = \\emptyset.\\) Denote the training series as \\(\\mathcal{T}_1^{\\text{train}} = \\{1, 2, \\ldots, k\\}\\). Denote the validation series as \\(\\mathcal{T}_1^{\\text{val}} = \\{k+1, \\ldots, T_0\\}\\), where \\(k = \\left\\lfloor \\frac{T_1}{2} \\right\\rfloor\\). We then estimate the model for the training period, and compute our predictions from \\(\\{k+1, \\ldots, T_0\\}\\). The tau we select is\n\\[\\begin{aligned}\n\\tau^{\\ast} = \\operatorname*{argmin}_{\\tau} \\left( \\frac{1}{|\\mathcal{T}_1^{\\text{val}}|} \\| \\mathbf{y}^{\\ell_2} - \\mathbf{y}_1 \\|_2^2 \\right),\n\\end{aligned}\\]\nor the one that minimizes the validation Root Mean Squared Error. The performance is pretty close to what Zhentao and Yishu do for their empirical example (see the code at the documentation). In the original paper, they do a 70-30 train test split over the pre-intervention period, where they take the first 70% of the pre-treatment data and supply some values of tau.\nOkay hard part done. Let’s apply this to a real-life example, shall we?"
  },
  {
    "objectID": "scmo.html",
    "href": "scmo.html",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "",
    "text": "Sometimes, analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may possibly predict a target/outcome variable that we care about, and plenty of other papers have commented on this fact before. Some analysts even argue for the surrogate approach, where we treat other outcomes or affected units as a kind of instrument for the counterfactual trajcectory of the target unit.\nHowever, all of this is most uncommon. As it turns out, most people in academia and industry who use synthetic controls use only a single focal outcome in their analyses. Perhaps they will adjust their unit weights by some diagonal matrix, a diagonal matrix \\(\\mathbf{V}\\) in most applications. The point of this matrix is basically to assist the main optimization in choosing the unit weights. However, even this is limited by the number of pretreatment periods you have- if you have more covariates than you have pretreatment periods, you cannot estimate the regression. Recent papers by econometricians have tried to get around this, though. This blog post covers a few recent recent papers which have advocated for this. I explain the econometric method and apply it in a simulated setting."
  },
  {
    "objectID": "scmo.html#standard-synthetic-control",
    "href": "scmo.html#standard-synthetic-control",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Standard Synthetic Control",
    "text": "Standard Synthetic Control\nBefore introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator minimizes\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThis is a constrained least squares program in which we regress the treated unit’s pre-treatment outcomes onto the control matrix under the constraint that \\(\\mathbf{w}\\) lies in the simplex \\(\\Delta^{N_0}\\). For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights \\(\\mathbf{w}^\\ast\\) are estimated, the out-of-sample estimates are obtained by applying the same weights to the control matrix\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0^{\\text{post}} \\mathbf{w}^\\ast,\n\\]\nwith the concatenation between the in and out of sample vectors corresponding to the full predictions of the model. The estimated treatment effect at each post-treatment time point is then given by the difference between observed and out-of-sample outcomes: \\(\\hat{\\tau}_{1t} = y_{1t} - \\hat{y}_{1t}\\) for \\(t \\in \\mathcal{T}_2\\)."
  },
  {
    "objectID": "scmo.html#model-averaging",
    "href": "scmo.html#model-averaging",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Model Averaging",
    "text": "Model Averaging\nWe may also model average these models together, which sometimes results in better fit than using either model alone. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have \\(\\mathbf{y}_1^{\\text{CAT}} \\in \\mathbb{R}^{T_0}\\), which denotes the pre-treatment fit from the concatenated model by Tian, Lee, and Panchenko, and on the other hand, we have \\(\\mathbf{y}_1^{\\text{AVG}} \\in \\mathbb{R}^{T_0}\\), the corresponding fit from the demeaned model by Sun, Ben-Michael, and Feller. As before, we observe the treated unit’s pre-treatment trajectory, \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\).\nTo begin, we stack the two counterfactuals into a single matrix:\n\\[\n\\mathbf{Y}^{\\text{MA}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT}} & \\mathbf{y}_1^{\\text{AVG}}\n\\end{bmatrix} \\in \\mathbb{R}^{T_0 \\times 2}.\n\\]\nWe define the model-averaged pre-treatment fit as a convex combination of the two predictions\n\\[\n\\mathbf{y}_1^{\\text{MA}}(\\boldsymbol{\\lambda}) = \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda},\n\\]\nwhere \\(\\boldsymbol{\\lambda} \\in \\Delta^2\\) is a 2-dimensional simplex weight vector\n\\[\n\\Delta^2 = \\left\\{ \\boldsymbol{\\lambda} \\in \\mathbb{R}_{\\geq 0}^2 : \\| \\boldsymbol{\\lambda} \\|_1 = 1 \\right\\}.\n\\]\nThe model averaged objective function minimizes\n\\[\n\\boldsymbol{\\lambda}^\\ast = \\underset{\\boldsymbol{\\lambda} \\in \\Delta^2}{\\operatorname{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda} \\right\\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThe interpretation of the convex hull remains the same as in the traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the global minimum and maximum of the two individual estimators\n\\[\n\\mathbf{y}_1^{\\text{MA}} \\in \\left[\n\\min\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right),\n\\max\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right)\n\\right]\n\\quad \\forall t \\in \\mathcal{T}.\n\\]\nOnce \\(\\boldsymbol{\\lambda}^\\ast\\) is found, the model-averaged out-of-sample predictions are estimated like\n\\[\n\\mathbf{Y}^{\\text{MA, post}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT, post}} & \\mathbf{y}_1^{\\text{AVG, post}}\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_1^{\\text{MA, post}} = \\mathbf{Y}^{\\text{MA, post}} \\boldsymbol{\\lambda}^\\ast.\n\\]\nEssentially, this is a mixture of both models."
  },
  {
    "objectID": "scmo.html#conformal-prediction-via-agnostic-means",
    "href": "scmo.html#conformal-prediction-via-agnostic-means",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Conformal Prediction via Agnostic Means",
    "text": "Conformal Prediction via Agnostic Means\nNow a final word on infernece. I use conformal prediction intervals to conduct inference here, developed in this paper. Precisely, I use the agnostic approach (yes, I know other approaches exist; users of mlsynth will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as \\(\\mathbf{u}_{\\text{pre}} = \\mathbf{y}_{1,\\text{pre}} - \\mathbf{y}^{\\text{SC}}_{1,\\text{pre}}\\), or just the pretreatment difference betwixt the observed values and its counterfactual. Furthermore, let \\(\\hat{\\sigma}^2 = \\frac{1}{T_0 - 1} \\left\\| \\mathbf{u}_{\\text{pre}} - \\bar{u} \\mathbf{1} \\right\\|^2\\) be the unbiased estimator of the residual variance, where \\(\\bar{u} = \\frac{1}{T_0} \\sum_{t=1}^{T_0} u_t\\) is the mean residual.\nWe aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period \\(\\mathbf{y}^{\\text{SC}}_{1,\\text{post}} \\in \\mathbb{R}^{T_1}\\) be the post-treatment SC predictions for some generic estimator. Assuming that the out-of-sample error is sub-Gaussian given the history \\(\\mathscr{H}\\) (in plain English, this just means that large errors are unlikely, which makes sense given that SC is less biased in a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via concentration inequalities. Specifically, we have \\(\\delta_\\alpha = \\sqrt{2 \\hat{\\sigma}^2 \\log(2 / \\alpha)}\\). The conformal prediction intervals are then defined as \\(\\mathbf{p}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} - \\delta_\\alpha \\mathbf{1}\\), \\(\\mathbf{u}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} + \\delta_\\alpha \\mathbf{1}\\). These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will likely be incorporated in the future."
  },
  {
    "objectID": "scmo.html#simulation",
    "href": "scmo.html#simulation",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Simulation",
    "text": "Simulation\nSuppose we are working at Airbnb, and we wish to see the causal effect of the introduction of Airbnb Experiences on Gross Booking Value (GBV), a metric which is defined as ‘’the total revenue generated by room or property rentals before any costs or expenses are subtracted’’. Airbnb Experiences connects users of the platform to local tour guides or other local attractions. It serves as a kind of competition to Travelocity, Viator and other booking/ travel services. In other words, this program may make makes this city an attraction, and we may see an increase in GBV as a result.\nWell, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy. The point of this simulation is to use the SCMO (synthetic control multiple outcomes) estimator to measure the causal impact.\nFor each unit, the observed outcome \\(\\mathbf{Y}_{jtk}\\) evolves according to an autoregressive process with latent structure for time, place, and seasonality\n\\[\n\\mathbf{Y}_{jtk} =\n\\rho_k \\mathbf{Y}_{jt-1k} +\n(1 - \\rho_k) \\left(\n\\alpha_{jk} + \\beta_{tk} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{tk} + \\mathbf{S}_{jt} + \\delta_k\n\\right) + \\varepsilon_{jtk}, \\quad \\text{for } t &gt; 1,\n\\]\nwith initial condition\n\\[\n\\mathbf{Y}_{j1k} =\n\\alpha_{jk} + \\beta_{1k} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{1k} + \\mathbf{S}_{j1} + \\delta_k + \\varepsilon_{j1k}.\n\\]\nHere, \\(\\alpha_{jk} \\sim \\mathcal{N}(0, 1)\\) and \\(\\beta_{tk} \\sim \\mathcal{N}(0, 1)\\) represent unit-outcome and time-outcome fixed effects, respectively. Each unit \\(j\\) possesses latent attributes \\(\\boldsymbol{\\phi}_j \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\), while each time-outcome pair \\((tk)\\) has associated latent loadings \\(\\boldsymbol{\\mu}_{tk} \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\). The seasonal component \\(\\mathbf{S}_{jt}\\) captures unit-specific periodicity and is defined as \\(\\gamma_j \\cos\\left( \\frac{4\\pi(t - \\tau_j)}{T_{\\text{season}}} \\right)\\), with \\(\\gamma_j \\sim \\text{Unif}(0, \\bar{\\gamma})\\) representing the amplitude and \\(\\tau_j \\sim \\text{Unif}\\{0, \\dots, T_{\\text{season}} - 1\\}\\) the phase shift. Each outcome \\(k\\) has a baseline shift \\(\\delta_k \\sim \\text{Unif}(200, 500)\\), an autocorrelation parameter \\(\\rho_k \\in (0, 1)\\), and an idiosyncratic noise component \\(\\varepsilon_{jtk} \\sim \\mathcal{N}(0, \\sigma^2)\\). One unit (Iquique, Chile in this draw) is designated as treated. To introduce selection bias, the unit with the second-largest realization on the first latent factor dimension is treated, meaning methods like difference-in-differences or interrupted time series methods will not perform well. The target unit’s GBV receives an additive treatment effect of \\(+5\\) during all post-treatment periods."
  },
  {
    "objectID": "scmo.html#results",
    "href": "scmo.html#results",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Results",
    "text": "Results\nWhen we run this estimator, we need to specify one of three estimators: TLP, SBMF, or both, where the abbreviations are obviouslyfor the surnames of the authors. We also need to supply a dictionary entry to mlsynth called addout. This is either a string or a list which lists the additional outcomes we care about in the dataframe. When we run the estimator, we get:\n\n# Run simulation\ndf = simulate(seed=10000, r=3)\nconfig = {\n    \"df\": df,\n    \"outcome\": 'Gross Booking Value',\n    \"treat\": 'Experiences',\n    \"unitid\": 'Market',\n    \"time\": 'Week',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\"], \"addout\": list(df.columns[4:]),\n    \"method\": \"both\"\n}\n\narco = SCMO(config).fit()\n\n\n\n\n\n\n\n\nUsing the model averaging estimator, our pre-treatment Root Mean Squared Error is 0.276. The ATT is 5.046. The weights are also a sparse vector. The model averaged estimator returns Arequipa (0.237), Bogotá (0.232), San Salvador (0.218), Santiago de Chile (0.174), San Luis Potosí (0.081), Montevidio (0.032), and Manzanillo (0.026), as the contributing units or only 7 of the 98 donor units. The optimal mixing between the models is 0.538 for the intercept-shifted estimator and 0.461 for the concatenated method. For DID, the RMSE is 0.878 and the ATT is 5.2, meaning that the intercept adjusted average of all donor units is clearly a biased estimator.\nCompare to Forward DID, this is NOT true: we have an ATT of 5.063 and a pre-intervention RMSE of 0.289, selecting Antofagasta, Bocas del Toro, Punta del Este, San Pedro Sula, and Santiago as the optimal donor pool (this method uses no additional outcomes, only the GBV metric). When I compare to the clustered PCR method, the positively weighted donors are San Pedro Sula (0.296), Punta del Este (0.255), Santiago (0.199), Antofagasta (0.190), and La Plata (0.060)."
  },
  {
    "objectID": "synthinter.html",
    "href": "synthinter.html",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "",
    "text": "Most data scientists who use synthetic control methods likely aim to estimate the counterfactual outcome for a treated unit if it had not been treated at all. This framework works quite neatly in the setting with a dummy treatment status (exposed or not exposed). But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, we are typically concered with the outcome under the scenario of no treatment. A research question could be how the store with the cash backprogram would have fared had it done nothing at all. In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus.\nBut what if we wish to estimate the counterfactual for a given unit as if it did another policy from what it actually did. In other words, How would Store A have performed if it had adopted Program B instead of Program A? What if City 1 had imposed a soda tax instead of banning large sodas? How would health metrics evolved if the other policy was done instead? Plenty of academic papers have addressed this idea before, but only to assess the counterfactual scenario of no tax at all.\nThis is where the Synthetic Interventions estimator is useful. SI estimates how a treated unit (or even a never treated unit) would have performed under an intervention it did not actually receive. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI substantially expands the range of counterfactual questions that can be credibly answered.\nBefore we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like this circulate that claimed to be able to estimate things like “what would NYC’s COVID rate look like had it locked down earlier than it actually did”. I had never heard of tensors, or really even matrices at the time, so I would always ask “wow, how can we even do this?” So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others."
  },
  {
    "objectID": "synthinter.html#si-in-mlsynth",
    "href": "synthinter.html#si-in-mlsynth",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "SI in mlsynth",
    "text": "SI in mlsynth\nNow I will give an example of how to use SI for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nNext we load the Proposition 99 data, using the dataset that has all of the states in the United States.\n\nimport pandas as pd\nfrom mlsynth.mlsynth import SI\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/jehangiramjad/tslib/refs/heads/master/tests/testdata/prop99.csv\"\ndf = pd.read_csv(url)\nlast_column = df.columns[-1]\ndf_filtered = df[df[last_column] == 2]\n\ncolumns_to_keep = list(df.columns[0:4]) + [df.columns[7]]\ndf_filtered = df_filtered[columns_to_keep]\n\nsort_columns = [df_filtered.columns[1], df_filtered.columns[2]]\ndf_filtered = df_filtered.sort_values(by=sort_columns)\n\ndf_filtered = df_filtered[df_filtered[df_filtered.columns[2]] &lt; 2000]\n\ndf_filtered['SynthInter'] = ((df_filtered[df_filtered.columns[0]] == 'LA') &\n                         (df_filtered[df_filtered.columns[2]] &gt;= 1992)).astype(int)\n\n\ntax_states = ['MA', 'AZ', 'OR', 'FL']  # Massachusetts, Arizona, Oregon, Florida abbreviations\ndf_filtered['Taxes'] = (df_filtered[df_filtered.columns[0]].isin(tax_states)).astype(int)\n\nprogram_states = ['AK', 'HI', 'MD', 'MI', 'NJ', 'NY', 'WA', 'CA']\ndf_filtered['Program'] = (df_filtered[df_filtered.columns[0]].isin(program_states)).astype(int)\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nWe begin with simple data cleaning. All we do is keep the relevant metrics, setting a treatment variable “SynthInter” to be 1 if the year is greater than or equal to 1992 and the unit of interest is Louisiana. I choose 1992 because this is the year Massachusetts passed its anti tobacco policy. Note that no policy in fact happened in Louisiana, so we will see the effect of keeping the status quo of no tobacco control policy at all. We then define as an indicator which states did the taxes. According to Abadie’s 2010 paper those states are Massachusetts, Arizona, Oregon, and Florida. We also define an indicator for states that did an anti-tobacco statewide program. This way, we can see how per capita smoking would have evolved under either policy. Under the hood, we just loop through each of the different policies the user specifies.\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nconfig = {\n    \"df\": df_filtered,\n    \"outcome\": 'cigsale',\n    \"treat\": 'SynthInter',\n    \"unitid\": 'State',\n    \"time\": 'Year',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"inters\": [\"Taxes\", \"Program\"],\n    \"objective\": \"OLS\"\n}\n\narco = SI(config).fit()\n\n\n\n\n\n\n\n\nFor Louisiana, the ATT of keeping the status quo relative to the state-wide tobacco program maintained per capita consumption 19.134 higher than what it would have been otherwise. The ATT of not passing taxes on tobacco, compared to taxes, mainatined the same at roughly 19.77 higher than what would have been otherwise. To me, these findings are not very surprising: “Anti tobacco programs/taxes reduce slightly tobacco consumption, wow, how shocking!” But now, we can actually answer these kinds of questions using econometric methods, instead of just speculating.\nWhen we look at the dictionary that the SI class returns, we see that the policies would have been pretty effective at reducing tobacco consumption. The dictionary is policy specific, one dictionary per policy the user chooses to consider. The SI estimator also only works for 1 treated unit, however analysts can easily make (say) a list of dataframes with an indicator for each synthetic treatment and loop over them. Once they have done this, we may extract the ATT per policy, per treated unit, and compute the sample average for an event-time ATT version for a given policy across units. I do not optimize the code to do this for a few reasons: one, the more policies and treated units we have, the less efficient it becomes to estimate tractably. Furthermore, the SI estimator is about personalized causal inference. So, I will likely leave this class for users to adapt to their own purposes (with some adjustments, if demand exists for something else)."
  },
  {
    "objectID": "synthregcontrol.html",
    "href": "synthregcontrol.html",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "",
    "text": "This blog post covers the synthetic regressing control method. It is a new flavor of SCM that was once only available in R. The basic innovation method is to penalize poor donors who do not predict the target vector in a demeaned space. The paper itself is extremely notationally dense. The writing is all over the place, and I literally put off coding the estimator for a year because of how complicated of a read the paper is (not the econometric theory, I mean the literal description of the estimator). But, I think I’ve finally figured it out. This post introduces and hopefully simplifies the explication of the estimator, and allows for people to use it for their own work."
  },
  {
    "objectID": "synthregcontrol.html#synthetic-controls",
    "href": "synthregcontrol.html#synthetic-controls",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "Synthetic Controls",
    "text": "Synthetic Controls\nI begin by reviewing basic synthetic controls. Our main goal is to estimate the counterfactual outcomes in the post-treatment period (i.e., for \\(t &gt; T_0\\)), whose accuracy is guaranteed only on the basis of quality fit in the pre-intervention period. We define the synthetic control as a weighted average of the donor units. The goal is to find the weight vector \\(\\mathbf{w}\\) that best approximates the outcome vector of the treated unit during the pre-treatment period. We minimize\n\\[\n\\underset{\\mathbf{w} \\in \\mathcal{W}}{\\operatorname*{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\right\\|_2^2\n\\]\nwhere the entries for the weight vector live on the simplex:\n\\[\n\\mathcal{W} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{N_0} : \\mathbf{w} \\geq 0, \\|\\mathbf{w}\\|_1 = 1 \\right\\}\n\\]\nOnce the weights \\(\\mathbf{w}\\) are estimated, the counterfactual outcome for the treated unit during the post-treatment period is given by\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0 \\mathbf{w}, \\quad \\text{for } t &gt; T_0\n\\]\nThus, the counterfactual outcome for each time period in the post-treatment period is the weighted sum of the donor units’ outcomes, with the weights derived from the pre-treatment period."
  },
  {
    "objectID": "synthregcontrol.html#step-1-computing-the-alignment",
    "href": "synthregcontrol.html#step-1-computing-the-alignment",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "Step 1: Computing the Alignment",
    "text": "Step 1: Computing the Alignment\nFor \\(\\forall \\: j \\in \\mathcal{N}\\), an alignment coefficient \\(\\hat{\\theta}_j\\) is computed by projecting \\(\\widetilde{\\mathbf{y}}_1\\) onto the corresponding column \\(\\widetilde{\\mathbf{Y}}_{0,j}\\) of the demeaned donor matrix. In plain English, this amounts to running \\(N_0\\) separate OLS regressions, each using one donor to predict the treated unit: \\[\n\\hat{\\theta}_j = \\frac{\\widetilde{\\mathbf{y}}_1^\\top \\widetilde{\\mathbf{Y}}_{0,j}}{\\| \\widetilde{\\mathbf{Y}}_{0,j} \\|^2}, \\quad \\forall \\: j \\in \\mathcal{N}_0.\n\\] These coefficients collectively form the vector \\(\\hat{\\boldsymbol{\\theta}} \\in \\mathbb{R}^{N_0}\\). In plain English, these coefficients tell us how well each donor unit matches the treated unit in terms of specific trends (not levels, trends) in the data, after adjusting for broader trends via the original demeaning. If the coefficient for a donor is high, it means that unit’s behavior is closely aligned with the treated unit in this adjusted space. It is the equivalent of running univariate regression for each donor unit.\nWe then define the alignment-adjusted donor matrix (which we will later use in optimization) as \\[\n\\mathbf{Y}_{\\boldsymbol{\\theta}} = \\mathbf{Y}_0^{\\text{pre}} \\cdot \\mathrm{diag}(\\hat{\\boldsymbol{\\theta}}),\n\\] which rescales each donor column by its alignment with the treated unit."
  },
  {
    "objectID": "synthregcontrol.html#estimating-the-noise-variance",
    "href": "synthregcontrol.html#estimating-the-noise-variance",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "Estimating the Noise Variance",
    "text": "Estimating the Noise Variance\nThe SRC method needs to estimate the residual noise variance of the donor projection (or, our donor matrix multiplied by the theta values) with the target unit’s demeaned values. We call this variance term \\(\\hat{\\sigma}^2\\). Using the centering matrix \\(\\mathbf{C}_{T_0}\\), we define \\[\n\\mathbf{G} = \\mathbf{Y}_0^{\\text{pre}^\\top} \\mathbf{C}_{T_0} \\mathbf{Y}_0^{\\text{pre}}.\n\\] \\(\\mathbf{G}\\) is what we call a centered Gram matrix. It tells us how similar each donor unit’s behavior is to the others, but with the broad, common time trends removed (that’s what makes it demeaned). Think of it as a covairance matrix showing how the donor units’ patterns align with each other in terms of their behaviors in the pre-treatment period. We extract its diagonal to normalize the donor projection, forming the operator \\[\n\\mathbf{Z} = \\mathbf{Y}_0^{\\text{pre}} \\cdot \\mathrm{diag}\\left( \\mathrm{diag}(\\mathbf{G})^{-1} \\right) \\cdot \\mathbf{Y}_0^{\\text{pre}^\\top}.\n\\] is like a weighted version of the donor data, where the “importance” of each donor unit is based on its relationship to the others (measured by \\(\\mathbf{G}\\)). Donors whose behavior is more idiosyncratic (in the demeaned space) are weighted more heavily in this matrix, reflecting their unique explanatory power for the treated unit. We care about how well these demeaned adjusted donors are related to the target unit. We measure this by estimating a residual (equation 14 from the paper), which is constructed like \\[\n\\mathbf{r} = \\mathbf{C}_{T_0} \\mathbf{y}_1^{\\text{pre}} - \\mathbf{C}_{T_0} \\mathbf{Z} \\mathbf{C}_{T_0} \\mathbf{y}_1^{\\text{pre}}.\n\\] The equation intimidated me at first, but the left hand term is just the demeaned target vector and the right hand term projects the demeaned, normalized version of the donor matrix on to the target unit. The estimated noise variance is \\[\n\\hat{\\sigma}^2 = \\| \\mathbf{r} \\|^2.\n\\]\nIn English, this tells us how much of the treated unit’s unique behavior cannot be explained by the normalized, centered donor pool’s projection on to the treated unit. The closer the residual is to 0, the better fitting the demeaned adjusted donors will be to the target unit. The more dissimilar they are, the higher the variance will be. Why did I explain any of this? Because this sets up the main equations from the paper, equations 15 and 16. In these equations, we penalize their discrepancies using the estimated noise variance \\(\\hat{\\sigma}^2\\)."
  },
  {
    "objectID": "synthregcontrol.html#solving-the-optimization-problem",
    "href": "synthregcontrol.html#solving-the-optimization-problem",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "Solving the Optimization Problem",
    "text": "Solving the Optimization Problem\nWith \\(\\hat{\\boldsymbol{\\theta}}\\) and \\(\\hat{\\sigma}^2\\) in hand, we solve the program\n\\[\n\\underset{\\mathbf{w} \\in \\mathcal{W}}{\\operatorname*{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_{\\boldsymbol{\\theta}} \\mathbf{w} \\right\\|^2 + 2 \\hat{\\sigma}^2 \\cdot \\mathbf{1}_{N_0}^\\top \\mathbf{w},\n\\]\nThe left term on the RHS is the standard convex optimization of SCM (except we use the theta-adjusted donor matrix, Y0_pre * theta_hat). The right term is the penalization term upon the weighted for donors that are dissimilar to the target unit. After we estimate the weights, we can compute the in-sample and out of sampl predictions. Let\n\\[\n\\bar{y}_1 = \\frac{1}{T_0} \\mathbf{1}_{T_0}^\\top \\mathbf{y}_1^{\\text{pre}}, \\quad \\text{and} \\quad \\bar{\\mathbf{y}}_0 = \\frac{1}{T_0} \\mathbf{Y}_0^{\\text{pre}^\\top} \\mathbf{1}_{T_0}\n\\] be the mean of the treated unit and the vector of donor means, respectively, in the pre-treatment period. Let \\(\\mathbf{Y}_0^{\\text{post}} \\in \\mathbb{R}^{T_1 \\times N_0}\\) denote the donor outcomes in the post-treatment period. Our SRC predictions take the form of\n\\[\n\\mathbf{y}^{\\text{SRC}}_1 = \\begin{bmatrix} \\hat{\\mathbf{y}}_1^{\\text{pre}} \\\\ \\hat{\\mathbf{y}}_1^{\\text{post}} \\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf{Y}_0^{\\text{pre}} (\\hat{\\boldsymbol{\\theta}} \\odot \\mathbf{w}) \\\\\n\\bar{y}_1 \\cdot \\mathbf{1}_{T_1} + \\left( \\mathbf{Y}_0^{\\text{post}} - \\mathbf{1}_{T_1} \\bar{\\mathbf{y}}_0^\\top \\right)(\\hat{\\boldsymbol{\\theta}} \\odot \\mathbf{w})\n\\end{bmatrix},\n\\]\nusing the coefficients we just calculated."
  },
  {
    "objectID": "synthregcontrol.html#estimating-src-in-mlsynth",
    "href": "synthregcontrol.html#estimating-src-in-mlsynth",
    "title": "The Synthetic Regressing Control Method for Python",
    "section": "Estimating SRC in mlsynth",
    "text": "Estimating SRC in mlsynth\nNow I will give an example of how to use SRC for your own applied work. We begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nFor comparison sake, I use the Basque data since that’s what the author uses.\n\nimport pandas as pd\nfrom mlsynth.mlsynth import SRC, FSCM\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nubertheme = {\n    \"figure.facecolor\": \"white\",\n    \"figure.figsize\": (11,5),\n    \"figure.dpi\": 100,\n    \"figure.titlesize\": 16,\n    \"figure.titleweight\": \"bold\",\n    \"lines.linewidth\": 1.2,\n    \"patch.facecolor\": \"#0072B2\",  # Blue shade for patches\n    \"xtick.direction\": \"out\",\n    \"ytick.direction\": \"out\",\n    \"font.size\": 14,\n    \"font.family\": \"sans-serif\",\n    \"font.sans-serif\": [\"DejaVu Sans\"],\n    \"axes.grid\": True,\n    \"axes.facecolor\": \"white\",\n    \"axes.linewidth\": 0.1,\n    \"axes.titlesize\": \"large\",\n    \"axes.titleweight\": \"bold\",\n    \"axes.labelsize\": \"medium\",\n    \"axes.labelweight\": \"bold\",\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.left\": False,\n    \"axes.spines.bottom\": False,\n    \"axes.titlepad\": 25,\n    \"axes.labelpad\": 20,\n    \"grid.alpha\": 0.1,\n    \"grid.linewidth\": 0.5,\n    \"grid.color\": \"#000000\",\n    \"legend.framealpha\": 0.5,\n    \"legend.fancybox\": True,\n    \"legend.borderpad\": 0.5,\n    \"legend.loc\": \"best\",\n    \"legend.fontsize\": \"small\",\n}\n\nmatplotlib.rcParams.update(ubertheme)\n\n# URL to fetch the dataset\nurl = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'\ndf = pd.read_csv(url)\n\ntreat = df.columns[-1]\ntime = df.columns[1]\noutcome = df.columns[2]\nunitid = df.columns[0]\n\nconfig = {\n    \"df\": df,\n    \"treat\": treat,\n    \"time\": time,\n    \"outcome\": outcome,\n    \"unitid\": unitid,\n    \"display_graphs\": False,\n    \"counterfactual_color\": \"blue\"\n}\n\n# Create instances of each model with the config dictionary\nmodels = [SRC(config), FSCM(config)]\n\n\nresults = []\nfor model in models:\n    result = model.fit()\n    results.append((type(model).__name__, result))  # Store the model name and result as a tuple\n\nplt.axvline(x=20, color='grey', linestyle='--', label=\"Terrorism Begins\")\n\ncolors = ['red', 'blue']\n\nfor i, (model_name, result) in enumerate(results):\n    counterfactual = result[\"Vectors\"][\"Counterfactual\"]\n    plt.plot(counterfactual, label=f\"{model_name} Basque\", linestyle='-', color=colors[i])\n\nobserved_unit = result[\"Vectors\"][\"Observed Unit\"]\nplt.plot(observed_unit, label=f\"Basque\", color='black', linewidth=2)\n\nplt.xlabel('Time Periods')\nplt.ylabel('GDP per Capita')\nplt.title('Synthetic Regression Control Versus Forward SCM')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nHere we plot the counterfactuals for each method versus the observed donor units. The donor weights for the SRC are Aragon (0.044), Cantabria (0.13), Cataluna (0.241), Madrid (0.348), Navarra, Asturias (0.128), and Rioja (La) (0.104). These are not the same weights from the paper, since the paper incorporates the covariates from the orginal paper. In the paper, Austurias got 0.001, Cantabria got 0.311, Cataluna has 0.028, Madrid has 0.276, and La Rioja has 0.587. The ATT is -0.647, and the pre-treatment is 0.087. Forward SCM gets an ATT of -0.692 and an RMSE of 0.084. The FSCM algorithm selected 9 weights of the 16 donor units, Cataluna (0.826), Madrid (Comunidad De) (0.168), and Asturias (0.005). So, while the methods differ, they attain similar ATTs and reach similar practical conclusions. In the paper, the author also allows the donors to be screened via sure independent ranking and screening (PDF page 17). I didn’t implement this myself, but it seems to be a way to reduce the donor pool so that our estimates are improved because our donor pool has improved. I’ll include this as an option for SRC in the future, so users can implement it and maybe compare it to Forward Selection or the other methods mlsynth offers.\nAs ususal, email me with comments or questions, and share the post on LinkedIn should others find this helpful."
  }
]