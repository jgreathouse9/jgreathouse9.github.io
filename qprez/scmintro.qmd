---
title: "The Synthetic Control Method"
date: today
author:
  - name: "[Jared Greathouse](https://jgreathouse9.github.io)"
format:
  revealjs: 
    slide-number: true
    transition: fade
    scrollable: true
    background-color: "#EAF7F1"
    theme: clean.scss
highlight-style: github
---

## Introduction to Synthetic Control Methods (SCM)

- **Historical roots**  

  Originates from the *comparative case study* framework:  

  - Small number of treated units.  
  - Compare treated units against untreated units to infer causal effects.

- **Connection to Difference-in-Differences (DiD)**  
  - DiD assumes potential outcomes for unit $j$ at time $t$ under no treatment follow:  

    $$
    y_{jt}(0) = \mu_j + \lambda_t + \epsilon_{jt},
    $$

    where $\mu_j$ is a *unit-specific intercept* (unobserved) and $\lambda_t$ is a *time-specific effect* common to all units.  
    The errors $\epsilon_{jt}$ capture idiosyncratic shocks.

  - SCM generalizes this to an *interactive fixed effects model*, allowing time factors to affect units differently:  

    $$
    y_{jt}(0) = \mathbf{\mu}_j^\top \mathbf{\lambda}_t + \epsilon_{jt},
    $$

    where $\mathbf{\mu}_j \in \mathbb{R}^r$ and $\mathbf{\lambda}_t \in \mathbb{R}^r$ are *latent factor vectors*.  
    - Each dimension $k = 1,\dots,r$ represents a latent factor driving variation across units and time.  
    - SCM attempts to construct a *weighted combination of control units* that approximates $\mathbf{\mu}_j^\top \mathbf{\lambda}_t$ for the treated unit.


## What Is An Average?

We need to understand what an average is.

::: {.panel-tabset}

### Scalars

- The arithmetic mean of a finite sequence of real numbers  
  $(x_1, x_2, \dots, x_n) \in \mathbb{R}^n$  
  is defined as  

  $$
  \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
  = \frac{x_1 + x_2 + \cdots + x_n}{n}.
  $$

- For example, say you have 12 apples and your friend has 18.  
  Denote this as the sequence  
  $A = (12, 18) \in \mathbb{N}^2$:  

  $$
  \bar{x} = \frac{1}{2}(12 + 18) = 15.
  $$

On average, you have 15 apples. Pretty straightforward.

---

### Vectors

- We can also represent the same example as a row vector:

$$
\mathbf{x} = \begin{bmatrix} 12 & 18 \end{bmatrix} \in \mathbb{R}^{1 \times 2}.
$$

- Here, the sequence $A$ is mapped onto a spreadsheet, where each column entry represents a person. The value in each column is the number of apples that person has.

- To compute the average, we apply the same formula. Since there are two people, each entry gets weight $1/2$. We multiply each entry by its weight and sum:

$$
\bar{x} = \frac{1}{2} \cdot 12 + \frac{1}{2} \cdot 18 = 15.
$$

- This is mathematically equivalent to dividing the sum of the entries by 2, as we did with scalars.  
  In other words, arithmetic averaging is just taking a **weighted sum**, where each weight is the reciprocal of the number of entries.

---

### Matrices

- Let’s extend the idea of averaging to multiple time periods. Suppose today you have 12 apples and tomorrow 30, while your friend has 18 today and 22 tomorrow. We can put this into a matrix, or a collection of vectors, where each row is a time period and each column is a person:

$$
\mathbf{Y} =
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\in \mathbb{R}^{2 \times 2}.
$$

- Now, we have columns *and* rows. Each row represents a snapshot in time, and each column represents a person.

- To find the average number of apples per time period, we use the same weights as before (1/2 for each person). Multiplying the matrix by the weight vector gives a row-wise weighted sum:

$$
\mathbf{w} = 
\begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix}, \quad
\bar{\mathbf{x}} = \mathbf{Y} \mathbf{w} 
=
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix} 
=
\begin{bmatrix} 15 \\ 26 \end{bmatrix}.
$$

- Interpretation: Today, the average is 15 apples; tomorrow, the average is 26 apples.  

- Row-wise averaging is thus just taking a weighted sum across the columns for each row.  
  Each row "collapses" into a single number representing the average for that time period.  

- This concept generalizes to any time-series or panel data, such as GDPs, incomes, or other outcomes measured across multiple units.

:::


## What Are We Weighting For? Introducing Weighted Averages

::: {.panel-tabset}

### Simple Averages

You are at a bar with your friend. You earn \$60k/year. Your friend earns \$70k/year.  
The simple average of your incomes is:

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \begin{bmatrix} 60 & 70 \end{bmatrix} 
\begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix} 
= \frac{1}{2}\cdot 60 + \frac{1}{2}\cdot 70 
= 65 \text{ k/year}.
$$

Now, suddenly Bill Gates walks in, with a net worth of \$107B/year. Taking the simple average of all three income levels:  

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{3} \\[1mm] \frac{1}{3} \\[1mm] \frac{1}{3} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \frac{1}{3}\cdot 60 + \frac{1}{3}\cdot 70 + \frac{1}{3}\cdot 1.07\times 10^{11}
\approx 35.7 \text{ B/year}.
$$

Clearly, treating Bill Gates the same as the others **skews the average**.

- If the bartender next day bragged about earning 50k that night, we'd say:  
  "It's not that you earned 50k organically; a wealthy dude happened to be there that night.  
  We can't use this as evidence that your business is booming."

- In other words, if we want a meaningful "bar average," Bill should likely get much **less weight** (if any) than the other patrons.

---

### Weighted Average

- There is no law of nature that says the weight must be $1/n$. Practically, we can assign weights however we like.

- Instead of treating Bill as equal to you two, we can give each person a weight $w_i \in [0,1]$, deciding how much they count in the average.  
  Previously, in the arithmetic mean, $w_i = 1/n$.  

- Formally, the **weighted average** is:

$$
\bar{x}_{\text{weighted}} = \sum_{i=1}^{n} w_i x_i, \quad \sum_{i=1}^{n} w_i = 1.
$$

- Here, Bill can get a tiny weight (if any), and you and your friend get larger weights, giving a more reasonable "average bar income."

---

### Weighted Average Cont.

Suppose we only care about you and your friend, and ignore Bill Gates (weight = 0):

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} 0.5 \\[1mm] 0.5 \\[1mm] 0 \end{bmatrix}
$$

Then the weighted average is:

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= 0.5\cdot 60 + 0.5\cdot 70 + 0 \cdot 1.07\times 10^{11}
= 65 \text{ k/year}.
$$

Now the average is back to what it was initially.  
Bill clearly is **not representative** of a bar or even a country. Giving him as much weight as regular humans does not make sense.

:::


## Applied Averages

::: {.panel-tabset}

### National Averages

- In the earlier examples, the weights we used were arbitrary.  
  We split 50/50 between you and your friend, but there was no real reason for that choice.  
  In real life, weights are never handed down from the heavens: we, the econometricians, must assign them.  
  Assigning them poorly can produce nonsensical averages.

- The arithmetic average is familiar to economists as the **national average**.  
  It is used all the time in policy discussions, for example:  
  - [Crime and militarized crackdowns](https://www.pbs.org/newshour/politics/trump-deploys-national-guard-to-memphis-calling-it-a-replica-of-his-crackdown-on-washington)  
  - [Healthcare spending](https://www.cnn.com/2025/03/07/politics/medicaid-cuts-republican-districts-analysis)  
  - [Gasoline prices](https://calmatters.org/commentary/2024/10/political-theater-californias-gasoline-prices/)

- But again, this assumes that the **aggregate weights** produce a meaningful comparison for a single unit — which may not be true (recall the Bill Gates example).


### Basque

- [Suppose](https://www.aeaweb.org/articles?id=10.1257/000282803321455188) we have one unit exposed to terrorism in 1975 (the Basque Country), and a set of 16 other units that are not exposed. We want to estimate the effect of terrorism on GDP per capita. How to choose the weights?

- The Basque Country is much wealthier than many areas of Spain, has a unique political climate, and was more educated and industrialized than other areas.

- Optimization can solve this problem. Instead of us just arbitrarily assigning weights, we *choose the weights that minimize the pre-intervention gap* between the Basque Country and the weighted donor units.  

```{python}


import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown
import matplotlib

# matplotlib theme
jared_theme = {'axes.grid': True,
              'grid.linestyle': '--',
              'legend.framealpha': 1,
              'legend.facecolor': 'white',
              'legend.shadow': True,
              'legend.fontsize': 14,
              'legend.title_fontsize': 16,
              'xtick.labelsize': 14,
              'ytick.labelsize': 14,
              'axes.labelsize': 16,
              'axes.titlesize': 20,
              'figure.dpi': 100}

matplotlib.rcParams.update(jared_theme)

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Compute empirical donor mean ---
donor_mean = Y0.mean(axis=1)  # average across columns (donor units)

# --- Create figure with 2 subplots ---
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# --- Left subplot: observed Basque only ---
axes[0].plot(time, y, color='black', linewidth=2, label='Basque')
axes[0].plot(time, donor_mean, color='blue', linewidth=2, linestyle='--', label='National Mean')
axes[0].set_title("Basque GDP")
axes[0].set_xlabel("Year")
axes[0].set_ylabel("GDP")
axes[0].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism Begins')
axes[0].legend()

# --- Right subplot: observed Basque + donor units ---
# Plot all donors as thin grey lines
for j in range(Y0.shape[1]):
    axes[1].plot(time, Y0[:, j], color='grey', linewidth=0.8, alpha=0.7)

# Plot Basque and donor mean on top
axes[1].plot(time, y, color='black', linewidth=2, label='Basque')
axes[1].plot(time, donor_mean, color='blue', linewidth=2, linestyle='--', label='National Mean')
axes[1].set_title("Basque vs National Donor Mean/Donor Units")
axes[1].set_xlabel("Year")
axes[1].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism Begins')
axes[1].legend()

plt.tight_layout()
plt.show()

```

:::

## Using Least Squares

We could use ordinary least-squares, where the coefficient vector returned to us will be the weights for our control units.

::: {.panel-tabset}

### Notation

- $\mathbf{y}_1 \in \mathbb{R}^{T}$: outcome vector (e.g., GDP per capita) for the treated unit (Basque Country) over all $T$ time periods
- $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$: donor matrix; each column is a control unit, each row is a time period ($J=16$ control units in this case)
- $\mathbf{w} \in \mathbb{R}^{J}$: vector of donor weights where each entry $w_j$ indicates the contribution of control unit $j \in \mathcal{J}_0$ to the weighted average of the treated unit
- $\mathcal{T}_1 = \{1,\dots,T_0\}$: set of pre-treatment periods, with cardinality $|\mathcal{T}_1| = T_0$
- $\mathcal{T}_2 = \{T_0+1,\dots,T\}$: set of post-treatment periods, with cardinality $|\mathcal{T}_2| = T - T_0$
- $\mathcal{J}_0 = \{1,\dots,J\}$: set of untreated donor units, with cardinality $|\mathcal{J}_0| = J$

### Data

Here is the dataset, literally displayed in preparation for regression. We can see the Basque column as well as the donor columns. We wish to take a weighted average of these control unit columns to well approximate the Basque Country in the preintervention period, such that the weighted average provides a counterfactual estimate for the Basque Country in the post-intervention period.

*Reminder: a weighted average can be written as*  

$$
\hat{y}_{1t} = \mathbf{Y}_0 \mathbf{w} = \sum_{j \in \mathcal{J}_0} w_jy_{jt}, \qquad t \in \mathcal{T}_1 \cup \mathcal{T}_2.
$$

```{python}

# Extract Ywide
Ywide = prepped_data["Ywide"]

# First 3 rows, first 10 columns
subset = Ywide.iloc[:10, :10]

# Convert to markdown
md_table = subset.to_markdown(index=True)

# Display
display(Markdown(md_table))


```
---

### OLS Problem

- We can choose weights to minimize the squared pre-intervention difference between Basque and the weighted donors:  

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^J}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2  \quad \forall \, t \in \mathcal{T}_{1}
$$

- In practice, this is just OLS on the pre-1975 period, where $\mathbf{y}_1$ is the Basque GDP column and $\mathbf{Y}_0$ are the columns for the donor pool. The regression coefficients (what we call the betas ($\beta$) in our introductory econometrics classes) are the weights.

- Once we have $\mathbf{w}^{\ast}$, we multiply all the values for each control unit by its regression coefficient and sum these across donors.  

- This gives the predicted Basque trajectory pre- and post-intervention.

---

### Treatment Effect

- First, define the period-level treatment effect as the difference between the observed outcome and the synthetic control prediction:

$$
\hat{\tau}_{1t} = y_{1t} - \hat{y}_{1t}, 
\qquad t \in \mathcal{T}_2
$$

- **Total treatment effect** (sum across all post-treatment periods):

$$
\text{Total Treatment Effect (TTE)} = \sum_{t \in \mathcal{T}_2} \hat{\tau}_{1t}
= \sum_{t \in \mathcal{T}_2} \Big( y_{1t} - \hat{y}_{1t} \Big)
$$

- **Average treatment effect on the treated (ATT)**, the mean of the period-level effects:

$$
\text{ATT} = \frac{1}{T_2} \sum_{t \in \mathcal{T}_2} \hat{\tau}_{1t}
= T_{2}^{-1}\sum_{t \in \mathcal{T}_2} \Big( y_{1t} - \hat{y}_{1t} \Big)
$$

- **Percentage treatment effect (PTE)** relative to the counterfactual mean:

$$
\%\text{Treatment Effect} = 
\frac{\frac{1}{T_2} \sum_{t \in \mathcal{T}_2} \hat{\tau}_{1t}}{T_{2}^{-1}\sum_{t \in \mathcal{T}_2} \hat{y}_{1t}} \times 100
$$

*Intuition:*  

- $\hat{\tau}_{1t}$ shows the effect at each post-treatment time point.  

- ATT averages these effects to summarize the overall post-treatment deviation. 

- Total treatment effect aggregates the deviations into a single cumulative number.

- Percentage treatment effect scales the ATT relative to the counterfactual, giving a sense of magnitude in context.


:::

---

## Least Squares Cont.

::: {.panel-tabset}

### Our Weights

Here are the weights OLS returns to us for the 16 donor units:

```{python}
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Define CVXPY variable for donor weights ---
J = Y0.shape[1]
w = cp.Variable(J)

# --- Define unconstrained OLS problem on pre-intervention ---
objective = cp.Minimize(cp.sum_squares(y[:T0] - Y0[:T0, :] @ w))
problem = cp.Problem(objective)
problem.solve()

# --- Extract weights ---
weights = w.value

# --- Predict outcomes using weights ---
y_pred = Y0 @ weights

# --- Compute ATT (mean post-treatment difference) ---
att = np.mean(y[T0:] - y_pred[T0:])

# --- Get donor names ---
donor_names = prepped_data['donor_names']

# --- Build DataFrame ---
weights_df = pd.DataFrame(weights, index=donor_names, columns=["Weight"])

md_table = weights_df.to_markdown(index=True)

display(Markdown(md_table))
```

Notice that OLS assigns some positive and some negative weights. These tell us how similar the control units is to the treated unit. Notice how some units have negative weights. These can be hard to interpret substantively: which units are truly most similar to the Basque Country before terrorism happened? Is it Andalucia or Castilla Y Leon?

### Plotting the Donors

Here are some of the predicted donors plotted against the Basque Country


```{python}


import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0 = Ywide_data[["Andalucia", "Castilla Y Leon"]].values
time = prepped_data["time_labels"]   # actual time labels
T0 = prepped_data["pre_periods"]     # number of pre-treatment periods


import matplotlib.pyplot as plt

# Extract series
basque = Ywide_data["Basque"].values
andalucia = Ywide_data["Andalucia"].values
castilla = Ywide_data["Castilla Y Leon"].values
cataluna = Ywide_data["Cataluna"].values
plt.plot(time, basque, color="black", linewidth=2, label="Basque Country")
plt.plot(time, andalucia, color="blue", linestyle="--", label="Andalucia (w ≈ 4.18)")
plt.plot(time, castilla, color="green", linestyle="--", label="Castilla Y Leon (w ≈ 6.08)")
plt.plot(time, cataluna, color="#B22222", linestyle="--", label="Cataluna (w ≈ -0.95)")

# Mark intervention
plt.axvline(x=time[T0], color="grey", linestyle="--", label="Terrorism Begins")

# Labels
plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque Country vs Selected Donors")
plt.legend()
plt.show()
```

Much of this does not really square very nicely with intution. Cataluna, which is very close to the Basque Country (both literally in terms of GDP and being a French border region in Northern Spain) gets **negative** weight from OLS. The units which are much farther away from the Basque Country (on top of having much different political/economic histories) get much more weight

### Plotting Predictions

Here is our prediction using OLS. Unfortunately, while OLS fits very well to the Basque Country before terrorism happened, the out-of-sample/post-1975 predicions exhibits a lot more variance than any control unit in the pre-treatment period did. The counterfactual has a weird zig-zag-y prediction. We can see that the prediction line for the Basque country suggests that its economy would have fallen off a cliff, had terrorism **not** happened. In other words, if the OLS estmate is to be taken seriously, the onset of terrorism saved the Basque economy... This does not seem like a sensible finding historically, or a very reasonable prediction statistically.

```{python}
# --- Plot results ---
plt.figure(figsize=(10,6))
plt.plot(time, y, label="Observed Basque", color='black')
plt.plot(time, y_pred, label="OLS Basque", color='blue', linestyle='--')
plt.axvline(x=time[T0], color='red', linestyle='-', label='Terrorism Begins')
plt.xlabel("Year")
plt.ylabel("GDP")
plt.title(f"Effect of Terrorism in the Basque Country (ATT = {att:.3f})")
plt.legend()
plt.show()
```

:::

## How To Change The Weights

- What if we could adjust the weights so the synthetic Basque stays within the range of the donor units?  

- In other words, we want the model predictions to lie inside the full range of the donors only.  

- The plot below shows the area spanned by all donor regions in Spain (shaded), along with the Basque Country. This visualizes the full feasible range for any weighted average of donors.

```{python}

import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']


# Compute min and max across donors at each time point
donor_min = Y0.min(axis=1)
donor_max = Y0.max(axis=1)

plt.figure(figsize=(10,6))

# Shade the convex hull (area between min and max)
plt.fill_between(time, donor_min, donor_max, color='lightgrey', alpha=0.4, label="Donor Span")

# Plot Basque GDP
plt.plot(time, y, color='black', linewidth=2, label='Observed Basque')
# Intervention line
plt.axvline(x=time[T0], color='red', linestyle='-', linewidth=2, label='Terrorism Begins')

plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque GDP vs Donor Span")
plt.legend()
plt.show()


```

---


## Classic Synthetic Control Model

The classic SCM estimator solves the following ordinary least-squares regression problem:

$$
\begin{aligned}
\mathbf{w}^{\mathrm{SCM}} 
&= \underset{\mathbf{w}^{\mathrm{SCM}} \in \mathcal{W}_{\mathrm{conv}}}{\operatorname*{argmin}} \; 
\left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w}^{\mathrm{SCM}} \right\|_2^2 \quad \forall \, t \in \mathcal{T}_{1}, \\
\mathcal{W}_{\mathrm{conv}} 
&= \left\{ \mathbf{w}^{\mathrm{SCM}} \in \mathbb{R}_{\ge 0}^{J} \;\middle|\; \mathbf{1}^\top \mathbf{w} = 1 \right\}
\end{aligned}
$$

We want to combine the donor regions to best match the treated unit before the intervention. By requiring all weights to be nonnegative and sum to 1, the synthetic Basque is a weighted average that stays inside the range of the donors—it can’t "overshoot" or assign negative importance to any region.

Given the optimal weights $\mathbf{w}^{\mathrm{SCM}}$, the SCM prediction for the treated unit is

$$
\hat{\mathbf{y}}_1^{\mathrm{SCM}} = \mathbf{Y}_0 \mathbf{w}^{\mathrm{SCM}}\equiv\sum_{j \in \mathcal{J}_0} w_j^{\mathrm{SCM}} \, y_{jt}, \quad \forall \, t \in \mathcal{T}
$$
---

## SCM for Basque

- Here is the result of the regression model. The Basque Country is best reproduced by 85% Catalonia and 15% Madrid.

```{python}

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0 = Ywide_data[["Cataluna", "Madrid (Comunidad De)"]].values
time = prepped_data["time_labels"]  # actual time labels
T0 = prepped_data["pre_periods"]  # number of pre-treatment periods

# --- Weights ---
w_final = np.array([0.85, 0.15])
w_start = np.array([0.1, 0.9])
steps = 50
weights_history = np.array([
    w_start + (w_final - w_start) * t / (steps - 1) for t in range(steps)
])

# --- Figure setup ---
fig, ax = plt.subplots(figsize=(12, 6))

line_y, = ax.plot(time, y, label="Observed Basque", color="black", linewidth=2)
line_hatY, = ax.plot(time, w_start[0] * Y0[:, 0] + w_start[1] * Y0[:, 1],
                     label="Convex Basque", color="blue", linestyle="--", linewidth=2)
line_cataluna, = ax.plot(time, Y0[:, 0], label="Catalonia", color="red", alpha=0.5)
line_madrid, = ax.plot(time, Y0[:, 1], label="Madrid", color="blue", alpha=0.5)

# Vertical reference line at treatment
ax.axvline(x=time[T0], color="black", linestyle="--", label="Terrorism Begins")
ax.set_xlabel("Year")
ax.set_ylabel("GDP per Capita")
ax.set_ylim(y.min() - 1, y.max() + 1)
ax.legend()

# Initialize shaded ATT variable
att_shade = None


# --- Animation function ---
def update(frame):
    global att_shade

    # Current weights and synthetic prediction
    w_curr = weights_history[frame]
    y_hat = w_curr[0] * Y0[:, 0] + w_curr[1] * Y0[:, 1]
    line_hatY.set_ydata(y_hat)

    # Remove old shading if it exists
    if att_shade is not None:
        att_shade.remove()

    # Update shaded ATT region dynamically
    att_shade = ax.fill_between(
        time[T0:], y[T0:], y_hat[T0:], color="gray", alpha=0.3, label="ATT Gap"
    )

    # Pre-treatment RMSE
    rmse_pre = np.sqrt(np.mean((y[:T0] - y_hat[:T0]) ** 2))

    # Update title dynamically
    if frame == steps - 1:
        # Compute ATT
        att = np.mean(y[T0:] - y_hat[T0:])
        line_cataluna.set_visible(False)
        line_madrid.set_visible(False)
        ax.set_title(
            f"Final SCM: w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}, "
            f"Pre-RMSE={rmse_pre:.3f}, ATT={att:.2f}"
        )
    else:
        ax.set_title(
            f"w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}, "
            f"Pre-RMSE={rmse_pre:.3f}"
        )

    return line_hatY, line_cataluna, line_madrid, att_shade


# --- Animate with repeated final frame ---
extra_frames = 50  # hold final frame ~5 seconds at 10 fps
all_frames = list(range(steps)) + [steps - 1] * extra_frames

ani = FuncAnimation(fig, update, frames=all_frames, blit=False)
ani.save("scm_weighted_combo.gif", writer=PillowWriter(fps=10))

plt.close(fig)




```

![Convex Combination](scm_weighted_combo.gif)

## Inference: Placebos

Validating SCM designs are important as well, Consider a few robustness checks:

::: {.panel-tabset}

### In Time

ideally, shifting the treatement back in time should not change the practical results of the conclusions very much.

```{python}

import pandas as pd
from mlsynth import CLUSTERSC

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)

# Identify columns
treat_col = data.columns[-1]   # treatment dummy
unit_col = data.columns[0]     # region/unit column (assuming first col is "regionname")
year_col = data.columns[1]     # year column

# Update treatment: 1 if Basque and year >= 1969, else 0
data[treat_col] = ((data[unit_col] == "Basque") & (data[year_col] >= 1973)).astype(int)


config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["red"], "method": "PCR"}

arco = CLUSTERSC(config).fit()

```



### In Space

ideally, the treated unit's treatment effect should look extreme compared to the placebo effect given to other untreated units. Here, we rerun the SCM over the other donor units.

```{python}

import pandas as pd
from mlsynth import CLUSTERSC

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)

# Identify columns
unit_col = data.columns[0]   # region
year_col = data.columns[1]   # year
treat_col = data.columns[-1] # treatment dummy
outcome_col = data.columns[2]

# Function to build placebo datasets
def make_placebo_datasets(data, unit_col, year_col, treat_col, treat_start_year):
    placebo_dfs = {}
    for unit in data[unit_col].unique():
        df_copy = data.copy()
        df_copy[treat_col] = ((df_copy[unit_col] == unit) &
                              (df_copy[year_col] >= treat_start_year)).astype(int)
        placebo_dfs[unit] = df_copy
    return placebo_dfs

# Generate datasets
placebo_datasets = make_placebo_datasets(
    data,
    unit_col=unit_col,
    year_col=year_col,
    treat_col=treat_col,
    treat_start_year=1975
)

# Loop through each placebo dataset and fit RESCM
results = {}
for unit, df_placebo in placebo_datasets.items():
    config = {
        "df": df_placebo,
        "outcome": outcome_col,
        "treat": treat_col,
        "unitid": unit_col,
        "time": year_col,
        "display_graphs": False,
        "save": False, "method": "PCR", "cluster": False, "objective": "SIMPLEX"
    }
    model = CLUSTERSC(config).fit()
    results[unit] = model

all_gaps = []

for unit, model in results.items():
    # This is a numpy array; take first column
    gap_vec = model.sub_method_results['PCR'].time_series.estimated_gap[:, 0]

    df_gap = pd.DataFrame({
        "unit": unit,
        "time": model.sub_method_results['PCR'].time_series.estimated_gap[:, 1],
        "gap": gap_vec
    })
    all_gaps.append(df_gap)

all_gaps_df = pd.concat(all_gaps, ignore_index=True)


import matplotlib.pyplot as plt

# Plot all placebo gaps first (thin grey lines)
for unit in all_gaps_df['unit'].unique():
    if unit == "Basque":
        continue
    subset = all_gaps_df[all_gaps_df['unit'] == unit]
    plt.plot(subset['time'], subset['gap'], color='grey', alpha=0.5, linewidth=0.8)

# Plot Basque gap (thick black line)
basque_subset = all_gaps_df[all_gaps_df['unit'] == "Basque"]
plt.plot(basque_subset['time'], basque_subset['gap'], color='black', linewidth=3, label='Basque')

plt.xlabel("Year")
plt.ylabel("Estimated Gap")
plt.title("In-space Placebo Gaps")
plt.legend()
plt.show()
```

:::

## Block Permutation Inference

A more modern approach however is block permutation. Block permutation accounts for temporal dependence in pre-treatment residuals.

::: {.panel-tabset}

### Residuals

Pre-treatment residuals:

$$
e_t = y_{1t} - \hat{y}_{1t}, \quad t \in \mathcal{T}_1
$$

Post-treatment treatment effects:

$$
\hat{\tau}_{1t} = y_{1t} - \hat{y}_{1t}, \quad t \in \mathcal{T}_2
$$

Full-period effects:

$$
\hat{\tau}_{1t}^{\text{full}} = y_{1t} - \hat{y}_{1t}, \quad t \in \{1, \dots, T\}
$$

### Block Size

Lag-$k$ autocorrelation of pre-treatment residuals:

$$
\text{ACF}(k) = \frac{\sum_{t=1}^{T_0-k} (e_t - \bar{e})(e_{t+k}-\bar{e})}{\sum_{t=1}^{T_0} (e_t-\bar{e})^2}
$$

Block size $B$ is chosen as the smallest lag such that $|\text{ACF}(B)| < c$ where $c$ is a positive constant

Number of blocks:

$$
N_B = \left\lfloor \frac{T_0}{B} \right\rfloor
$$

$$
\mathbf{e}^{(b)} = \{ e_{(b-1)B+1}, \dots, e_{bB} \}, \quad b=1,\dots,N_B
$$

$$
S_{\text{obs}} = \frac{1}{T_2}\sum_{t \in \mathcal{T}_2} |\hat{\tau}_{1t}|
$$

### Permutation

For $m = 1, \dots, M$ permutations:

$$
S^{*(m)} = \frac{1}{T_2} \sum_{t \in \mathcal{T}_2} \big| \mathbf{e}^{*(m)}_t \big|
$$

where $\mathbf{e}^{*(m)}$ is a concatenation of randomly sampled blocks with replacement.

### P-Value

Global p-value:

$$
p_{\text{global}} = \frac{1}{M} \sum_{m=1}^{M} \mathbf{1}\big(S^{*(m)} \ge S_{\text{obs}}\big)
$$

Per-period p-values:

$$
p_t = \frac{1}{T_0} \sum_{s \in \mathcal{T}_1} \mathbf{1}\big(|e_s| \ge |\hat{\tau}_{1t}|\big), \quad t \in \mathcal{T}_2
$$

### Confidence Intervals

$$
q_{1-\alpha} = \text{Quantile}_{1-\alpha}(|e_t|, t \in \mathcal{T}_1)
$$

$$
\text{CI}_t = \big[\hat{\tau}_{1t} - q_{1-\alpha}, \, \hat{\tau}_{1t} + q_{1-\alpha}\big], \quad t \in \mathcal{T}_2
$$

:::

## Block Results

```{python}

import pandas as pd # To work with panel data
from mlsynth import FSCM # The method of interest
import numpy as np


def block_permutation_inference(Y_treated, Y_control, T0, alpha=0.05, max_combinations=2000, acf_threshold=0.1,
                                random_state=None):
    """
    Compute synthetic control inference using block permutation.

    Parameters
    ----------
    Y_treated : np.ndarray
        Treated unit outcomes (1D array, length T).
    Y_control : np.ndarray
        Synthetic control outcomes (1D array, length T).
    T0 : int
        Index separating pre-treatment (0..T0-1) from post-treatment (T0..T-1).
    alpha : float
        Significance level for split-conformal confidence intervals.
    max_combinations : int
        Number of permutations to run.
    acf_threshold : float
        Threshold for autocorrelation to determine block size.
    random_state : int or None
        Random seed for reproducibility.

    Returns
    -------
    results : dict
        Dictionary containing:
        - treated_effects: post-treatment treatment effects
        - fulltreated_effects: all-period treatment effects
        - S_obs: observed global statistic
        - global_p_value: p-value from block permutation
        - per_period_pvals: per-period p-values
        - CI: split-conformal confidence intervals
        - block_size: estimated block size
    """
    rng = np.random.default_rng(random_state)
    T = len(Y_treated)

    # --- Compute residuals ---
    residuals = Y_treated[:T0] - Y_control[:T0]
    treated_effects = Y_treated[T0:] - Y_control[T0:]
    fulltreated_effects = Y_treated - Y_control

    # --- Estimate block size from autocorrelation ---
    def estimate_block_size(residuals, threshold):
        acf_vals = [1.0]  # lag 0
        n = len(residuals)
        mean_resid = np.mean(residuals)
        var_resid = np.var(residuals)
        for lag in range(1, n):
            cov = np.mean((residuals[:n - lag] - mean_resid) * (residuals[lag:] - mean_resid))
            acf_vals.append(cov / var_resid)
            if abs(cov / var_resid) < threshold:
                return lag
        return n // 2  # fallback

    block_size = estimate_block_size(residuals, acf_threshold)

    # --- Prepare blocks ---
    n_blocks = len(residuals) // block_size
    block_indices = np.arange(n_blocks)

    # --- Block permutation test ---
    T_post = len(treated_effects)
    S_obs = np.mean(np.abs(treated_effects))

    S_perm = []
    for _ in range(max_combinations):
        sampled_blocks = rng.choice(block_indices, size=(T_post // block_size + 1), replace=True)
        permuted = np.concatenate([
            residuals[b * block_size:(b + 1) * block_size] for b in sampled_blocks
        ])[:T_post]
        S_perm.append(np.mean(np.abs(permuted)))
    S_perm = np.array(S_perm)

    global_p_value = np.mean(S_perm >= S_obs)

    # --- Per-period p-values ---
    per_period_pvals = np.mean(
        np.abs(residuals)[:, None] >= np.abs(treated_effects)[None, :],
        axis=0
    )

    # --- Split-conformal confidence intervals ---
    q = np.quantile(np.abs(residuals), 1 - alpha)
    interval = np.column_stack([treated_effects - q, treated_effects + q])
    CI = np.vstack([np.full((T0, 2), np.nan), interval])

    return {
        "treated_effects": treated_effects,
        "fulltreated_effects": fulltreated_effects,
        "S_obs": S_obs,
        "global_p_value": global_p_value,
        "per_period_pvals": per_period_pvals,
        "CI": CI,
        "block_size": block_size
    }


url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"

# Feel free to change "smoking" with "basque" above in the URL

data = pd.read_csv(url)

# Our method inputs

config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": False,
    "save": False,
    "counterfactual_color": ["red"]}

arco = FSCM(config).fit()


obs = arco.raw_results['Vectors']['Observed Unit']

cf = arco.raw_results['Vectors']['Counterfactual']

T0 = arco.raw_results['Fit']['Pre-Periods']


inference = block_permutation_inference(obs,cf,T0)

import matplotlib.pyplot as plt
import numpy as np


def plot_scm_treatment_effect(obs, cf, T0, CI=None, alpha=0.05, title_prefix="Treatment Effect Plot",
                              style_params=None):
    """
    Plot SCM treatment effects with in-sample residuals and confidence intervals.

    Parameters
    ----------
    obs : np.ndarray
        Observed unit outcomes (1D array, length T).
    cf : np.ndarray
        Counterfactual / synthetic control outcomes (1D array, length T).
    T0 : int
        Pre-treatment period length (index separating pre- and post-treatment).
    CI : np.ndarray, optional
        Split-conformal or other confidence intervals, shape (T, 2). Default None.
    alpha : float
        Significance level for CI (default 0.05).
    title_prefix : str
        Title prefix for the plot.
    style_params : dict, optional
        Matplotlib styling parameters. Default is a preset style.

    Returns
    -------
    fig, ax : matplotlib Figure and Axes
        The created figure and axes.
    """
    T = len(obs)
    time = np.arange(T)

    # --- Compute in-sample residuals and post-treatment effects ---
    residuals = obs[:T0] - cf[:T0]
    treated_effects = obs[T0:] - cf[T0:]

    # --- Compute ATT and global CI ---
    ATT = np.mean(treated_effects)
    q_global = np.quantile(np.abs(residuals), 1 - alpha)
    global_CI_lower = ATT - q_global
    global_CI_upper = ATT + q_global

    # --- Default styling ---
    if style_params is None:
        style_params = {
            "figure.figsize": (10, 6),
            "figure.dpi": 100,
            "axes.spines.top": False,
            "axes.spines.right": False,
            "font.size": 14,
            "font.family": "sans-serif",
            "axes.titlesize": 18,
            "axes.titleweight": "bold",
            "axes.labelsize": "large",
            "xtick.labelsize": "medium",
            "ytick.labelsize": "medium",
            "legend.fontsize": 10,
            "axes.grid": True,
            "axes.axisbelow": True,
            "grid.color": "#d3d3d3",
            "grid.linestyle": ":",
            "grid.linewidth": 1.0,
            "lines.linewidth": 1.0,
        }
    plt.rcParams.update(style_params)

    # --- Create plot ---
    fig, ax = plt.subplots(figsize=(style_params["figure.figsize"]))

    # In-sample residuals
    ax.plot(time[:T0], residuals, color="black", linewidth=1.5, label="In-Sample Residuals")

    # Post-treatment effects
    ax.plot(time[T0:], treated_effects, color="black", linewidth=2, label="Treatment Effect")

    # Confidence interval
    if CI is not None:
        ax.fill_between(time[T0:], CI[T0:, 0], CI[T0:, 1], color="#dbe1dd", alpha=0.7,
                        label=f"{100 * (1 - alpha):.0f}% CI")

    # Treatment start marker
    ax.axvline(x=T0 - 1, color="Red", linestyle="--", label="Treatment Begins")

    # Labels and legend
    ax.set_xlabel("Time")
    ax.set_ylabel("Effect on Outcome")
    ax.set_title(
        f"{title_prefix} | ATT: {ATT:.2f} | 95% CI: [{global_CI_lower:.2f}, {global_CI_upper:.2f}]"
    )
    ax.legend()

    plt.show()

    return fig, ax

# --- Usage example ---
fig, ax = plot_scm_treatment_effect(obs, cf, T0, CI=inference["CI"])

```


## [Key Takeaways for Classic SCM](https://arxiv.org/pdf/2203.06279)

- **Low-volatility series**  
  Synthetic controls work best with aggregate, relatively smooth series.  
  *Condition (informal):* the pre-treatment volatility should be small relative to the signal, e.g.
  $$
  \frac{\operatorname{sd}(y_{1t \in \mathcal{T}_{1}})}{\overline{y}_{1t \in \mathcal{T}_{1}}} \quad\text{is small.}
  $$

- **Extended pre-intervention period**  
  A long pre-treatment window improves identification.  
  *Condition:* the number of pre-periods $T_0$ should be sufficiently large:
  $$
  T_0 \gg r \quad\text{(where $r$ is the effective number of latent factors).}
  $$

- **Small, relevant donor pool**  
  More donors can increase interpolation bias and overfitting. Prefer a smaller donor set.  
  *Condition (selection):* restrict donor count $J$ relative to pre-period information, e.g.
  $$
  J \lesssim c \cdot T_0 \quad\text{for some modest constant } c,
  $$

- **Sparsity aids interpretability**  
  Fewer nonzero weights make the synthetic control easier to interpret.  
  *Condition:* prefer weight vectors $\mathbf{w}$ with small support:
  $$
  \|\mathbf{w}\|_0 << J \quad\text{or use the } \ell_1 \text{ norm: } \|\mathbf{w}\|_1
  $$

- **Plot your treated unit versus the donors (seriously)**  
  Visual diagnostics: check level/trend alignment and extreme donors before fitting.  
  *Diagnostic:* inspect $\{y_{jt}\}_{j\in \mathcal{J}_{0}}$ vs.\ $y_{1t}$ for $t \in \mathcal{T}_{1}$.

- **Good pre-treatment fit matters**  
  If pre-fit error is large, post-treatment inferences are unreliable.  
  *Condition (quantitative):* require a low pre-fit RMSE:
  $$
  \text{RMSE}_{t \in \mathcal{T}_{1}} = \sqrt{\frac{1}{T_0}\sum_{t \in \mathcal{T}_{1}}\big(y_{1t}-\hat y_{1t}\big)^2}
  $$

- **Out-of-sample validation is essential**  
  Use placebo tests (in-time and in-space) to check robustness.  
  *Condition (placebo):* the treated unit's post-treatment gap should be an outlier relative to the placebo distribution. If $\widehat{\text{ATT}}_1$ is the treated ATT and $\{\widehat{\text{ATT}}_j\}_{j\neq 1}$ are placebo ATTs,
  $$
  p\text{-value} \approx \frac{1}{J}\sum_{j\neq 1} \mathbf{1}\big(|\widehat{\text{ATT}}_j|\ge |\widehat{\text{ATT}}_1|\big)
  $$
  should be small. Furthermore, the ATT for the real treatment period should not change substantively given *reasonable* in-time placebos.

## Extensions

Synthetic Control methods have undergone many advances since their inception. If you wish to use Python to estimate synthetic controls, I wrote the largest such package that exists. See here for details:

```{python}
#| echo: false
#| fig-align: center

import qrcode
from PIL import Image
import matplotlib.pyplot as plt

# Generate QR code
url = "https://jgreathouse9.github.io/mlsynth/webdir/mlsynthlanding.html"

# Display the QR code in Quarto
plt.imshow(qrcode.make(url), cmap="gray")
plt.axis("off")  # Hide axes
plt.show()

```
