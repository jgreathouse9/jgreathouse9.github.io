---
title: "The Synthetic Control Method"
date: today
author:
  - name: "Jared Greathouse"
    url: "https://jgreathouse9.github.io"
format:
  revealjs:
    theme: sky
    slide-number: true
    transition: fade
    scrollable: true
    background-color: "#EAF7F1"
highlight-style: github
---

## Synthetic Controls

- This is the lecture on synthetic control methodologies (SCM). SCM is a quasi-experimental method we can use to estimate the causal impact of policies, marketing campaigns, or other treatments we care about.

- SCM has its roots in the comparative case study framework, where a single/few units are exposed to a treatment and we wish to compare the treated units to those that were not treated in order to draw inferences/conclusions about the effect of the treatment/policy.

## What Is An Average?

Before we cover SCM, we need to be very clear about what an average is.

::: {.panel-tabset}

### Scalars

- The arithmetic mean of a finite set of numbers  $S = \{x_1, x_2, \dots, x_n\} \subset \mathbb{R}$ is:  

$$
\bar{x} = \frac{1}{n} \sum_{x \in S} x
= \tfrac{x_1 + x_2 + \cdots + x_n}{n}.
$$

- For example, say you have 12 apples and your friend has 18. This can be called set $A = \{12, 18\} \subset \mathbb{R}$:

$$
15 = \bar{x} = \frac{1}{2} \sum_{x \in \{12,18\}} x
= (\tfrac{1}{2} \cdot 12) + (\tfrac{1}{2} \cdot 18)
= \frac{12 + 18}{2}.
$$

On average, you have 15 apples.

---

### Vectors

- We can also write the same example as a row vector: 

$$
\mathbf{x} = \begin{bmatrix} 12 & 18 \end{bmatrix} \in \mathbb{R}^{1 \times 2}.
$$

Here, each column entry represents a person’s apples . 

- To compute the average using linear algebra, we use weights. Since there are two people, each gets weight $1/2$: 

$$
\mathbf{w} = \begin{bmatrix} \frac{1}{2} \\[2mm] \frac{1}{2} \end{bmatrix} \in \mathbb{R}^{2 \times 1}.
$$

- Then, the average is just a weighted sum (technically called the *dot product*), which in vector form is: 

$$
\bar{x} = \mathbf{x}\mathbf{w} 
= 
\begin{bmatrix} 12 & 18 \end{bmatrix}
\begin{bmatrix} \tfrac{1}{2} \\[1mm] \tfrac{1}{2} \end{bmatrix}
= 12 \cdot \tfrac{1}{2} + 18 \cdot \tfrac{1}{2} = 15.
$$
See? We still have 15 apples, on average.

---

### Matrices

- Let’s extend the idea of averaging to multiple time periods. Suppose today you have 12 apples and tomorrow 30, while your friend has 18 today and 22 tomorrow. We can put this into a matrix, where each row is a time period and each column is a person:

$$
\mathbf{Y} =
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\in \mathbb{R}^{2 \times 2}.
$$

- Each row represents a snapshot in time, and each column represents a person.

- To find the average number of apples per time period, we use the same weights as before (1/2 for each person). Multiplying the matrix by the weight vector gives a row-wise weighted sum:

$$
\mathbf{w} = 
\begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix}, \quad
\bar{\mathbf{x}} = \mathbf{Y} \mathbf{w} 
=
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix} 
=
\begin{bmatrix} 12\cdot 1/2 + 18\cdot 1/2 \\ 30\cdot 1/2 + 22\cdot 1/2 \end{bmatrix} 
=
\begin{bmatrix} 15 \\ 26 \end{bmatrix}.
$$

- Interpretation: Today, the average is 15 apples; tomorrow, the average is 26 apples.  

- This shows that row-wise averaging is just taking a weighted sum across the columns for each row. Each row "collapses" into a single number representing the average for that time period.  

- This concept generalizes to any time-series or panel data, like GDPs, incomes, or other outcomes measured across multiple units.


:::

## What Are We Weighting For? Introducing Weighted Averages

::: {.panel-tabset}

### Simple Averages

You are at a bar with your friend. You earn \$60k/year. Your friend earns \$70k/year. The simple average of your incomes is:

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \begin{bmatrix} 60 & 70 \end{bmatrix} 
\begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix} 
= \frac{1}{2}\cdot 60 + \frac{1}{2}\cdot 70 
= 65 \text{ (k/year)}
$$

Now, suddenly Bill Gates walks in, with a net worth of \$107B/year. Taking the simple average of all three income levels:  

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{3} \\[1mm] \frac{1}{3} \\[1mm] \frac{1}{3} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \frac{1}{3}\cdot 60 + \frac{1}{3}\cdot 70 + \frac{1}{3}\cdot 1.07\times 10^{11}
\approx 35.7 \text{ billion/year}
$$

Clearly, treating Bill Gates the same as the others skews the average.

- If the bartender next day bragged about earning 50k that night, we'd say: "It's not that you earned 50k organically; a wealthy dude happened to be there that night. We can't use this as evidence that your business is booming."  

- In other words, if we want a meaningful "bar average," Bill should likely get much less weight than the other patrons.

---

### Weighted Average

Instead of treating Bill as equal to you two, we can give each person a weight $w_i$ to decide how much they count in the average. This is exactly the fraction we used before:  

$$
\bar{x}_{\text{weighted}} = \sum_{i=1}^{n} w_i x_i, \quad \sum_{i=1}^{n} w_i = 1.
$$

Here, we compute a weighted average where Bill can get a tiny weight, and you and your friend get larger weights, giving a more reasonable "average bar income."

---

### Weighted Average Cont.

Suppose we only care about you and your friend, and ignore Bill Gates (weight = 0):

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} 0.5 \\[1mm] 0.5 \\[1mm] 0 \end{bmatrix}
$$

Then the weighted average is:

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= 0.5\cdot 60 + 0.5\cdot 70 + 0 \cdot 1.07\times 10^{11}
= 65 \text{ (k/year)}
$$

See how the average is as it was at first when it was just you and your friend? Bill is clearly not representative of a bar or even a country. Giving him as much weight as regular humans does not make sense.

:::

## How to Choose the Weights?

- In the earlier examples, the weights we used were arbitrary. We split 50/50 between you and your friend, but there was no real reason for that choice. In real life, weights are usually unknown — and assigning them poorly can create nonsense averages.  

- Think back to the bar example: if Bill Gates walks in, and we give him the same weight as everyone else, the “average income” of the bar will be absurdly high. To represent the *bar’s patrons*, Gates should either receive a much smaller weight or none at all.  

- Suppose we have one unit exposed to terrorism in 1975 (the Basque Country), and a set of 16 other units that are not exposed. We want to estimate the effect of terrorism on GDP per capita via taking a weighted average of control units, but how to choose the weights?

- Optimization solves this problem. Instead of us just making up weights, we *choose the weights that minimize the pre-intervention gap* between the Basque Country and the weighted donor units. Units that closely track Basque GDP get higher weights; dissimilar ones get lower or zero weights.  

```{python}
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown
import matplotlib

# matplotlib theme
jared_theme = {'axes.grid': True,
              'grid.linestyle': '--',
              'legend.framealpha': 1,
              'legend.facecolor': 'white',
              'legend.shadow': True,
              'legend.fontsize': 14,
              'legend.title_fontsize': 16,
              'xtick.labelsize': 14,
              'ytick.labelsize': 14,
              'axes.labelsize': 16,
              'axes.titlesize': 20,
              'figure.dpi': 100}

matplotlib.rcParams.update(jared_theme)

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Create figure with 2 subplots ---
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# --- Left subplot: observed Basque only ---
axes[0].plot(time, y, color='black', linewidth=2, label='Basque')
axes[0].set_title("Observed Basque GDP")
axes[0].set_xlabel("Year")
axes[0].set_ylabel("GDP")
axes[0].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism Begins')
axes[0].legend()

# --- Right subplot: observed Basque + donor units ---
# Plot all donors as thin grey lines
for j in range(Y0.shape[1]):
    axes[1].plot(time, Y0[:, j], color='grey', linewidth=0.8, alpha=0.7)

# Plot Basque on top
axes[1].plot(time, y, color='black', linewidth=2, label='Basque')
axes[1].set_title("Basque vs Donor Units")
axes[1].set_xlabel("Year")
axes[1].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism Begins')
axes[1].legend()

plt.tight_layout()
plt.show()
```

## Using Least Squares

One method of optimization we could use to do this is least-squares in the form of a linear regression model, which economists are quite familiar with. In multiple linear regression, the coefficients returned to us can be interpreted as weights for our control group.

::: {.panel-tabset}

### Notation

- $\mathbf{y}_1 \in \mathbb{R}^{T}$: column vector of outcomes (GDP per capita) for the treated unit (Basque Country) over $T$ time periods  
- $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$: donor matrix; each column is a control unit, each row is a time period ($J=16$ control units)  
- $\mathbf{w} \in \mathbb{R}^{J}$: vector of weights where each entry indicates how much each control unit contributes to the weighted average version of the Basque Country  

*Reminder: a weighted average can be written as*  

$$
\hat{y}_1 = \mathbf{Y}_0 \mathbf{w} = w_1 y_{1} + w_2 y_{2} + \dots + w_J y_{J}
$$

In this case, we are taking a weighted average of our control units.

### Data

Here is the dataset, literally displayed in preparation for regression

```{python}

# Extract Ywide
Ywide = prepped_data["Ywide"]

# First 3 rows, first 10 columns
subset = Ywide.iloc[:10, :10]

# Convert to markdown
md_table = subset.to_markdown(index=True)

# Display
display(Markdown(md_table))


```
---

### OLS Problem

- We can choose weights to minimize the squared pre-intervention difference between Basque and the weighted donors:  

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^J}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2
$$

- In practice, this is just OLS on the pre-1975 period, where $\mathbf{y}_1$ is the Basque GDP column and $\mathbf{Y}_0$ are the columns for the donor pool. The regression coefficients (what we call the betas ($\beta$) in our introductory econometrics classes) are the weights.

---

### Interpretation

- Once we have $\mathbf{w}^{\ast}$, we compute the weighted average of controls:

$$
\hat{y}_1 = \mathbf{Y}_0 \mathbf{w}^{\ast} = w_1 y_{1} + w_2 y_{2} + \dots + w_J y_{J}
$$

- That is: multiply all the values for each control unit by its regression coefficient and sum across donors.  

- This gives the predicted Basque trajectory pre- and post-intervention.

---

### Treatment Effect

- The average treatment effect on the treated (ATT) can be expressed as the mean difference between observed outcomes and the weighted average in the post-intervention period:

$$
\text{ATT} = \frac{1}{T - T_0} \sum_{t=T_0+1}^{T} \left( y_{1t} - \hat{y}_{1t} \right)
$$

- where $T_0$: last pre-intervention period (1974)  
- $y_{1t}$: observed outcome of the treated unit at time $t$  
- $\hat{y}_{1t}$: OLS prediction at time $t$, in the post-intervention period 

*Intuition:* ATT measures how much the real, treated Basque GDP deviates from the model's prediction (estimated via OLS in this case).

:::

---

## Least Squares Cont.

::: {.panel-tabset}

### Our Weights

Here are the weights OLS returns to us for the 16 donor units:

```{python}
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Define CVXPY variable for donor weights ---
J = Y0.shape[1]
w = cp.Variable(J)

# --- Define unconstrained OLS problem on pre-intervention ---
objective = cp.Minimize(cp.sum_squares(y[:T0] - Y0[:T0, :] @ w))
problem = cp.Problem(objective)
problem.solve()

# --- Extract weights ---
weights = w.value

# --- Predict outcomes using weights ---
y_pred = Y0 @ weights

# --- Compute ATT (mean post-treatment difference) ---
att = np.mean(y[T0:] - y_pred[T0:])

# --- Get donor names ---
donor_names = prepped_data['donor_names']

# --- Build DataFrame ---
weights_df = pd.DataFrame(weights, index=donor_names, columns=["Weight"])

md_table = weights_df.to_markdown(index=True)

display(Markdown(md_table))
```

Notice that OLS assigns some positive and some negative weights. These tell us how similar the control units is to the treated unit. Notice how some units have negative weights. These can be hard to interpret substantively: which units are truly most similar to the Basque Country before terrorism happened? Is it Andalucia or Castilla Y Leon?

### Plotting the Donors

Here are some of the predicted donors plotted against the Basque Country


```{python}


import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0 = Ywide_data[["Andalucia", "Castilla Y Leon"]].values
time = prepped_data["time_labels"]   # actual time labels
T0 = prepped_data["pre_periods"]     # number of pre-treatment periods


import matplotlib.pyplot as plt

# Extract series
basque = Ywide_data["Basque"].values
andalucia = Ywide_data["Andalucia"].values
castilla = Ywide_data["Castilla Y Leon"].values
cataluna = Ywide_data["Cataluna"].values
plt.plot(time, basque, color="black", linewidth=2, label="Basque Country")
plt.plot(time, andalucia, color="blue", linestyle="--", label="Andalucia (w ≈ 4.18)")
plt.plot(time, castilla, color="green", linestyle="--", label="Castilla Y Leon (w ≈ 6.08)")
plt.plot(time, cataluna, color="#B22222", linestyle="--", label="Cataluna (w ≈ -0.95)")

# Mark intervention
plt.axvline(x=time[T0], color="grey", linestyle="--", label="Terrorism Begins")

# Labels
plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque Country vs Selected Donors")
plt.legend()
plt.show()
```

### Plotting

Here is our prediction using OLS. Unfortunately, while OLS fits very well to the Basque Country before terrorism happened, it has a weird zig-zag-y prediction after the treatment happens. We can see that the prediction line for the Basque country suggests that its economy would have fallen off a cliff, had terrorism not happened. In other words, if the OLS estmate is to be taken seriously, terrorism was great for business to the tune of 1500 dollars per person... This does not seem like a sensible finding substantively, or a very reasonable prediciton.

```{python}
# --- Plot results ---
plt.figure(figsize=(10,6))
plt.plot(time, y, label="Observed Basque", color='black')
plt.plot(time, y_pred, label="OLS Basque", color='blue', linestyle='--')
plt.axvline(x=time[T0], color='red', linestyle='-', label='Terrorism Begins')
plt.xlabel("Year")
plt.ylabel("GDP")
plt.title(f"Effect of Terrorism in the Basque Country (ATT = {att:.3f})")
plt.legend()
plt.show()
```

:::

## How To Change The Weights

- What if we could adjust the weights so the synthetic Basque stays within the range of the donor units?  

- In other words, we want the model predictions to lie inside the full range of the donors only.  

- The plot below shows the area spanned by all donor regions in Spain (shaded), along with the Basque Country. This visualizes the full feasible range for any weighted average of donors.

```{python}

import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']


# Compute min and max across donors at each time point
donor_min = Y0.min(axis=1)
donor_max = Y0.max(axis=1)

plt.figure(figsize=(10,6))

# Shade the convex hull (area between min and max)
plt.fill_between(time, donor_min, donor_max, color='lightgrey', alpha=0.4, label="Donor Span")

# Plot Basque GDP
plt.plot(time, y, color='black', linewidth=2, label='Observed Basque')
# Intervention line
plt.axvline(x=time[T0], color='red', linestyle='-', linewidth=2, label='Terrorism Begins')

plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque GDP vs Donor Span")
plt.legend()
plt.show()


```

---


## Classic Synthetic Control Model

The classic SCM estimator solves the following ordinary least-squares regression problem:

$$
\begin{aligned}
\mathbf{w}^{\mathrm{SCM}} 
&= \underset{\mathbf{w}^{\mathrm{SCM}} \in \mathcal{W}_{\mathrm{conv}}}{\operatorname*{argmin}} \; 
\left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w}^{\mathrm{SCM}} \right\|_2^2, \\
\mathcal{W}_{\mathrm{conv}} 
&= \left\{ \mathbf{w}^{\mathrm{SCM}} \in \mathbb{R}_{\ge 0}^{J} \;\middle|\; \mathbf{1}^\top \mathbf{w} = 1 \right\}
\end{aligned}
$$

We are looking for the weight vector $\mathbf{w}^{\mathrm{SCM}}$ that best fits the treated unit's pre-treatment outcomes with a weighted average of donors. The nonnegativity and summation to one constraints, known as the "simplex", mean the synthetic control must lie inside the support of the donor pool.

---

## SCM for Basque

- Here is the result of the regression model. The Basque Country is best reproduced by 85% Catalonia and 15% Madrid.

```{python}

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0 = Ywide_data[["Cataluna", "Madrid (Comunidad De)"]].values
time = prepped_data["time_labels"]   # actual time labels
T0 = prepped_data["pre_periods"]     # number of pre-treatment periods

# --- Weights ---
w_final = np.array([0.85, 0.15])
w_start = np.array([0.1, 0.9])
steps = 50
weights_history = np.array([
    w_start + (w_final - w_start) * t/(steps-1) for t in range(steps)
])

# --- Figure setup ---
fig, ax = plt.subplots(figsize=(12,6))

line_y, = ax.plot(time, y, label="Observed Basque", color="black", linewidth=2)

line_hatY, = ax.plot(time, w_start[0]*Y0[:,0] + w_start[1]*Y0[:,1], 
                     label="Convex Basque", color="blue", linestyle="--", linewidth=2)

line_cataluna, = ax.plot(time, Y0[:,0], label="Catalonia", color="red", alpha=0.5)

line_madrid, = ax.plot(time, Y0[:,1], label="Madrid", color="blue", alpha=0.5)

# Axis labels
ax.set_xlabel("Year")
ax.set_ylabel("GDP per Capita")

# Vertical reference line at treatment, labeled
ax.axvline(x=time[T0], color="black", linestyle="--", label="Terrorism Begins")

ax.legend()
ax.set_ylim(y.min() - 1, y.max() + 1)

# --- Animation function ---
def update(frame):
    w_curr = weights_history[frame]
    y_hat = w_curr[0]*Y0[:,0] + w_curr[1]*Y0[:,1]
    line_hatY.set_ydata(y_hat)
    
    # Pre-treatment RMSE
    rmse_pre = np.sqrt(np.mean((y[:T0] - y_hat[:T0])2))
    
    if frame == steps - 1:
        line_cataluna.set_visible(False)
        line_madrid.set_visible(False)
        ax.set_title(
            f"Final SCM: w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}, "
            f"Pre-treatment RMSE={rmse_pre:.3f}"
        )
    else:
        ax.set_title(
            f"w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}, "
            f"Pre-treatment RMSE={rmse_pre:.3f}"
        )
    
    return line_hatY, line_cataluna, line_madrid

# --- Animate with repeated final frame ---
extra_frames = 50  # ~5 seconds at 10 fps
all_frames = list(range(steps)) + [steps-1]*extra_frames

ani = FuncAnimation(fig, update, frames=all_frames, blit=False)
ani.save("scm_weighted_combo.gif", writer=PillowWriter(fps=10))

plt.close(fig)
```

![Convex Combination](scm_weighted_combo.gif)

## Assumptions of SCM

- SCM is predicated on quality pre-intervention fit.

- Ideally, only a few donor units will be selected for the synthetic control.

- SCM also makes the standard assumptions about SUTVA (no interference).

## Placebos

::: {.panel-tabset}

### In Time

```{python}

import pandas as pd
from mlsynth import CLUSTERSC

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)

# Identify columns
treat_col = data.columns[-1]   # treatment dummy
unit_col = data.columns[0]     # region/unit column (assuming first col is "regionname")
year_col = data.columns[1]     # year column

# Update treatment: 1 if Basque and year >= 1969, else 0
data[treat_col] = ((data[unit_col] == "Basque") & (data[year_col] >= 1973)).astype(int)


config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["red"], "method": "PCR"}

arco = CLUSTERSC(config).fit()

```



### In Space

```{python}

import pandas as pd
from mlsynth import CLUSTERSC

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)

# Identify columns
unit_col = data.columns[0]   # region
year_col = data.columns[1]   # year
treat_col = data.columns[-1] # treatment dummy
outcome_col = data.columns[2]

# Function to build placebo datasets
def make_placebo_datasets(data, unit_col, year_col, treat_col, treat_start_year):
    placebo_dfs = {}
    for unit in data[unit_col].unique():
        df_copy = data.copy()
        df_copy[treat_col] = ((df_copy[unit_col] == unit) &
                              (df_copy[year_col] >= treat_start_year)).astype(int)
        placebo_dfs[unit] = df_copy
    return placebo_dfs

# Generate datasets
placebo_datasets = make_placebo_datasets(
    data,
    unit_col=unit_col,
    year_col=year_col,
    treat_col=treat_col,
    treat_start_year=1975
)

# Loop through each placebo dataset and fit RESCM
results = {}
for unit, df_placebo in placebo_datasets.items():
    config = {
        "df": df_placebo,
        "outcome": outcome_col,
        "treat": treat_col,
        "unitid": unit_col,
        "time": year_col,
        "display_graphs": False,
        "save": False, "method": "PCR", "cluster": False, "objective": "SIMPLEX"
    }
    model = CLUSTERSC(config).fit()
    results[unit] = model

all_gaps = []

for unit, model in results.items():
    # This is a numpy array; take first column
    gap_vec = model.sub_method_results['PCR'].time_series.estimated_gap[:, 0]

    df_gap = pd.DataFrame({
        "unit": unit,
        "time": model.sub_method_results['PCR'].time_series.estimated_gap[:, 1],
        "gap": gap_vec
    })
    all_gaps.append(df_gap)

all_gaps_df = pd.concat(all_gaps, ignore_index=True)


import matplotlib.pyplot as plt

# Plot all placebo gaps first (thin grey lines)
for unit in all_gaps_df['unit'].unique():
    if unit == "Basque":
        continue
    subset = all_gaps_df[all_gaps_df['unit'] == unit]
    plt.plot(subset['time'], subset['gap'], color='grey', alpha=0.5, linewidth=0.8)

# Plot Basque gap (thick black line)
basque_subset = all_gaps_df[all_gaps_df['unit'] == "Basque"]
plt.plot(basque_subset['time'], basque_subset['gap'], color='black', linewidth=3, label='Basque')

plt.xlabel("Year")
plt.ylabel("Estimated Gap")
plt.title("In-space Placebo Gaps")
plt.legend()
plt.show()
```

:::

## Limitations of SCM

- Uusally a long pre-treatment series forthe bias to shrink asymptotically

- Requires a good donor pool, otherwise is vulnerable to interpolation biases.

- Requires a relatively smooth time series to match on

