---
title: "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation"
date: 2025-05-19
categories: [Causal Inference, Econometrics]
---

Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where [the non-linear synthetic control] method (https://arxiv.org/abs/2306.01967) comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works.


# Non-Linear SC

Let $\mathbf{y} \in \mathbb{R}^T$ denote the outcome vector of the treated unit over $T$ pre-treatment periods. Let $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$ denote the matrix of outcomes for $J$ control units over the same pre-treatment periods. We seek to find a vector of weights $\mathbf{w} \in \mathbb{R}^J$ that forms an affine combination of the control units' outcomes to best approximate the treated unit.

We define the vector of pairwise discrepancies $\boldsymbol{\delta} \in \mathbb{R}^J$ as
$$
\delta_j = \|\mathbf{y} - \mathbf{Y}_0^{(:,j)}\|_2, \quad j = 1, \dots, J,
$$
which captures the $L_2$ distance between the treated unit and each control unit in the pre-treatment period. The objective is to solve the following constrained optimization problem:
$$
\begin{aligned}
\min_{\mathbf{w} \in \mathbb{R}^J} \quad & \|\mathbf{y} - \mathbf{Y}_0 \mathbf{w}\|_2^2 + a \sum_{j=1}^J \delta_j |w_j| + b \|\mathbf{w}\|_2^2 \\
\text{s.t.} \quad & \mathbf{1}^\top \mathbf{w} = 1
\end{aligned}
$$

The first term, $\|\mathbf{y} - \mathbf{Y}_0 \mathbf{w}\|_2^2$, is a standard least squares loss that penalizes the squared discrepancy between the treated unit's outcomes and the weighted combination of control units. This term ensures that the synthetic control closely approximates the treated unit in the pre-treatment period. The second term, $a \sum_{j=1}^J \delta_j |w_j|$, introduces a discrepancy-weighted $\ell_1$ penalty on the weights. The scalar parameter $a > 0$ controls the strength of this penalty. This term discourages assigning large weights to control units whose outcomes differ substantially from the treated unit, thereby promoting selection of more similar controls. The third term, $b \|\mathbf{w}\|_2^2$, is an $\ell_2$ regularization term (or Tikhonov regularization) that shrinks the weights toward zero in order to stabilize the solution and prevent overfitting. The scalar parameter $b > 0$ determines the degree of shrinkage. The constraint $\mathbf{1}^\top \mathbf{w} = 1$ ensures that the resulting synthetic control is an affine combination of the control units, maintaining scale comparability with the treated unit.

To select appropriate values of the regularization parameters $a$ and $b$, we perform $k$-fold cross-validation over a grid of candidate values. Let $\mathbf{y} \in \mathbb{R}^T$ denote the outcome vector for the treated unit in the pre-treatment period, and let $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$ be the matrix of pre-treatment outcomes for the $J$ control units.

Since the treated unit is unique and cannot be split into training and validation samples, we assess the quality of regularization parameters by leveraging only the control units. Specifically, we treat each control unit in turn as a "pseudo-treated" unit and attempt to reconstruct its pre-treatment outcomes using a weighted combination of the remaining controls. These reconstructions are repeated over several validation folds.

In each fold, we partition the control units into a training set $\mathbf{Y}_{\text{train}}$ and a validation set $\mathbf{Y}_{\text{val}}$. For each unit $\mathbf{y}^{(j)}$ in the validation set, we solve the following optimization problem:
$$
\begin{aligned}
\min_{\mathbf{w} \in \mathbb{R}^J} \quad & \|\mathbf{y}^{(j)} - \mathbf{Y}_{\text{train}} \mathbf{w}\|_2^2 + a \sum_{l=1}^J \delta_l |w_l| + b \|\mathbf{w}\|_2^2 \\
\text{s.t.} \quad & \mathbf{1}^\top \mathbf{w} = 1,
\end{aligned}
$$
where $\delta_l = \|\mathbf{y}^{(j)} - \mathbf{Y}_{\text{train}}^{(:,l)}\|_2$ measures the pairwise discrepancy between the pseudo-treated unit and each donor unit in the training set.

Let $\widehat{\mathbf{y}}^{(j)} = \mathbf{Y}_{\text{train}} \mathbf{w}^{(j)}$ be the predicted outcome for unit $j$. The validation error is given by the mean squared error:
$$
\text{MSE}^{(j)} = \frac{1}{T} \|\mathbf{y}^{(j)} - \widehat{\mathbf{y}}^{(j)}\|_2^2.
$$

Averaging over all validation units across all folds, we compute the total cross-validated prediction error for a given $(a, b)$. The optimal parameters are then defined as
$$
(a^*, b^*) = \arg\min_{a \in \mathcal{A},\, b \in \mathcal{B}} \; \text{CVError}(a, b),
$$
where $\mathcal{A}$ and $\mathcal{B}$ denote the candidate grids and $\text{CVError}(a, b)$ is the average mean squared error across folds.

This procedure selects regularization parameters that yield weights performing well not only on the treated unit but also on unseen units, providing a robust estimate of predictive generalization.
