---
title: 'On the Use of Clustering for Synthetic Controls Methods, using mlsynth'
date: 2025-02-03
categories: [Causal Inference, Machine Learning]
---

Picking a donor pool is very important with the synthetic control method. The vanilla method suffers from interpolation biases in the case when the donor pool is not suitable for the target unit. This is farily common with applied SCM in real life, as oftentimes we may have a high-dimensional donor pool and not know what the right control units would be _a priori_. The standard advice given in this instance is to limit our control group to units that are already similar to the target unit. But how do we do this? One way is to use clustering methods to select the control group. Consider [the classic Basque Country example](https://economics.mit.edu/sites/default/files/publications/The%20Economic%20Costs%20of%20Conflict.pdf) below, where the Basque Country undergoes a wave of terrorism in the mid-1970s which is thought to impact their GDP per Capita relative to other areas of Spain.

```{python}
#| fig-align: center
#| echo: false

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

def plot_donors_and_treated(donor_matrix, treated_vector, pre_periods, title):
    plt.figure(figsize=(10, 6))

    # Plot the donor units
    for i in range(donor_matrix.shape[1]):
        plt.plot(donor_matrix[:, i], color='gray', linewidth=0.5, alpha=0.8, label='_nolegend_')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Treated Unit')

    # Plot the normalized average of controls
    plt.plot(donor_matrix.mean(axis=1), color='blue', linewidth=2, label='Average of Controls')

    # Add vertical line for the pre-periods (waterline)
    plt.axvline(x=pre_periods, color='red', linestyle='--', linewidth=1.5, label='Terrorism')

    # Add labels and title
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.legend()

    plt.show()

# Set up theme for Matplotlib
def set_theme():
    theme = {
        "axes.grid": True,
        "grid.linestyle": "-",
        "grid.color": "black",
        "legend.framealpha": 1,
        "legend.facecolor": "white",
        "legend.shadow": True,
        "legend.fontsize": 12,
        "legend.title_fontsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        "axes.labelsize": 10,
        "axes.titlesize": 12,
        "figure.dpi": 100,
        "axes.facecolor": "white",
        "figure.figsize": (10, 6),
    }
    matplotlib.rcParams.update(theme)

set_theme()


file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)
treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

basque_prepped = dataprep(df, unitid, time, outcome, treat)

plot_donors_and_treated(basque_prepped['donor_matrix'], basque_prepped['y'], basque_prepped['pre_periods'], 'Basque vs. Controls')

```

Here, we plot the Basque versus the average of its controls as well as the individual donor outcome vectors themselves. We can see that the Basque Country is one of the wealthiest areas of Spain, up there with Madrid, Cataluna, and the Balearic Islands. We have other donors too, which by comparison are less wealthy. The key, then, is to ask which donors we should select for the synthetic control algorithm to consider in the first place, by exploiting pre-intervention similarities between the treated unit and control group. With "better" pre-policy donors, there is a higher chance that our out-of-sample predictions would be closer to the actual counterfactual.

# Clustering for SCM

To begin, we use SVD over the outcomes matrix $\mathbf{Y}_0 \in \mathbb{R}^{N_0 \times T}$. SVD decomposes it into $\mathbf{Y}_0 = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$, where $\mathbf{U} \in \mathbb{R}^{N_0 \times T}$ contains the left singular vectors, $\mathbf{\Sigma} \in \mathbb{R}^{T \times T}$ is a diagonal matrix of singular values, and $\mathbf{V} \in \mathbb{R}^{T \times T}$ contains the right singular vectors. Using singular value thresholding or taking the spectral rank, keeping only the top $r$ singular values, we have a low-dimensional representation of the outcomes matrix, $\mathbf{M} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$. The transformed representation is then defined as $\tilde{\mathbf{U}} = \mathbf{U} \mathbf{\Sigma}_r$, where $\mathbf{\Sigma}_r$ is the truncated diagonal matrix containing only the retained singular values.

We then perform k-means clustering over the the rows of $\tilde{\mathbf{U}}$, or the singular vectors. We seek to partition the units into $k$ clusters by minimizing the within-cluster variance, solving $\underset{P}{\operatorname*{argmin}} \sum_{i=1}^{N_0} \min_{j \in [k]} \|\tilde{\mathbf{U}}_i - P_j\|^2$, where $P_j$ represents the cluster centers. ``mlsynth`` uses the silhouette method to choose the number of clusters. Each unit is assigned a cluster, and we use the cluster that contains the treated unit as the donor pool.

We now may apply the SCM estimator. The weight vector $\mathbf{w}$ is obtained by solving $\underset{\mathbf{w} \in \mathbb{R}^{n_A}}{\operatorname*{argmin}} \lVert \mathbf{y}_1^- - \mathbf{w}^\top \mathbf{A}^- \rVert_2^2$, where $n_A$ is the cardinality of the selected subset of controls. For the FPCA method, see [the documentation](https://mlsynth.readthedocs.io/en/latest/clustersc.html) for ``mlsynth``.


```{python}
#| fig-align: center

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)


treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "Robust": "HQF",
    "Frequentist": False
}

model = CLUSTERSC(config)

arco = model.fit()
```

Here we have the Basque dataset.
