---
title: 'A Novel Two Stage Synthetic Control Method'
date: 2025-04-06
categories: [Causal Inference, Econometrics]
---
I define.

# Notation

Let $\mathbb{R}$ denote the set of real numbers. A calligraphic letter, such as $\mathcal{S}$, represents a discrete set with cardinality $S = |\mathcal{S}|$. Let $\mathbf{C}_T \in \mathbb{R}^{T \times T}$ represent the centering matrix defined as:

$$
\mathbf{C}_T \coloneqq \mathbf{I}_T - \frac{1}{T} \mathbf{1}_T \mathbf{1}_T^\top
$$

Where $\mathbf{I}_T$ is the identity matrix of size $T$, and $\mathbf{1}_T \in \mathbb{R}^T$ is the vector of ones of length $T$. For any vector $\mathbf{x} \in \mathbb{R}^T$, the demeaned version is:

$$
\widetilde{\mathbf{x}} \coloneqq \mathbf{C}_T \mathbf{x}
$$ 

For any pair of vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{N_0}$, we define the Hadamard (elementwise) product as $(\mathbf{a} \odot \mathbf{b})_j \coloneqq a_j b_j \quad\forall j \in \mathcal{N}_0$. Depending on orientation, there are two dot products we work with. For column vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{N_0}$,
  $$
  \mathbf{a}^\top \mathbf{b} \coloneqq \sum_{j \in \mathcal{N}_0} a_j b_j \in \mathbb{R}
  $$

This is equivalent to viewing $\mathbf{a}^\top \in \mathbb{R}^{1 \times N_0}$ as a row vector and multiplying it by the column vector $\mathbf{b} \in \mathbb{R}^{N_0}$. The result is a scalar:

  $$
  \mathbf{a}^\top \mathbf{b} \in \mathbb{R}
  $$

More generally, when multiplying a matrix $\mathbf{A} \in \mathbb{R}^{T \times N_0}$ by a vector $\mathbf{b} \in \mathbb{R}^{N_0}$, the result is a new vector in $\mathbb{R}^T$, formed as a linear combination of the columns of $\mathbf{A}$:
$$
\mathbf{A} \mathbf{b} = \sum_{j \in \mathcal{N}_0} b_j \mathbf{A}_{:,j} \in \mathbb{R}^T
$$

Here, each entry in the resulting vector is a dot product between a row of $\mathbf{A}$ and the vector $\mathbf{b}$. Let $j \in \mathbb{N}$ represent indices for a total of $N$ units and $t \in \mathbb{N}$ index time. Let $j = 1$ be the treated unit, with the set of controls being $\mathcal{N}_0 = \mathcal{N} \setminus \{1\}$, with cardinality $N_0$. The pre-treatment period consists of the set $\mathcal{T}_1 = \{ t \in \mathbb{N} : t \leq T_0 \}$, where $T_0$ is the final period before treatment. Similarly, the post-treatment period is given by $\mathcal{T}_2 = \{ t \in \mathbb{N} : t > T_0 \}$.

The observed outcome for unit $j$ at time $t$ is $y_{jt}$, where a generic outcome vector for a given unit in the dataset is $\mathbf{y}_j \in \mathbb{R}^T$, with $\mathbf{y}_j = (y_{j1}, y_{j2}, \dots, y_{jT})^\top \in \mathbb{R}^{T}$. The outcome vector for the treated unit specifically is $\mathbf{y}_1$. The donor matrix is defined as $\mathbf{Y}_0 \coloneqq \begin{bmatrix} \mathbf{y}_j \end{bmatrix}_{j \in \mathcal{N}_0} \in \mathbb{R}^{T \times N_0}$, where each column indexes a donor unit and each row is indexed to a time period. We denote by $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{T_0}$ the subvector of outcomes for unit $j$ in the pre-treatment period, and by $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{T_1}$ the corresponding post-treatment vector, where $T_1 = T - T_0$. Then, we define our pre and post intervention analogs for the data:

$$
\mathbf{y}_1^{\text{pre}} = (y_{1t})_{t \in \mathcal{T}_1} \in \mathbb{R}^{T_0}, \quad \mathbf{y}_1^{\text{post}} = (y_{1t})_{t \in \mathcal{T}_2} \in \mathbb{R}^{T_1}
$$

$$
\mathbf{Y}_0^{\text{pre}} = \left[ \mathbf{y}_j^{\text{pre}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_0 \times N_0}, \quad \mathbf{Y}_0^{\text{post}} = \left[ \mathbf{y}_j^{\text{post}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_1 \times N_0}
$$

## Synthetic Controls

Before I get to the SRC, I review basic synthetic controls. Let $\mathbf{y}_1 \in \mathbb{R}^T$ represent the outcome vector of the treated unit, where we seek to estimate the counterfactual outcomes in the post-treatment period (i.e., for $t > T_0$). We define the synthetic control as a weighted average of the donor units' outcome vectors $\mathbf{y}_j$, where $j \in \mathcal{N}_0$. Specifically, we use a weight vector $\mathbf{w} \in \mathbb{R}^{N_0}$ to form the synthetic control, such that the counterfactual outcome for the treated unit is

$$
\mathbf{y}_1^{\text{SC}} = \mathbf{Y}_0 \mathbf{w}
$$

where $\mathbf{y}_1^{\text{SC}}$ is the predicted counterfactual outcome for the treated unit, $\mathbf{Y}_0 \in \mathbb{R}^{T \times N_0}$ is the matrix of outcome vectors for the donor units, and $\mathbf{w} \in \mathbb{R}^{N_0}$ is the vector of weights applied to each donor unit. The goal is to find $\mathbf{w}$ that best approximates the outcome vector of the treated unit during the pre-treatment period.

The weights are estimated by minimizing the distance between the treated unit's pre-treatment outcomes $\mathbf{y}_{1,\text{pre}}$ and the weighted average of the donor units' pre-treatment outcomes. This can be expressed as

$$
\underset{\mathbf{w} \in \mathbb{I}^{N_0}}{\operatorname*{argmin}} \left\| \mathbf{y}_{1,\text{pre}} - \mathbf{Y}_{0,\text{pre}} \mathbf{w} \right\|_2^2
$$

where $\mathbf{y}_{1,\text{pre}} \in \mathbb{R}^{T_0}$ is the pre-treatment outcome vector for the treated unit and $\mathbf{Y}_{0,\text{pre}} \in \mathbb{R}^{T_0 \times N_0}$ is the pre-treatment outcome matrix for the donor units. The objective is to minimize the mean squared error (MSE) between the pre-treatment outcomes of the treated unit and the synthetic control.

$$
\mathcal{W} = \left\{ \mathbf{w} \in \mathbb{R}^{N_0} : \mathbf{w} \geq 0, \|\mathbf{w}\|_1 = 1 \right\}
$$

Once the weights $\mathbf{w}$ are estimated, the counterfactual outcome for the treated unit during the post-treatment period is given by

$$
\mathbf{y}^{\text{SC}}_1 = \mathbf{Y}_0 \mathbf{w}, \quad \text{for } t > T_0
$$

Thus, the counterfactual outcome for each time period in the post-treatment period is the weighted sum of the donor units' outcomes, with the weights derived from the pre-treatment period.

# Synthetic Regression Control (SRC)

The SRC estimation procedure consists of three stages: estimation of alignment coefficients, estimation of the noise variance, and penalized optimization of donor weights. To account for level differences, we first demean both the treated and donor series using the centering matrix. The demeaned treated outcome vector is given by
$$
\widetilde{\mathbf{y}}_1 = \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}}, \quad \text{and} \quad \widetilde{\mathbf{Y}}_0 = \mathbf{C}_{T_0} \mathbf{Y}_0^{\text{pre}}.
$$

For $\forall \: j \in \mathcal{N}$, an alignment coefficient $\hat{\theta}_j$ is computed by projecting $\widetilde{\mathbf{y}}_1$ onto the corresponding column $\widetilde{\mathbf{Y}}_{0,j}$ of the demeaned donor matrix:
$$
\hat{\theta}_j = \frac{\widetilde{\mathbf{y}}_1^\top \widetilde{\mathbf{Y}}_{0,j}}{\| \widetilde{\mathbf{Y}}_{0,j} \|^2}, \quad \text{for each } j \in \mathcal{N}_0.
$$
These coefficients collectively form the vector $\hat{\boldsymbol{\theta}} \in \mathbb{R}^{N_0}$. We then define the alignment-adjusted donor matrix as
$$
\mathbf{Y}_{\boldsymbol{\theta}} = \mathbf{Y}_0^{\text{pre}} \cdot \mathrm{diag}(\hat{\boldsymbol{\theta}}),
$$
which rescales each donor column by its alignment with the treated unit. To estimate the residual noise varianceof the donor projection with the target unit's demeaned values, $\hat{\sigma}^2$, we use the centering matrix $\mathbf{C}_{T_0}$ and define
$$
\mathbf{G} = \mathbf{Y}_0^{\text{pre}^\top} \mathbf{C}_{T_0} \mathbf{Y}_0^{\text{pre}}.
$$
We extract its diagonal to normalize the projection, forming the operator
$$
\mathbf{Z} = \mathbf{Y}_0^{\text{pre}} \cdot \mathrm{diag}\left( \mathrm{diag}(\mathbf{G})^{-1} \right) \cdot \mathbf{Y}_0^{\text{pre}^\top}.
$$
The residual itself then is computed as
$$
\mathbf{r} = \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}} - \mathbf{C}_{T_0} \mathbf{Z} \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}},
$$
and the estimated noise variance is
$$
\hat{\sigma}^2 = \| \mathbf{r} \|^2.
$$

With $\hat{\boldsymbol{\theta}}$ and $\hat{\sigma}^2$ in hand, we solve a penalized least squares problem to obtain donor weights. Specifically, we minimize the objective
$$
\underset{\mathbf{w} \in \mathcal{W}}{\operatorname*{argmin}} \left\| \mathbf{y}_1^{\text{pre}} - \mathbf{Y}_{\boldsymbol{\theta}} \mathbf{w} \right\|^2 + 2 \hat{\sigma}^2 \cdot \mathbf{1}_{N_0}^\top \mathbf{w},
$$
The solution $\mathbf{w}$ balances goodness-of-fit with regularization based on the estimated noise. Finally, we construct predictions. Let
$$
\bar{y}_1 = \frac{1}{T_0} \mathbf{1}_{T_0}^\top \mathbf{y}_1^{\text{pre}}, \quad \text{and} \quad \bar{\mathbf{y}}_0 = \frac{1}{T_0} \mathbf{Y}_0^{\text{pre}^\top} \mathbf{1}_{T_0}
$$
be the mean of the treated unit and the vector of donor means, respectively, in the pre-treatment period. Let $\mathbf{Y}_0^{\text{post}} \in \mathbb{R}^{T_1 \times N_0}$ denote the donor outcomes in the post-treatment period. Our SRC predictions take the form of

$$
\mathbf{y}^{\text{SRC}}_1 = \begin{bmatrix} \hat{\mathbf{y}}_1^{\text{pre}} \\ \hat{\mathbf{y}}_1^{\text{post}} \end{bmatrix}
= \begin{bmatrix} 
\mathbf{Y}_0^{\text{pre}} (\hat{\boldsymbol{\theta}} \odot \mathbf{w}) \\ 
\bar{y}_1 \cdot \mathbf{1}_{T_1} + \left( \mathbf{Y}_0^{\text{post}} - \mathbf{1}_{T_1} \bar{\mathbf{y}}_0^\top \right)(\hat{\boldsymbol{\theta}} \odot \mathbf{w})
\end{bmatrix}
$$

## Estimating SRC in ```mlsynth```

Now I will give an example of how to use SRC for your own applied work. We begin by installing ```mlsynth``` from my Github

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

For comparison sake, I use the Basque data since that's what the author uses.

```{python}

import pandas as pd
from mlsynth.mlsynth import SRC, FSCM
import matplotlib.pyplot as plt
import matplotlib

# matplotlib theme
jaredplot = {'axes.grid': True,
              'grid.linestyle': '--',
              'legend.framealpha': 1,
              'legend.facecolor': 'white',
              'legend.shadow': True,
              'legend.fontsize': 14,
              'legend.title_fontsize': 16,
              'xtick.labelsize': 14,
              'ytick.labelsize': 14,
              'axes.labelsize': 16,
              'axes.titlesize': 20,
              'figure.dpi': 100}

matplotlib.rcParams.update(jaredplot)

# URL to fetch the dataset
url = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
df = pd.read_csv(url)

treat = df.columns[-1]
time = df.columns[1]
outcome = df.columns[2]
unitid = df.columns[0]

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": False,
    "counterfactual_color": "blue"
}

# Create instances of each model with the config dictionary
models = [SRC(config), FSCM(config)]


results = []
for model in models:
    result = model.fit()
    results.append((type(model).__name__, result))  # Store the model name and result as a tuple

plt.figure(figsize=(10, 6))
plt.axvline(x=20, color='red', linestyle=':', label="Terrorism")

for model_name, result in results:
    counterfactual = result["Vectors"]["Counterfactual"]
    # Plot the Counterfactual vs Observed Unit with dynamic labels
    plt.plot(counterfactual, label=f"{model_name} Basque", linestyle='--')
observed_unit = result["Vectors"]["Observed Unit"]
plt.plot(observed_unit, label=f"Basque", color='black', linewidth=2)

plt.xlabel('Time Periods')
plt.ylabel('GDP per Capita')
plt.title('Synthetic Regression Control Versus Forward SCM')
plt.legend()

plt.show()
```
