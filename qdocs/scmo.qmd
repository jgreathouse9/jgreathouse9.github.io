---
title: "Synthetic Control Methods With Multiple Outcomes"
date: 2025-04-15
categories: [Causal Inference, Econometrics]
---

Sometimes, analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may predict a single target/outcome variable that we care about. However, most people who use synthetic controls use only a single outcome in their analyses. Recent papers by econometricians have extended the standard model to incorporating multiple outcomes into the objective function, and a few recent [recent](https://doi.org/10.48550/arXiv.2311.16260) [papers](https://doi.org/10.48550/arXiv.2304.02272) have advocated for this in applied settings. This blog demonstrates this approach for ```mlsynth```.

## Notation

We adopt the following notational conventions throughout. Scalars are denoted by lowercase italicized letters such as $g$, vectors are denoted by bold lowercase letters such as $\mathbf{v} \in \mathbb{R}^n$, and matrices are denoted by bold uppercase letters such as $\mathbf{A} \in \mathbb{R}^{m \times n}$. Sets are denoted by calligraphic letters, such as $\mathcal{N}$ or $\mathcal{T}$, and the cardinality of a finite set $\mathcal{A}$ is written $|\mathcal{A}|$. The dot product between vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^T$ is written as $\langle \mathbf{a}, \mathbf{b} \rangle = \sum_{t=1}^T a_t b_t$, and the squared Euclidean norm is $\| \mathbf{a} \|^2 = \langle \mathbf{a}, \mathbf{a} \rangle$. For a matrix $\mathbf{A}$, we write $\| \mathbf{A} \|_F^2 = \sum_{i,j} A_{ij}^2$ for the squared Frobenius norm and $\| \mathbf{A} \|_2$ for its spectral norm. For any $n \in \mathbb{N}$, we define the $n$-dimensional probability simplex by

$$
\Delta^{n} = \left\{ \mathbf{w} \in \mathbb{R}^{n+1} \mid w_i \geq 0 \text{ for all } i, \sum_{i=1}^{n+1} w_i = 1 \right\}.
$$

Matrix multiplication is written in the usual way; when dimensions are compatible, products like $\mathbf{A}^\top \mathbf{b}$, $\mathbf{A} \mathbf{B}$, or $\mathbf{b}^\top \mathbf{c}$ denote standard matrix-vector or matrix-matrix products. Elementwise operations (e.g., $\mathbf{a} \odot \mathbf{b}$) are avoided unless explicitly defined.

Let $\mathcal{N} = \{1, 2, \dots, N\}$ index the units in the panel, with unit $1$ denoting the treated unit and $\mathcal{N}_0 = \{2, 3, \dots, N\}$ denoting the set of control units. We write $N_0 = |\mathcal{N}_0| = N - 1$ for its cardinality. Let $\mathcal{T} \subset \mathbb{N}$ be a finite set of time periods with $|\mathcal{T}| = T$. Let $T_0 < T$ denote the final pre-treatment period, and define $\mathcal{T}_1 = \{1, 2, \dots, T_0\}$ and $\mathcal{T}_2 = \{T_0 + 1, \dots, T\}$ as the pre-treatment and post-treatment periods, respectively.

Let $\mathbf{y}_j \in \mathbb{R}^T$ denote the time series for outcome zero (our main outcome of interest) for unit $j \in \mathcal{N}$. Let $\mathbf{Y}_0 \in \mathbb{R}^{T \times N_0}$ denote the matrix that stacks these vectors for all control units $j \in \mathcal{N}_0$. We write $\mathbf{y}_1 = \begin{bmatrix} y_{1,1} & \cdots & y_{1,T} \end{bmatrix}^\top$ for the treated unit’s outcome vector. We partition each $\mathbf{y}_j$ into a pre-treatment vector $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{|\mathcal{T}_1|}$ and post-treatment vector $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{|\mathcal{T}_2|}$, and likewise for $\mathbf{Y}_0$, defining $\mathbf{Y}_0^{\text{pre}} \in \mathbb{R}^{|\mathcal{T}_1| \times N_0}$ and $\mathbf{Y}_0^{\text{post}} \in \mathbb{R}^{|\mathcal{T}_2| \times N_0}$.

Let $K$ denote the number of auxiliary outcomes (excluding the main one). For each outcome $\ell \in \{1, 2, \dots, K\}$, let $\mathbf{y}_j^{(\ell)} \in \mathbb{R}^T$ denote the $\ell$-th outcome vector for unit $j$, and let $\mathbf{Y}_0^{(\ell)} \in \mathbb{R}^{T \times N_0}$ denote the matrix collecting the $\ell$-th outcome across all controls. These are also split into pre- and post-treatment periods in the usual way.

We define the "stacking" operation—denoted by a prime—as the vertical concatenation of multiple outcomes. Specifically, let $\mathbf{Y}_0' \in \mathbb{R}^{K |\mathcal{T}_1| \times N_0}$ denote the vertically stacked matrix:

$$
\mathbf{Y}_0' = \begin{bmatrix}
\mathbf{Y}_0^{(1), \text{pre}} \\
\mathbf{Y}_0^{(2), \text{pre}} \\
\vdots \\
\mathbf{Y}_0^{(K), \text{pre}}
\end{bmatrix},
\quad
\mathbf{y}_1' = \begin{bmatrix}
\mathbf{y}_1^{(1), \text{pre}} \\
\mathbf{y}_1^{(2), \text{pre}} \\
\vdots \\
\mathbf{y}_1^{(K), \text{pre}}
\end{bmatrix}.
$$

This stacking procedure will allow us to define synthetic control weights that balance auxiliary outcomes as well as the main outcome.

## Standard Synthetic Control

Before introducing our new estimator, we begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator selects weights $\mathbf{w} \in \Delta^{N_0}$ to minimize the pre-treatment discrepancy between the treated unit and a convex combination of the controls:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}}{\operatorname*{argmin}} \|\mathbf{y}_1^{\text{pre}} - \mathbf{Y}_0^{\text{pre}} \mathbf{w} \|_2^2.
$$

This is a constrained least squares problem in which we regress the treated unit’s pre-treatment outcomes onto the control matrix under the constraint that $\mathbf{w}$ lies in the simplex $\Delta^{N_0}$. That is, the synthetic control estimator builds a linear approximation to $\mathbf{y}_1^{\text{pre}}$ using a weighted average of the control outcomes, where the weights are non-negative and sum to one. This corresponds to restricting attention to points in the convex hull of the control units.

Once the optimal weights $\mathbf{w}^\ast$ are selected, the counterfactual trajectory for the treated unit in the post-treatment period is estimated by applying the same weights to the control matrix:

$$
\hat{\mathbf{y}}_1^{\text{post}} = \mathbf{Y}_0^{\text{post}} \mathbf{w}^\ast.
$$

The estimated treatment effect at each post-treatment time point is then given by the difference between observed and counterfactual outcomes: $\hat{\tau}_{1,t} = y_{1,t} - \hat{y}_{1,t}$ for $t \in \mathcal{T}_2$.

## Stacked SCM with Multiple Outcomes

When multiple outcomes are present, we have two primary approaches to adapt the SCM. The first approach is to concatenate the outcomes for each unit (including the treated unit and the control units) into a single stacked matrix. This method treats all the outcomes as if they are part of a single vector and optimizes the weights accordingly. The stacking procedure is expressed as:

$$
\mathbf{Y}_0' = \begin{bmatrix}
\mathbf{Y}_0^{(1), \text{pre}} \\
\mathbf{Y}_0^{(2), \text{pre}} \\
\vdots \\
\mathbf{Y}_0^{(K), \text{pre}}
\end{bmatrix},
\quad
\mathbf{y}_1' = \begin{bmatrix}
\mathbf{y}_1^{(1), \text{pre}} \\
\mathbf{y}_1^{(2), \text{pre}} \\
\vdots \\
\mathbf{y}_1^{(K), \text{pre}}
\end{bmatrix}.
$$

In this approach, we essentially treat the set of outcomes as a concatenated vector and apply the standard SCM optimization problem to the stacked data. The objective function for this optimization is:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}}{\operatorname*{argmin}} \|\mathbf{y}_1' - \mathbf{Y}_0' \mathbf{w} \|_2^2 \quad \forall t \in \mathcal{T}_1.
$$

Here, the vector \(\mathbf{y}_1'\) is the stacked pre-treatment outcomes of the treated unit, and \(\mathbf{Y}_0'\) is the stacked matrix of control outcomes. The weights \(\mathbf{w}^*\) are found such that the counterfactual outcomes for the treated unit (in the pre-treatment period) are as close as possible to the weighted combination of the control outcomes. The counterfactual prediction for the treated unit during the post-treatment period is then computed as:

$$
\mathbf{y}_1^{\text{counterfactual}} = \mathbf{Y}_0' \mathbf{w}^*.
$$

The second approach involves including an intercept term for each outcome to control for differences in levels across outcomes. Instead of demeaning the outcomes manually, we add an intercept (a constant term) to the optimization, which effectively adjusts for any systematic shifts in the data. This approach ensures that the model captures any baseline differences between the outcomes.

The objective function in this case is:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}, \beta \in \mathbb{R}}{\operatorname*{argmin}} \|\mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} - \beta \|_2^2 \quad \forall t \in \mathcal{T}_1.
$$

Here, \(\beta\) represents the intercept term, which accounts for baseline shifts between the treated and control units. The weights \(\mathbf{w}^*\) are determined by solving this optimization, which adjusts the outcomes for the intercept term while finding the best fit for the counterfactual prediction. The counterfactual for the treated unit is then computed as:

$$
\mathbf{y}_1^{\text{counterfactual}} = \mathbf{Y}_0 \mathbf{w}^* + \beta.
$$

This extension of the SCM can be understood as solving multiple systems of equations simultaneously. By including auxiliary outcomes such as local hotel prices, tourist arrivals, or any other relevant metrics, we allow the model to capture the factors that contribute to the outcome of interest—such as Gross Booking Revenue (GBV). These auxiliary outcomes provide additional information about the latent factors that influence the main outcome, thereby improving the match of the synthetic control. In essence, by solving for the weights $\mathbf{w}^\ast$ that best fit the pre-treatment data, we are not just matching the treated unit on the outcome of interest (GBV), but we are also matching on the auxiliary factors that influence the outcome. This process ensures that the synthetic control reflects the underlying dynamics of the treated unit in a more nuanced way, leading to a better counterfactual prediction. The optimization framework effectively allows us to match on both the main outcome and the auxiliary factors, capturing the factor loadings that generate the observed data. Thus, stacking the outcomes or including an intercept term aligns the model with the idea that multiple systems of equations, representing different outcomes or contributing factors, must be solved simultaneously. This yields a more accurate synthetic control by aligning the underlying factor loadings of both the treatment and control units.



# Estimation in Python

```{python}

#| fig-align: center
#| echo: false

import numpy as np
import pandas as pd

from mlsynth.mlsynth import SCMO, CLUSTERSC

def simulate(
    N=99, T0=52*3, T1=52, K=4, r=3, sigma=.20,
    max_gamma=.5, T_season=12, seed=34305
):
    np.random.seed(seed)
    T = T0 + T1

    # Latent factors
    phi = np.random.normal(0, 1, size=(N, r))           # Market-specific latent factors
    mu = np.random.normal(0, 1, size=(T, K, r))          # Time-and-outcome-specific latent factors

    # Fixed effects
    alpha = np.random.normal(0, 1, size=(N, K))          # Market fixed effects
    beta = np.random.normal(0, 1, size=(T, K))           # Time fixed effects

    # Market-specific seasonal parameters
    gamma_i = np.random.uniform(0, max_gamma, size=N)    # amplitude of seasonal effect
    tau_i = np.random.randint(0, T_season, size=N)       # phase shift (peak week)

    # Construct seasonal matrix S (N x T): market-time-specific seasonality
    t_grid = np.arange(T)
    S = np.array([
        gamma_i[i] * np.cos(4 * np.pi * (t_grid - tau_i[i]) / T_season)
        for i in range(N)
    ])

    # Initialize outcome matrix
    Y = np.zeros((N, T, K))

    # Define baseline offsets for each outcome with random integers between 200 and 500
    baseline_shift = [np.random.randint(200, 500) for _ in range(K)]  # Random base values for each outcome

    # Autocorrelation coefficients for each outcome
    rho = np.array([0.8, 0.6, 0.5, 0.3])  # AR(1) coefficients for each outcome

    # Generate outcomes with autocorrelation
    for k in range(K):
        latent = phi @ mu[:, k, :].T  # N x T
        base = (
            alpha[:, [k]] +         # N x 1
            beta[:, k] +            # T,
            latent +                # N x T
            S +                     # N x T
            baseline_shift[k]       # scalar
        )
        noise = np.random.normal(0, sigma, size=(N, T))

        # First time point initialization
        Y[:, 0, k] = base[:, 0] + noise[:, 0]

        # AR(1) recursion
        for t in range(1, T):
            Y[:, t, k] = (
                rho[k] * Y[:, t-1, k] +
                (1 - rho[k]) * base[:, t] +
                noise[:, t]
            )

    # Identify treated market: second-highest on phi[:, 0]
    treated_unit = np.argsort(phi[:, 0])[-2]

    # Vectorized treatment assignment (N x T)
    time = np.arange(T)
    post_treatment = (time >= T0)
    treat = np.zeros((N, T), dtype=int)
    treat[treated_unit, post_treatment] = 1

    # Inject treatment effect into Gross Booking Value for treated market
    Y[treated_unit, post_treatment, 0] += 5  # add treatment effect of +5

    # Construct the dataframe without loops
    markets = np.arange(N)[:, None]           # shape (N, 1)
    weeks = np.arange(T)[None, :]             # shape (1, T)

    market_grid = np.repeat(markets, T, axis=1).flatten()  # shape (N*T,)
    week_grid = np.tile(weeks, (N, 1)).flatten()           # shape (N*T,)


    cities = [
        "São Paulo", "Mexico City", "San Carlos de Bariloche", "Rio de Janeiro", "Ushuaia",
        "Bogotá", "Santiago", "Caracas", "Guayaquil", "Quito",
        "Brasília", "Bocas del Toro", "Asunción", "Cabo San Lucas", "Playa del Carmen",
        "Medellín", "Porto Alegre", "Placencia", "Recife", "Salvador",
        "Zihuatanejo", "San José", "Panama City", "Montevidio", "Tegucigalpa",
        "Foz do Iguaçu", "Maracaibo", "Rosario", "Maracay", "Antofagasta",
        "San Pedro Sula", "San Juan", "Chihuahua", "Cayo District", "Maturín",
        "Buzios", "Puebla", "Mar del Plata", "Arequipa", "Fernando de Noronha", "Guatemala City",
        "Zacatecas", "Mérida", "Córdoba", "Cozumel", "Trujillo",
        "Corozal Town", "Santa Cruz de la Sierra", "San Luis Potosí", "Jalapão", "Potosí",
        "Tucumán", "Neuquén", "La Plata", "Viña del Mar", "Florianópolis", "Lagos de Moreno",
        "La Paz", "Belém", "Venezuela", "Ribeirão Preto", "Valparaíso",
        "Marília", "Campinas", "Vitoria", "Sorocaba", "Santa Fe",
        "San Salvador", "Lima", "Buenos Aires", "Curitiba", "Maceió",
        "Cartagena", "La Ceiba", "Puerto La Cruz", "Olinda", "Monterrey",
        "Ibagué", "Cúcuta", " Paraty", "Cancún", "Puerto Escondido", "Chiclayo", "Ambato",
        "Pucallpa", "Santa Marta", "Villavicencio", "Paraná", "Cauca", "San Vicente",
        "Cali", "Tarija", "Manzanillo", "El Alto", "Santiago de Chile", "Cochabamba",
        "Iquique", "Santo Domingo", "Durango", "Puerto Viejo de Talamanca"
    ]

    city_mapping = {i: cities[i] for i in range(N)}

    data = {
        'Market': [city_mapping[market] for market in market_grid],
        'Week': week_grid,
        'Experiences': treat.flatten()
    }

    for k in range(K):
        if k == 0:
            data['Gross Booking Value'] = Y[:, :, k].flatten()
        elif k == 1:
            data['Average Booking Price'] = Y[:, :, k].flatten()
        elif k == 2:
            data['Average Daily Visitors'] = Y[:, :, k].flatten()
        elif k == 3:
            data['Average Cost of Hotel Rooms'] = Y[:, :, k].flatten()

    return pd.DataFrame(data)

# Run simulation
df = simulate()


config = {
    "df": df,
    "outcome": 'Gross Booking Value',
    "treat": 'Experiences',
    "unitid": 'Market',
    "time": 'Week',
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue"], "addout": list(df.columns[4:]),
    "method": "both"
}

arco = SCMO(config).fit()

```

## Application to Prop 99

We can apply this to Prop 99 too.

```{python}

import pandas as pd # To work with panel data

from IPython.display import display, Markdown # To create the table

from mlsynth.mlsynth import SCMO # The method of interest

url = "https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/refs/heads/master/examples/datasets/smoking_data.csv"

datap99 = pd.read_csv(url)

datap99["Proposition 99"] = ((datap99["state"] == "California") & (datap99["year"] >= 1989)).astype(int)

config = {
    "df": datap99,
    "outcome": "cigsale",
    "treat": "Proposition 99",
    "unitid": "state",
    "time": "year",
    "display_graphs": True,
    "save": False, "method": "both",
    "counterfactual_color": "red", "addout": "retprice"}

p99 = SCMO(config).fit()

```
