---
title: 'Forward Synthetic Control Estimation'
date: 2025-03-31
categories: [Econometrics, Causal Inference, Data Science]
---
# Intro

Interpolation bias is a known issue in synthetic control methods. For valid counterfactual prediction, the donor units should be as similar as possible to the treated unit in the pre-treatment period. Selecting an appropriate donor pool is therefore critical, but this can be challenging in settings with many potential controls. This post introduces the [Forward Selected Synthetic Control Method](https://doi.org/10.1016/j.econlet.2024.111976), which applies Forward Selection to choose the donor pool for a synthetic control model before estimating out-of-sample predictions.

Let $\mathbb{R}$ denote the set of real numbers. A calligraphic letter, such as $\mathcal{S}$, represents a discrete set with cardinality $S = |\mathcal{S}|$. Let $t \in \mathbb{N}$ and $i \in \mathbb{N}$ represent indices for $T$ time periods and $N$ units, respectively. The pre-treatment period consists of consecutive time periods $\mathcal{T}_1 = \{1, 2, \dots, T_0\}$, with cardinality $T_1$, while the post-treatment period is given by $\mathcal{T}_2 = \{T_0 + 1, \dots, T\}$, with cardinality $$T_2$$. The treated unit is indexed by $i = 1$, while the remaining control units are indexed as $$\mathcal{N}_0 = \{2, \dots, N\}$$, with cardinality $$N_0 = N - 1$$. The observed outcome for unit $i$ at time $t$ is denoted by $y_{it}$. The outcome vector for the treated unit is $ \mathbf{y}_1 = \begin{bmatrix} y_{11} & y_{12} & \cdots & y_{1T} \end{bmatrix}^\top \in \mathbb{R}^T$, where each entry corresponds to the outcome of the treated unit at time $t$. The donor pool matrix, which concatenates the control unit outcome vectors, is defined as $ \mathbf{Y}_0 \in \mathbb{R}^{T \times N_0}$. The space of synthetic control weights is the $$N_0$$-dimensional probability simplex: $ \Delta^{N_0 - 1} = \left\{ \mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0} : \|\mathbf{w}\|_1 = 1 \right\}.$

Now, consider a restricted donor pool chosen by forward selection. This consists of a strict subset of control units, $\mathcal{S} \subseteq \mathcal{N}_0$, with cardinality $k = |\mathcal{S}|$ where $k \leq N_0$. This subset induces a subsimplex $\Delta^{k - 1}(\mathcal{S}) = \left\{ \mathbf{w}^\prime \in \mathbb{R}_{\geq 0}^{k} : \|\mathbf{w}^{\prime}\|_1 = 1 \right\}$. Now we discuss how the donor pool for the synthetic control estimator is chosen.

I construct the forward selection process as a sequence of tuples, where each tuple consists of a donor set and its corresponding pre-treatment mean squared error ($\text{MSE}$):

$$
\mathbb{T} = \left\{ (\mathcal{S}_1, \text{MSE}_1), (\mathcal{S}_2, \text{MSE}_2), \dots, (\mathcal{S}_K, \text{MSE}_K) \right\}.
$$

Each element $(\mathcal{S}_K, \text{MSE}_K)$ consists of $\mathcal{S}_K$, or the selected donor set after $K$ iterations of forward selection and $\text{MSE}_K$, or the pre-treatment mean squared error when using the donor set $\mathcal{S}_K$. We begin by minimizing

$$
\mathcal{S}_1 = \{j^\ast\}, \quad \text{where} \quad j^\ast = \underset{j \in \mathcal{N}_0}{\operatorname*{argmin}} \ \text{MSE}(\{j\}).
$$

At each step, we select a single control unit that minimizes the $\text{MSE}$ when combined with the previously selected set:

$$
 j^\ast = \underset{j \in \mathcal{N}_0 \setminus \mathcal{S}_{K-1}}{\operatorname*{argmin}} \ \text{MSE}(\mathcal{S}_{K-1} \cup \{j\}).
$$

The updated tuple is:

$$
(\mathcal{S}_K, \text{MSE}_K) = (\mathcal{S}_{K-1} \cup \{j^\ast\}, \text{MSE}(\mathcal{S}_{K-1} \cup \{j^\ast\})).
$$

This process continues until all $N_0$ donors have been evaluated. The final donor set is chosen as the tuple with the lowest $\text{MSE}$:

$$
\mathcal{S}^{\ast} = \underset{(\mathcal{S}_K, \text{MSE}_K) \in \mathbb{T}}{\operatorname*{argmin}} \ \text{MSE}_K.
$$

Thus, $\mathcal{S}^{\ast}$ is the optimal donor pool by forward selection. Note that even within $\mathcal{S}^{\ast}$, some donors may receive zero weight in the final solution, as these are just the units selected for inclusion in the donor pool, not the ones that will actually get weight, in contrast to methods such as [Forward Difference-in-Differences](https://doi.org/10.1287/mksc.2022.0212) or the [forward selection panel data method](https://doi.org/10.1016/j.jeconom.2021.04.009), both of which are available in ```mlsynth```.

## Estimation in ```mlsynth```

As ususal, we begin by installing ```mlsynth```.

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

And then we load the Prop 99 data and fit the model in the ususal ```mlsynth``` fashion.

```{python}
#| fig-align: center

import pandas as pd
from IPython.display import display, Markdown
from mlsynth.mlsynth import FSCM
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"
data = pd.read_csv(url)
config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": "red"}

arco = FSCM(config).fit()
```

After estimating, we can get the weights like

```{python}
weights_dict = arco['Weights'][0]
df = pd.DataFrame(list(weights_dict.items()), columns=['State', 'Weight'])
display(Markdown(df.to_markdown(index=False)))
```
These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the synthetic control, with only 6 being assigned positive weight.
