---
title: "Counterfactual Estimation via ```mlsynth```: A Practical Guide"
date: today
author:
  - name: "[Jared Greathouse](https://jgreathouse9.github.io)"
format:
  revealjs: 
    slide-number: true
    transition: fade
    scrollable: true
    background-color: "#EAF7F1"
    theme: clean.scss
highlight-style: github
---


## Counterfactual Estimation

- Maximizing value for business services and crafting effective public policy depends on knowing whether policies, promotions, new web features, taxes, or other interventions _meaningfully_ affect metrics that we care about.

- Researchers commonly use Difference-in-Differences designs and Synthetic Control methods (DID and SCM) to measure causal impacts.

- Both approaches use weighted averages of control groups from the pre-treatment period to estimate counterfactuals, or the post-treatment values of the outcome we'd see absent treatment.

---

## DID and SCM

- DID requires $\mathbb{E}[y_{1t}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t}(0)] = \mathbb{E}[y_{1t-1}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t-1}(0)]$, or parallel trends. Does not hold when the mean of controls is dissimilar to the trend of the treated unit/group.

For the SCM, we solve another least squares problem

\begin{align}
&\mathbf{w}^{\mathrm{SCM}}
= \underset{\mathbf{w}^{\mathrm{SCM}} \in \mathcal{W}_{\mathrm{conv}}}{\operatorname*{argmin}} \;
\left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w}^{\mathrm{SCM}} \right\|_2^2 \quad \forall \, t \in \mathcal{T}_{1}, \\
&\mathcal{W}_{\mathrm{conv}} \in \Big\{ \mathbf{w}^{\mathrm{SCM}} \in (\{0\} \cup \mathbb{R}_+)^{|\mathcal{N}_0|} : \sum_{j \in \mathcal{N}_0} w_j = 1 \Big\}.
\end{align}

- SCM struggles when $N_0>>T_0$, and is prone to overfitting in this setting. Also, smaller donor pools are preferable to mitigate interpolation biases. But picking a good donor pool can be challenging when there are very many controls.

---

## Many SCM Libraries Exist

| Estimator / Method       | Software  | Notes / Accessibility                                                                                                                  |
| ------------------------ | --------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Classical SCM**        | R / Stata | Original Abadie et al. implementation; widely used in policy/econ.                                                                     |
| **PySyncon**           | Python    | Python implementation of Robust/Augmented Synthetic Control.                                |
| **microsynth**           | R         | Focused on disaggregated data.                                                                 |
| **augsynth**             | R         | Augmented SCM; automates some covariate adjustments but still R-only.                                                                  |
| **sdid (Synthetic DID)** | R/Stata         | Implements Synthetic Difference-in-Differences.                                                            |

While having all these is nice, they are all scattered in differnt libraries.

---

## SCM Challenges in Practice

- **High-dimensional donor pools**: When $N_0 \gg T_0$, standard SCM can overfit/be undefined.  

- **Sparse weight assumption**: Convex SCM favors sparse solutions; if the true data-generating process is dense, the model misallocates weights.  

- **Donor pool selection**: Picking the "right" controls is tricky. Including irrelevant units can bias predictions.  

- **Missing data / noisy outcomes**: Classical SCM struggles if the pre-treatment data has high variance.   

**Implication:** Researchers need methods that handle high-dimensional, noisy, and imperfect real-world data while still producing interpretable counterfactuals.

---


## Enter ```mlsynth```...

- The Python library ```mlsynth``` is meant to further democratize causal inference and econometrics, making its benefits available to a wider class of researchers.

- ```mlsynth``` automates these estimators (and more!).

- Its syntax is very simple, and the steps are totally automated.

- This presentation will cover ```mlsynth``` and how to use it in practice. I will describe a few of the algorithms and give examples of how to use them.

## Installing ```mlsynth```

To install ```mlsynth```, we need:

- Python 3.9 or later

- ```cvxpy, matplotlib, numpy, pandas, scipy, scikit-learn, statsmodels, pydantic```

- And then, we can install this from the command line

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

---

## Using ```mlsynth```

- A long ```df``` where we have one observations per unit per time period.

- A Time variable (numeric or date-time are allowed).

- A unit variable (a string, to know which units are which).

- A numeric outcome variable.

- A dummy variable denoting treatment, 1 when the treatment is active and the unit is treated, else 0 for all other units and times.

---

## [Proposition 99](https://doi.org/10.1198/jasa.2009.ap08746)

Find the data [here](https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv).

::: {.panel-tabset}

### Inspection

This is the classical example from the Abadie 2010 paper.

```{python}
#| echo: true
#| results: 'hide'
#| fig-align: center

import pandas as pd
from mlsynth.utils.helperutils import sc_diagplot

# Load smoking data
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/main/basedata/smoking_data.csv"
df = pd.read_csv(url)

# Configure the SC_DIAGPLOT method
plotconfig = {
    "df": df,
    "outcome": df.columns[2],
    "treat": df.columns[-1],
    "unitid": df.columns[0],
    "time": df.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": "red",
    "method": "RPCA",
    "Frequentist": True
}

sc_diagplot([plotconfig])

```

Parallel trends does not appear to hold for California with respect to its donor pool. California has a steeper downward sloping trend of cigarette consumption compared to the average of the donor pool. No matter what scalar constant we would shift the mean of the donor pool by, the mean difference would not be constant with respect to California. This suggests one of two solutions is viable: either parallel trends would hold with _some_ control units, or we simply need to re-weight the donor pool to better match the pre-intervention trajectory.

### SCM

Here is the results of the ```CLUSTERSC``` class. This class implements the Robust Synthetic Control Method. We can see that this model fits the pre-treatment trajectory very well, without needing to use any of the seven covariates that the original paper uses.

```{python}
#| echo: true
#| results: 'hide'
#| fig-align: center

import pandas as pd
from mlsynth import CLUSTERSC

url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"
data = pd.read_csv(url)

SCconfig = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue"], "method": "PCR", "cluster": False
}

SCResult = CLUSTERSC(SCconfig).fit()
```

Of course, this estimator presumes that some units should matter more than others (hence the weighting scheme).

### FDID

```{python}
#| echo: true
#| results: 'hide'
#| fig-align: center

import pandas as pd
from mlsynth import FDID

url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"
data = pd.read_csv(url)

FDIDconfig = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue", "red"]
}

FDIDResult = FDID(FDIDconfig).fit()
```

Here all _selected_ units are given equal weight, adjusted only by an intercept.

:::
