---
title: 'Causal Inference Runs the World: Or, What Are "Actionable Insights", Anyways?'
date: 2025-03-07
categories: [Econometrics, Causal Inference, Data Science]
---

# What Even Are Actionable Insights?

Man, I hate industry jargon. Don't get me wrong, I'm not against all jargon. I've been in academia ten years, and I've spent maybe 6 of those being tormented by econometrics. We have our own jargon too, which doesn't always jive well with industry- I say "causal inference", marketers will say "incrementality testing". Maybe it's because I dislike corporate culture in general, but there's something about the data science industry lingo that irritates me. "Ambiguity" (I'll do a post on this one, one day). "Self starter". "_Business acumen_". And the favorite of the day, "actionable insights." This post is dedicated to showing you how I think about actionable insights, from the perspective of somebody who cares a lot about causal inference.

"Actionable insights". It's the one we hear about all the time, even if it isn't phrased quite like that. Job descriptions and data science pages will go on and on about how we're meant to deliver them as econometricians, policy analysts, and data scientists, staking our livelihoods to them. But, what are actionable insights anyways? [Some pages](https://loop-software.com/news-and-insights/what-are-actionable-insights) say "“Actionable insights” describes contextualised data analysis. It’s a piece of information that can actually be put into action — supported by the analysis, context and communication tools required to get the job done." I agree with this. This leads us to one question then: what is an insight and whatmakes an insight actionable? 

## A Motivating Example

Recently [I saw a post on LinkedIn](https://www.linkedin.com/posts/jared-greathouse-26315711a_data-science-intern-heres-a-bar-chart-activity-7301645259157970946-kJ_T?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2Q8asBlsCYlwKJgJ488VWbcV1CX14FdOw). It went something like this:

> Data Science Intern: "The chart shows that downloading of the App increased by 45%." Client: "So?"

>  Junior Data Scientist: "The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear." Client:  "Great! Why?"

> Principal Data Scientist: "The chart shows that downloading of the App increased by 45%, compared to the same time yesteryear due to our new pricing strategy. We should roll out Pricing Strategy in more markets since it'll likely increase revenue by Big Amount." Client:  "That's great! We'll get right to it."

In context, the post was about connecting your results to the next steps of actions businesses/clients should take. And that's great, we should all be doing that. But I find the hypothetical Principal DS's answer to be quite wanting. I do not understand why we are meant to see the Principal DS's remarks as any more wise than the Junior DS. Why? The Principal DS was making a counterfactual claim, implicitly. They were explaining to the client that their Policy W had X impact on Outcome Y, an even went as far as to say that if we kept doing Policy W, we may see gains elsewhere in the future. And this may be true, but how can we tell? For all the talk we have about showing impact in our resumes and results (instead of "I did X", do "I did X which had X impact on efficiency"), surely we can do better than this. How? Causal inference.

My issue with the post was this: the second you add the phrase "Impact X happened _given our new pricing strategy/policy/other intervention_", we're now very far aflung from Descriptiville, stuck in Counterfactual Land. In this domain, the governance is a bit different than simply getting Tableau/BI to make us a chart: here you see, we need to have some estimate of how sales (or whatever metric we're meant to care about) would've evolved ABSENT whatever the new policy was. Put differently, you can't just show a bar chart and say "You should do X cuz I made this chart and have verbally attached a reason to a number". It requires A LOT more work than that.

# Actionable Insights, Econometrics Style

## Defining the Problem

Before we get to the modeling (rest assured we will!) let's come up with a simple example. Suppose we work for Uber. Uber introduced Uber Green in September of 2020, as an initiative that is meant to (among other things) incentivize drivers to use electric cars/low emissions vehicles. For the purposes of this post, we'll be evaluating whether this policy in fact affected the number of drivers who use electric. Given this situation, we must roll out this intervention someplace first in order to see how it may work in other markets (say, Sayulita, Mexico), and we must generate a counterfactual. Or, the number of driver's who would've used electric cars, had we never rolled out Uber Green in this market at this time. In order to accomplish this task, I will use synthetic control based methodologies to estimate the impact. Of course, the goal here is to compute the average treatment effect on the treated, or the average of the differences between our treated unit and the out-of-sample predictions (post intervention period).

## Solving the Problem

<details>

Let $\mathcal{N}$ denote the set of cities indexed by $j$, where $N \coloneqq |\mathcal{N}|$ represents the total number of markets. Sayulita, the treated city, is indexed by $j = 1$, while the set of control cities is denoted by $\mathcal{N}_0 \coloneqq \mathcal{N} \setminus \{1\}$, with cardinality $N_0 \coloneqq |\mathcal{N}_0|$. Time periods are indexed by $t$, with pre-intervention periods $\mathcal{T}_1 \coloneqq \{1, 2, \dots, T_0\}$ and post-intervention periods $\mathcal{T}_2 \coloneqq \{T_0+1, \dots, T\}$, where $T_0$ is the last period before the intervention.

For each market $j$, let $\mathbf{y}_j \coloneqq [y_{jt}, \dots, y_{jT}]^\top \in \mathbb{R}^{T}$ represent the vector of new drivers who use electric cars, where $y_{jt}$ denotes the number of weekly new drivers in market $j$ at time $t$. Let $\mathbf{Y}_0 \coloneqq (\mathbf{y}_j)_{j \in \mathcal{N}_0} \in \mathbb{R}^{T \times N_0}$ be the matrix of control markets that did not do this intervention at this time. To estimate the counterfactual new driver supply in Sayulita, we construct a synthetic control by taking some weighted average of the control markets based on their pre-intervention trends, $\hat{\mathbf{y}}_1(0) = \mathbf{Y}_0 \mathbf{w}^\top$, where $\mathbf{w} \in \mathbb{R}^{N_0}$ is a vector of weights assigned to the control cities. These weights are chosen to minimize some loss function. The treatment effect at time $t$ is then estimated as the difference between the observed and counterfactual outcomes, $\widehat{\Delta}_t \coloneqq y_{1t} - \hat{y}_{1t}(0)$. Our result of interest is the average of these treatment effects over the post-intervention period:

$$
\widehat{ATT} \coloneqq \mathbb{E}_2[\widehat{\Delta}_t] = \frac{1}{T_2} \sum_{t \in \mathcal{T}_2} \widehat{\Delta}_t.
$$

If the introduction of Uber Green in Sayulita leads to an increase in the number of new drivers who use electric vehicles, we expect $\widehat{\Delta}_t$, we can then comment on how this program may be applied to other areas.[^1]. The observed new drivers who use electric cars in Sayulita follows a factor model:

$$
\mathbf{y}_1 = \mathbf{\Gamma} \mathbf{F} + \boldsymbol{\nu}_1 + \boldsymbol{\delta} \mathbb{1}(t \geq T_0),
$$

where $\mathbf{\Gamma} \in \mathbb{R}^{1 \times k}$ represents the factor loadings, $\mathbf{F} \in \mathbb{R}^{k \times T}$ is the matrix of latent common factors, and the factors evolve as:

$$
\mathbf{F}_t = \rho \mathbf{F}_{t-1} + \boldsymbol{\eta}_t, \quad \boldsymbol{\eta}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_k),
$$

where $\rho$ is the autocorrelation parameter and $\boldsymbol{\eta}_t$ is a noise term. The idiosyncratic error term, $\boldsymbol{\nu}_1$, is assumed to follow a normal distribution with zero mean and variance $\sigma^2$, i.e., $\boldsymbol{\nu}_1 \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$. The treatment effect vector $\boldsymbol{\delta} \coloneqq [\delta_{T_0+1}, \dots, \delta_T]^\top$ represents the change in driver supply due to the Uber Green intervention, which is assumed to affect Sayulita starting at time $T_0+1$.
</details>

Let's begin by plotting our outcome.

```{python}
#| fig-align: center
#| echo: false

import pandas as pd
from mlsynth.mlsynth import TSSC, FDID, CLUSTERSC, dataprep
import matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Custom theme to mimic Uber's style
ubertheme = {
    'axes.grid': True, 
    'grid.linestyle': '--',
    'legend.framealpha': 1,
    'legend.facecolor': 'white', 
    'legend.shadow': True, 
    'legend.fontsize': 14, 
    'legend.title_fontsize': 16,
    'xtick.labelsize': 14,
    'ytick.labelsize': 14, 
    'axes.labelsize': 16, 
    'axes.titlesize': 20, 
    'figure.dpi': 100, 
    'axes.facecolor': 'white', 
    'axes.edgecolor': 'black', 
    'axes.linewidth': 1, 
    'axes.grid': True,
    'grid.color': 'gray',
    'grid.linewidth': 0.5,
    'font.family': 'sans-serif', 
    'font.sans-serif': ['Helvetica', 'Arial'],
    'xtick.direction': 'in',
    'ytick.direction': 'in',
    'xtick.major.size': 5, 
    'ytick.major.size': 5,
    'axes.titleweight': 'bold', 
    'axes.labelweight': 'bold', 
}

matplotlib.rcParams.update(ubertheme)


def plot_treated_vs_controls(donor_matrix, treated_vector, pre_periods, title):
    """
    Plots a single treated unit against the control group.

    Parameters:
    - donor_matrix (numpy.ndarray): A 2D array where each column represents a control unit.
    - treated_vector (numpy.ndarray): A 1D array representing the treated unit.
    - pre_periods (int): The cutoff time index for the pre-treatment period.
    - title (str): The title of the plot.
    """

    # Indicate pre-treatment period cutoff
    plt.axvline(x=pre_periods, color='blue', linestyle='--', linewidth=1.5, label='Treatment Date')

    # Plot control group trajectories
    plt.plot(donor_matrix, color='gray', linewidth=0.2, alpha=0.35, label='_nolegend_')

    # Plot the average of control units
    average_controls = donor_matrix.mean(axis=1)
    plt.plot(average_controls, color='red', linewidth=1, label='Mean of Controls')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Sayulita')

    # Labels and legend
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.ylabel('New Drivers')
    plt.legend()

    plt.show()


np.random.seed(3111)

# Parameters
n_units = 100
n_periods = 104
n_factors = 12
rho = 0.85
sigma = 1.25

treated_unit = 54
treatment_period = 76
treatment_effect = 25

# List of Cities, for Monte Carlo Could simply be generalized to "Cities + n"
cities = [
"São Paulo", "Mexico City", "San Carlos de Bariloche", "Rio de Janeiro", "Ushuaia",
"Bogotá", "Santiago", "Caracas", "Guayaquil", "Quito",
"Brasília", "Montevideo", "Asunción", "El Paso", "Playa del Carmen",
"Medellín", "Porto Alegre", "Placencia", "Recife", "Salvador",
"Zihuatanejo", "San José", "Panama City", "Barranquilla", "Tegucigalpa",
"Foz do Iguaçu", "Maracaibo", "Rosario", "Maracay", "Antofagasta",
"San Pedro Sula", "San Juan", "Chihuahua", "Cayo District", "Maturín",
"Buzios", "Puebla", "Miami", "Arequipa", "Fernando de Noronha", "Guatemala City",
"Zacatecas", "Mérida", "Córdoba", "San Miguel", "Trujillo",
"Corozal Town", "Santa Cruz de la Sierra", "San Luis Potosí", "Jalapão", "Potosí",
"Tucumán", "Neuquén", "La Plata", "Sayulita", "Florianópolis", "Lagos de Moreno",
"La Paz", "Belém", "Venezuela", "Ribeirão Preto", "Valparaíso",
"Marília", "Campinas", "Vitoria", "Sorocaba", "Santa Fe",
"San Salvador", "Lima", "Buenos Aires", "Curitiba", "Maceió",
"Los Angeles", "La Ceiba", "Puerto La Cruz", "Olinda", "Monterrey",
"Ibagué", "Cúcuta", " Paraty", "Cancún", "Puerto Vallarta", "Chiclayo", "Ambato",
"Pucallpa", "Santa Marta", "Villavicencio", "Paraná", "Cauca", "San Vicente",
"Cali", "Tarija", "Manzanillo", "El Alto", "Santiago de Chile", "Cochabamba",
"Cartagena", "Santo Domingo", "Durango", "Puerto Viejo de Talamanca"
]

# Setting the common factors
common_factors = np.zeros((n_periods, n_factors))

# Setting the common factors for period 1
common_factors[0, :] = np.random.normal(0, 1, n_factors)

for t in range(1, n_periods):
common_factors[t, :] = rho * common_factors[t-1, :] + np.random.normal(0, 1, n_factors)

# Factor Loadings with error terms
factor_loadings = np.random.normal(0, 1, (n_units, n_factors))

# Error term for model
error_terms = np.random.normal(0, sigma, (n_periods, n_units))


data = np.zeros((n_periods, n_units))

for i in range(n_units):
data[:, i] = common_factors @ factor_loadings[i, :] + error_terms[:, i]+np.random.randint(40, 80)


data[treatment_period:, treated_unit] += treatment_effect


market_names = cities
time = np.repeat(np.arange(1, n_periods+1), n_units)
markets = np.tile(market_names, n_periods)

# Reshape data into the correct format for DataFrame
drivers = data.flatten()

indicator = np.where((markets ==cities[treated_unit]) & (time >= treatment_period), 1, 0)

# Create DataFrame
df = pd.DataFrame({
'Market': markets,
'Time': time,
'Drivers': drivers,
'Uber Green': indicator
})

treat = "Uber Green"
time = "Time"
outcome = "Drivers"
unitid = "Market"


prepped = dataprep(df, unitid, time, outcome, treat)

plot_treated_vs_controls(prepped["donor_matrix"], prepped["y"], prepped["pre_periods"], "Sayulita vs. Controls")

```

Here is our plot... Now to study this...

```{python}
#| fig-align: center
#| echo: false

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "counterfactual_color": "red"
}

model = CLUSTERSC(config)

arco = model.fit()

```

[^1]: Here is the footnote.
