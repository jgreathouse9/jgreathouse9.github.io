[
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, many datasets we work with come in pretty csv files that are clean. And while that’s great… oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible code/script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we’d need to ask AAA and pay thousands of dollars for an extended time series… but now we don’t need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for the scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA’s website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is “https://gasprices.aaa.com/?state=MA”. For Florida, the URL is “https://gasprices.aaa.com/?state=FL”. See the pattern? There’s a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python’s requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we’ve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year’s worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I’ve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "\nJared Greathouse\n",
    "section": "",
    "text": "Jared Greathouse\n\n\nEconometrician. Consultant.\n\n\n👋 Welcome\nI am Jared Amani Greathouse. I offer consulting services in data science and econometrics, especially with synthetic control methods.\n\n\n📄 Intake Form\nTo offer me a project:\n\nDownload the Request Intake Form (DOCX). If the form does not download, copy the link and put it in your browser.\nWhen it is filled out, email it to me with a short description of the project.\nFees begin at at $250/hour.\n\n\n\n📬 Contact\nSchool Email: jgreathouse3@student.gsu.edu (this is the best way to reach me)\nPersonal Email: j.greathouse200@gmail.com\nWebsite: jgreathouse9.github.io"
  },
  {
    "objectID": "fasc.html",
    "href": "fasc.html",
    "title": "Forward Selected Augmented Synthetic Controls",
    "section": "",
    "text": "Synthetic Control Methods (SCM) is a widely used framework for estimating causal effects when randomized experiments are not feasible. At its core, SCM constructs a weighted average of control (donor) units to approximate the treated unit’s pre-treatment trajectory. The goal is to find an in-sample/pre–treatment average of controls that closely mirrors the treated unit before the intervention.\nMuch of the method’s credibility hinges on the quality of this pre-treatment fit. Econometricians regularly warn that poor pre-treatment fit undermines the validity of SCM estimates. Even if the optimization problem is formally well-posed, poor alignment between the treated unit and its in-sample match can lead to substantial bias. The intuition is straightforward: if similar units are assumed to behave similarly, a control group that fails to mimic the treated unit before treatment is unlikely to produce a credible counterfactual afterward. Just as important as pre-treatment fit is the composition of the donor pool. Including irrelevant or poorly matched units, or omitting relevant ones, can distort the synthetic weights and lead to misleading inferences. But how should the donor pool be chosen?\nOne increasingly popular solution to the imperfect match is the Augmented Synthetic Control Method (ASCM), known in industry through Meta’s GeoLift library. Shops like Recast use it, and data scientists such as Mandy Liu and Svet Semov have helped bring it to applied audiences.\nMethods for donor pool selection have also received attention. In fact, this is part of what makes GeoLift so popular: it attempts to identify the most similar markets to a treated group before the intervention. In academic settings, approaches like forward selection, and even random forests have been proposed to automate or guide the choice of appropriate donors.\nBut what if we can do better?\nIn previous posts, I’ve written about donor selection strategies and how to handle imperfect pre-treatment fit. In this post, I introduce a synthesis of both: the Forward Augmented Synthetic Control estimator. By combining forward selection with a bias correction step, I show that we can reduce in-sample risk relative to the standard ASCM and Forward SCM alone. This approach is illustrated using two popular SCM case studies: the Kansas tax cut experiment and California’s Proposition 99.\n\n\nLet \\(\\mathbb{R}\\) denote the set of real numbers. Calligraphic letters, such as \\(\\mathcal{S}\\), represent discrete sets with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) index a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Unit \\(j=1\\) is the treated unit, with the set of control units \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\) of cardinality \\(N_0\\). The pre-treatment period is \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\le T_0 \\}\\) and the post-treatment period is \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}\\). The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), and the outcome vector for unit \\(j\\) is \\(\\mathbf{y}_j = (y_{j1}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^T\\). The treated unit’s outcome vector is \\(\\mathbf{y}_1\\), and the donor matrix is \\(\\mathbf{Y}_0 = \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\).\nWe frequently consider linear combinations of donor outcomes. The convex hull of the donor vectors is\n\\[\n\\operatorname{conv}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\},\n\\]\nand the affine hull is\n\\[\n\\operatorname{aff}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\}.\n\\]\nWhile both involve weighted averages of donors, the convex hull restricts weights to be non-negative, whereas the affine hull allows negative weights and extrapolation beyond the convex hull.\nThe corresponding sets of feasible weights are\n\\[\n\\mathcal{W}_{\\mathrm{conv}} = \\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}, \\quad\n\\mathcal{W}_{\\mathrm{aff}} = \\{ \\mathbf{w} \\in \\mathbb{R}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}.\n\\]"
  },
  {
    "objectID": "fasc.html#forward-selection-scm-fscm",
    "href": "fasc.html#forward-selection-scm-fscm",
    "title": "Forward Selected Augmented Synthetic Controls",
    "section": "Forward Selection SCM (FSCM)",
    "text": "Forward Selection SCM (FSCM)\nFSCM constructs a synthetic control iteratively by adding one donor at a time. Starting from the empty set \\(\\mathcal{S} = \\emptyset\\), at each iteration the algorithm considers each candidate donor \\(j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}\\) and forms \\(\\mathcal{S}^\\prime = \\mathcal{S} \\cup \\{j\\}\\). For each candidate set \\(\\mathcal{S}^\\prime\\), it solves the restricted SCM problem over the donor submatrix \\(\\mathbf{Y}_0^{\\mathcal{S}^\\prime}\\), defined as the columns of \\(\\mathbf{Y}_0\\) corresponding to units in \\(\\mathcal{S}^\\prime\\):\n\\[\n\\mathbf{w}_{\\mathcal{S}^\\prime}^\\ast =\n\\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime)}{\\operatorname*{argmin}} \\;\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, \\mathcal{S}^\\prime} \\mathbf{w} \\right\\|_2^2\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|\\mathcal{S}^\\prime|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe donor selected at each iteration is\n\\[\nj^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}}{\\operatorname*{argmin}} \\;\n\\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S} \\cup \\{j\\})}\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1}{}_{\\mathcal{S} \\cup \\{j\\}} \\mathbf{w} \\right\\|_2^2\n\\]\nand it is then added to the selected set by updating\n\\[\n\\mathcal{S} \\leftarrow \\mathcal{S} \\cup \\{j^\\ast\\}.\n\\]\nThe donor selection process proceeds sequentially according to this forward selection rule. In principle, the procedure may continue until all donor units have been exhausted, but in practice analysts use stopping rules to terminate the selection at a sensible point.\n\nExhaustive Search\nIn the exhaustive option, selection proceeds until the pre-treatment mean squared error (MSE) is minimized over all candidate donor subsets. Let \\(\\mathcal{P}(\\mathcal{N}_0)\\) denote the power set of donor indices, and for any \\(S \\subseteq \\mathcal{N}_0\\), define the within–pre-treatment fit as\n\\[\n\\text{MSE}(S) = \\frac{1}{T_0} \\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S)} \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^S \\mathbf{w} \\right\\|_2^2,\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(S) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|S|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe exhaustive-search estimator iestimates \\(\\sum_{i=0}^{k-1} (N_0 - i) = k N_0 - \\frac{k(k-1)}{2}\\) total models. It chooses\n\\[\nS^\\ast = \\underset{S \\in \\mathcal{P}(\\mathcal{N}_0)}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1,S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nThis means we continue until all donors are eventually selected.\n\n\nInformation Criteria, per Shi and Huang, 2023\nAnother option is to rely on a model selection criterion, such as the modified BIC (mBIC), which balances pre-treatment fit with model complexity. The modified BIC for a selected set \\(S\\) of donor units is defined as\n\\[\n\\text{mBIC}(S) = T_0 \\cdot \\log(\\text{MSE}) + |S| \\cdot \\log(T_0),\n\\]\nwhere\n\\[\n\\text{MSE} = \\frac{1}{T_0} \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, S} \\mathbf{w}_S \\right\\|_2^2.\n\\]\nThe forward selection procedure may terminate when adding a new donor increases the penalized error:\n\\[\n\\text{mBIC}(S \\cup \\{j\\}) &gt; \\text{mBIC}(S).\n\\]\n\n\nDonor Pool Cap\nA final practical option is to impose an upper bound on the number of selected donors. When a cardinality cap is imposed, let \\(p \\in (0,1]\\) and \\(K = \\lfloor p N_0 \\rfloor\\), so that the search is restricted to subsets \\(S \\subseteq \\mathcal{N}_0\\) with \\(|S| \\le K\\). The corresponding FSCM estimator chooses\n\\[\nS^\\ast = \\underset{\\substack{S \\subseteq \\mathcal{N}_0 \\\\ |S| \\le K}}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^{S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nWhichever variant of the forward selection algorithm is used—exhaustive search, model selection, or cardinality constraint—the result is a selected subset of donors \\(S^\\ast \\subseteq \\mathcal{N}_0\\) and a corresponding optimal weight vector \\(\\mathbf{w}_{S^\\ast}^\\ast \\in \\mathbb{R}^{|S^\\ast|}\\). To unify notation and simplify presentation going forward, we define the final forward-selected weights by embedding this sparse solution into the full donor space:\n\\[\nw_j^{\\mathrm{FSCM}} =\n\\begin{cases}\n\\left(\\mathbf{w}_{S^\\ast}^\\ast\\right)_j, & \\text{if } j \\in S^\\ast, \\\\\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nEquivalently, let \\(\\mathbf{w}^{\\mathrm{FSCM}} \\in \\mathbb{R}^{N_0}\\) be the zero-padded extension of \\(\\mathbf{w}_{S^\\ast}^\\ast\\), such that it assigns zero weight to all donors not selected by FSCM. This implementation provides flexibility in practice: exhaustive evaluation is feasible in low-dimensional settings, while early stopping or a capped selection is recommended in high-dimensional applications. The algorithm in mlsynth issues a warning if the donor pool contains 200 or more units."
  },
  {
    "objectID": "fasc.html#the-augmented-synthetic-control-estimator",
    "href": "fasc.html#the-augmented-synthetic-control-estimator",
    "title": "Forward Selected Augmented Synthetic Controls",
    "section": "The Augmented Synthetic Control Estimator",
    "text": "The Augmented Synthetic Control Estimator\nBuilding on this baseline formulation, the ASCM introduces a regularization term that penalizes deviations of the weight vector from a reference or initial weight vector, \\(\\mathbf{w}_0\\). The augmented objective can be written as\n\\[\n\\mathbf{w}^{\\mathrm{aug}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}} \\right\\|_2^2.\n\\]\nwhere \\(\\lambda \\ge 0\\) controls the strength of the penalty.\n\n\nAbout that lambda…\n\n\n\n\n\n\n\nNote\n\n\n\nOne may ask “Jared, why did you include the penalty on the weight deviation term instead of the fit term, as Ben-Michael and co. do in Equation 18 of their paper?” Here’s why.\nIn ASCM, the placement of the regularization parameter \\(\\lambda\\) determines how the estimator balances pre-treatment fit and fidelity to the original SCM weights. Their formulation minimizes:\n\\[\n\\mathbf{w}^\\ast_{\\text{alt}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\lambda \\|\\mathbf{y}_1^{\\mathcal{T}_1}- \\mathbf{Y}_{0}^{\\mathcal{T}_1}\\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nwhile ours solves:\n\\[\n\\mathbf{w}^{\\mathrm{aug}}= \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\|\\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\lambda \\|\\mathbf{w}^{\\mathrm{aug}} -\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nIt turns out these are mathematically equivalent under a simple reparameterization. If we define \\(\\lambda_{\\text{alt}} = \\frac{1}{\\lambda_{\\text{aug}}}\\), then both objectives yield the same solution. This follows directly from the first-order conditions of each problem, which differ only by a scaling of the Lagrange multiplier. So in truth, it’s just a matter of which interpretation you find more natural.\nI personally prefer our formulation because as \\(\\lambda \\to \\infty\\), the penalty on deviation dominates, and \\(\\mathbf{w}^\\ast_{\\text{aug}}\\) collapses to the projection of \\(\\mathbf{w}^{\\mathrm{SCM}}\\) onto the affine constraint. In other words, when the original FSCM fit is already good, we want to stick close to it. Conversely, as \\(\\lambda \\to 0\\), the regularization term disappears and the solution becomes the best-fitting affine combination of the donor units, completely unconstrained by the initial weights. That’s appropriate when the original fit is poor and we’re willing to learn something new (although this is probably going to be uncommon in practice). So while the math is equivalent, the perspective isn’t. I find it much more natural to think of \\(\\lambda\\) as controlling how much I “trust” the prior weights. And that’s easier to reason about when \\(\\lambda\\) is attached to the deviation term.\n\n\n\nThe intuition here is pretty simple. If the SCM weights are already giving us good pre-treatment fit, then there is little incentive to extrapolate away from the original (F)SCM solution. However, if there’s need for better fit, then we will extrapolate away from the convex hull solution. In practice, Ben-Michael and co advocate for choosing lambda via cross validation."
  },
  {
    "objectID": "fasc.html#conformal-prediction-intervals",
    "href": "fasc.html#conformal-prediction-intervals",
    "title": "Forward Selected Augmented Synthetic Controls",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\nTo quantify uncertainty around the estimated counterfactual trajectory, we apply conformal prediction intervals based on block-permuted pre-treatment residuals. These intervals are distribution-free, require no assumptions about the data-generating process, and provide valid finite-sample marginal coverage. Let \\(\\hat{y}_t^{\\text{cf}}\\) denote the estimated counterfactual outcome at time \\(t\\), and let \\(y_t^{\\text{obs}}\\) be the observed outcome. We begin by computing residuals for all time periods:\n\\[\n\\varepsilon_t = y_t^{\\text{obs}} - \\hat{y}_t^{\\text{cf}}.\n\\]\nWe then construct a conformal score by calculating the mean absolute residual over the post-treatment period. To simulate the distribution of this score under the null (i.e., assuming no treatment effect), we perform circular block permutations of the residual vector and recompute the same statistic for each shifted version.\nThis yields an empirical distribution of conformal scores under the null. We take the \\((1 - \\alpha)\\) quantile of this distribution as our conformal threshold, denoted \\(q_{1 - \\alpha}\\). To center the interval, we compute the mean residual over the pre-treatment period:\n\\[\n\\bar{\\varepsilon} = \\frac{1}{T_0} \\sum_{t \\leq T_0} \\varepsilon_t.\n\\]\nThe conformal prediction interval for each post-treatment time \\(t &gt; T_0\\) is then given by:\n\\[\n\\left[ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} - q_{1 - \\alpha},\\ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} + q_{1 - \\alpha} \\right].\n\\]\nThis approach ensures that the prediction intervals account for uncertainty in the counterfactual trajectory while adjusting for systematic bias in the pre-treatment fit. The shaded regions in our figures visualize these conformal intervals."
  },
  {
    "objectID": "fscm.html",
    "href": "fscm.html",
    "title": "Forward Selected Synthetic Control",
    "section": "",
    "text": "Interpolation bias is a known issue with synthetic control models (SCMs) For valid counterfactual prediction, the donor units, or the set of units that were never exposed to an intervention, should be as similar as possible to the treated unit in the pre-treatment periods. Selecting an appropriate donor pool is therefore critical, for practitioners. However, this can be challenging in settings with many potential controls, potentially many more control units than pre-treatment periods. Practically, researchers may wish to use this method when they have a high-dimensional donor pool and may be unsure as to which donors to include to reduce the impact of interpolation biases. To this end, this blog post introduces users the Forward Selected SCM. This applies Forward Selection (FS) to choose the donor pool for a SCM before estimating out-of-sample predictions."
  },
  {
    "objectID": "fscm.html#notation",
    "href": "fscm.html#notation",
    "title": "Forward Selected Synthetic Control",
    "section": "Notation",
    "text": "Notation\nLet \\(\\mathbb{R}\\) denote the set of real numbers. A calligraphic letter, such as \\(\\mathcal{S}\\), represents a discrete set with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) represent indices for a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Let \\(j = 1\\) be the treated unit, with the set of controls being \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0\\). The pre-treatment period consists of the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\},\\) where \\(T_0\\) is the final period before treatment. Similarly, the post-treatment period is given by \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}.\\) The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), where a generic outcome vector for a given unit in the dataset is \\(\\mathbf{y}_j \\in \\mathbb{R}^T\\), where \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^{T}\\). The outcome vector for the treated unit specifically is \\(\\mathbf{y}_1\\). The donor matrix, similarly, is defined as \\(\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\), where each column indexes a donor unit and each row is indexed to a time period.\nSCM estimates the counterfactual outcome for the treated unit by solving the program\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} \\|_2^2 \\: \\forall t \\in \\mathcal{T}_1.\n\\]\nWe seek the weight vector, \\(\\mathbf{w}\\), that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. For our purposes, the space of SC weights is the \\(N_0\\)-dimensional probability simplex \\(\\Delta^{N_0} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{N_0} : \\|\\mathbf{w}\\|_1 = 1 \\right\\}.\\) Practically this means that the post-intervention predictions will never be greater than the maximum outcome of the donor pool or lower than the minimum outcome of the donor pool."
  },
  {
    "objectID": "fscm.html#step-1",
    "href": "fscm.html#step-1",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 1",
    "text": "Step 1\nFS proceeds over \\(K \\in \\mathbb{N}\\) iterations, builds a sequence of tuples, \\(\\mathbb{T} = \\{(\\mathcal{S}_K, \\text{MSE}_K) \\}_{K=1}^{N_0}\\). The tuple contains two elements: the selected donor set for the \\(K\\)-th iteration and its corresponding \\(\\text{MSE}_K\\) (or the pre-treatment mean squared error). We begin by minimizing the SCM objective function as above, cycling through each donor unit vector one at a time instead of using the full control group. We denote these as submodels, which returns \\(N_0\\) one unit SCM models. We choose the single donor unit (the nearest neighbor in this specific case) that minimizes the MSE among all the \\(N_0\\) submodels. Our first tuple, then, is built with this single donor unit and the model’s corresponding MSE\n\\[\n\\mathcal{S}_1 = \\{j^\\ast\\}, \\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0}{\\operatorname*{argmin}} \\ \\text{MSE}(\\{j\\}).\n\\]"
  },
  {
    "objectID": "fscm.html#step-2",
    "href": "fscm.html#step-2",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 2",
    "text": "Step 2\nFor \\(K=2\\), we now estimate \\(N_0-1\\) two-unit SCMs. We include the originally selected donor along with the remaining controls, one remaining donor at a time. As above, the first and second elements of the second tuple, respectively, are\n\\[\n\\mathcal{S}_2 = \\mathcal{S}_1 \\cup \\{j^\\ast\\},\\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}_{K-1}}{\\operatorname*{argmin}} \\ \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j\\}).\n\\]\nNow, we have two tuples and two MSEs to choose from."
  },
  {
    "objectID": "fscm.html#generalizing",
    "href": "fscm.html#generalizing",
    "title": "Forward Selected Synthetic Control",
    "section": "Generalizing",
    "text": "Generalizing\nThis process generalizes, continuing for the rest of the donor pool. The general form for this algorithm, then, is\n\\[\n(\\mathcal{S}_K, \\text{MSE}_K) = (\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\}, \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\})).\n\\]\nThe algorithm continues until \\(S_K=N_0\\), when there are no more donors to add. FSCM chooses the tuple with the lowest \\(\\text{MSE}\\):\n\\[\n\\mathcal{S}^{\\ast} = \\underset{(\\mathcal{S}_K, \\text{MSE}_K) \\in \\mathbb{T}}{\\operatorname*{argmin}} \\ \\text{MSE}_K.\n\\]\nas the optimal donor set, \\(\\mathcal{S}^{\\ast}\\). Note that even within \\(\\mathcal{S}^{\\ast}\\) (as we will see below), some donors may receive zero weight in the final solution. The selected donors are just the units selected for inclusion in the donor pool in the first place, they are no guarantee of the unit having positive weight. This is in contrast to methods such as Forward Difference-in-Differences or the FS panel data method. Both of these designs are available in mlsynth too, in the FDID class and PDA class with the method of fs (the default). The main difference here is that FDID can never overfit because it estimates only one parameter, whereas (in theory) FSCM and fsPDA can overfit if they end up including too many parameters in the regression model. Unclear how likely this is, since as we see below, teh FS method reduces the full donor pool to just under half of the originally selected donor units."
  },
  {
    "objectID": "fscm.html#fscm-in-mlsynth",
    "href": "fscm.html#fscm-in-mlsynth",
    "title": "Forward Selected Synthetic Control",
    "section": "FSCM in mlsynth",
    "text": "FSCM in mlsynth\nNow I will give an example of how to use FSCM for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nAnd then we load the Proposition 99 dataset and fit the model in the ususal mlsynth fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.\n\nimport pandas as pd # To work with panel data\n\nfrom IPython.display import display, Markdown # To create the table\n\nfrom mlsynth import FSCM # The method of interest\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\n# Feel free to change \"smoking\" with \"basque\" above in the URL\n\ndata = pd.read_csv(url)\n\n# Our method inputs\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = FSCM(config).fit()\n\n\n\n\n\n\n\n\nAfter estimation, we can get the weights. These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the SC, with only 6 being assigned positive weight. The ATT of Prop 99 as estimated by FSCM is -19.51 and the pre-treatment Root Mean Squared Error for FSCM is 1.66. I compared these results to the same results we get in Stata, which includes the covariates that Abadie, Diamond, and Hainmuller originally adjusted for as well as customizes the period over which to minimize the MSE. The ATT using the original method is -19.0018, and the RMSE for the pre-treatment period is 1.76. The corresponding weights using the full donor pool are 0.334 for Utah, 0.235 for Nevada, 0.2020 for Montana, Colorado 0.161, and Connecticut 0.068. So as we can see, the ATTs are very similar, and the pre-treatment prediction errors are pretty much the same. When we estimate this in Stata (omitting the auxilary covariate predictors and estimate synth2 cigsale cigsale(1988) cigsale(1980) cigsale(1975) , trunit(3) trperiod(1989) xperiod(1980(1)1988) nested fig, we get a RMSE of 4.33 and an ATT of -22.88. Furthermore, with this specification, the weights are no longer a sparse vector.\nThe point of this article is very simple. The original SCM works well, however it can be very sensitive to the inclusion of covariates, which covariates are included, what their lags are, and so on and so forth. Furthermore, there is also an issue of covariate selection in settings where we have multiple covariates that can potentially inform our selection of the donor pool. Furthermore, collecting a rich list of covariates may also not be possible in some settings. In such situations, especially without some pre-existing grount truth donor pool, analysts may apply the FSCM algorithm to guard against interpolation biases.\nAt least with the California example (and West Germany and Basque datasets, which I also tested), we can sometimes get comparable results to the baseline estimates which used multiple covariates for acceptable results (in all three of the standard test cases, FSCM actually get lower MSE than the original applications). In the Proposition 99 example, we select some of the same donor units, get a slightly better MSE and a very similar ATT without needing to fit to the covariates originally specified in the JASA paper.\nThe promise of machine-learning methods in this space is to automate away donor/predictor selection to some acceptable degree. The key thing of interest (for me, from an econometric theory perspective anyways) is which methods are best suited for this task, when do they perform well, and why. For example, it might be useful to derive bias bounds for this estimator to quantify how much the MSE should improve by compared to the original SCM and Forward DID, as has been done with clustering based methods, for example.\nA final caveat: in the original paper, Giovanni uses cross-validation to estimate this model, and he also employs the same covariates. I have not done the cross validation yet on my end, but I will very soon. As ususal, email me with questions or comments."
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we’re scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it’s just “scrape”, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that’s the language I use, but I’m certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don’t add them and then the job ends (this is what happens if I try to run the action after it’s ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I’m updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I’m scraping (unless you’re a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I’m home to personally oversee it. The value here is that I’ve manually gotten my computer to do a specific task every single day, the correct way (assuming you’ve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA’s site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it’s worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you’re a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we’ve automated our tasks correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Analysis, Data Science, and Causal Inference",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "nsc.html",
    "href": "nsc.html",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "",
    "text": "Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where the non-linear synthetic control method comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works using the Proposition 99 dataset."
  },
  {
    "objectID": "nsc.html#optimization-problem",
    "href": "nsc.html#optimization-problem",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Optimization Problem",
    "text": "Optimization Problem\nLet \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the outcome vector for the treated unit over the pre-treatment period \\(\\mathcal{T}_1\\), and let \\(\\mathbf{Y}_0^{\\text{pre}} \\in \\mathbb{R}^{T_0 \\times N_0}\\) be the matrix of pre-treatment outcomes for the control units indexed by \\(\\mathcal{N}_0\\). We seek a weight vector \\(\\mathbf{w} \\in \\mathbb{R}^{N_0}\\) that minimizes the following objective:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{N_0}} \\quad \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\right\\|_2^2 + a \\sum_{j \\in \\mathcal{N}_0} \\delta_j |w_j| + b \\|\\mathbf{w}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}\\|_1 = 1\n\\]\nThe first term is the standard squared reconstruction error. The second term introduces a discrepancy-weighted \\(\\ell_1\\) penalty, where the discrepancy vector \\(\\boldsymbol{\\delta} \\in \\mathbb{R}^{N_0}\\) is defined by:\n\\[\n\\delta_j = \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{y}_j^{\\text{pre}} \\right\\|_2, \\quad \\forall j \\in \\mathcal{N}_0\n\\]\nThis term penalizes control units that differ more from the treated unit in pre-treatment trends. The final term is a Tikhonov regularization penalty that shrinks the weight vector toward zero to improve stability. The parameters \\(a &gt; 0\\) and \\(b &gt; 0\\) control the strength of the \\(\\ell_1\\) and \\(\\ell_2\\) penalties, respectively. The constraint \\(\\|\\mathbf{w}\\|_1 = 1\\) retains the SC to the probability simplex \\(\\Delta^{N_0 - 1}\\), forming an affine combination of the control units."
  },
  {
    "objectID": "nsc.html#cross-validation-to-tune-a-and-b",
    "href": "nsc.html#cross-validation-to-tune-a-and-b",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Cross-Validation to Tune \\(a\\) and \\(b\\)",
    "text": "Cross-Validation to Tune \\(a\\) and \\(b\\)\nTo select optimal regularization parameters \\((a, b)\\), we perform \\(k\\)-fold cross-validation using a pseudo-treated framework. Each control unit \\(j \\in \\mathcal{N}_0\\) is treated as if it were the treated unit, and we aim to reconstruct \\(\\mathbf{y}_j^{\\text{pre}}\\) from the remaining controls. For a given pseudo-treated unit \\(j\\), let \\(\\mathcal{N}_0^{(-j)} = \\mathcal{N}_0 \\setminus \\{j\\}\\) and define the donor matrix \\(\\mathbf{Y}_0^{\\text{pre}, (-j)}\\) by removing column \\(j\\) from \\(\\mathbf{Y}_0^{\\text{pre}}\\).\nFor each fold and pseudo-treated unit, we solve:\n\\[\n\\min_{\\mathbf{w}^{(j)} \\in \\mathbb{R}^{N_0 - 1}} \\quad \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)} \\right\\|_2^2 + a \\sum_{k \\in \\mathcal{N}_0^{(-j)}} \\delta_k^{(j)} |w_k^{(j)}| + b \\|\\mathbf{w}^{(j)}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}^{(j)}\\|_1 = 1\n\\]\nwhere\n\\[\n\\delta_k^{(j)} = \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{y}_k^{\\text{pre}} \\right\\|_2, \\quad \\forall k \\in \\mathcal{N}_0^{(-j)}\n\\]\nLet \\(\\widehat{\\mathbf{y}}_j^{\\text{pre}} = \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)}\\) denote the predicted outcome. The validation error for unit \\(j\\) is given by:\n\\[\n\\text{MSE}^{(j)} = T_0^{-1} \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\widehat{\\mathbf{y}}_j^{\\text{pre}} \\right\\|_2^2\n\\]\nThe cross-validated error is averaged over all pseudo-treated units and all folds. We then select:\n\\[\n(a^\\ast, b^\\ast) = \\arg\\min_{a \\in \\mathcal{A},\\, b \\in \\mathcal{B}} \\; \\text{CVError}(a, b)\n\\]\nHere, \\(k\\) indexes the donor units in the pseudo-treated optimization problem. \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are grids of candidate values. In practice, I employ sklearn’s cross-validation utilities to accelerate computation. I had to go rogue from the original approach because on my end it was computationally intensive. This modification seems to perform comparably while dramatically improving runtime. So while the empirical results will not be the same, they are about what we’d expect."
  },
  {
    "objectID": "sccare.html",
    "href": "sccare.html",
    "title": "Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?",
    "section": "",
    "text": "I was talking with someone recently about applying SCM, and I mentioned the idea of using a richer donor pool with different kinds of donors to answer a question. I had to articulate myself kind of carefully, since they were kind of puzzled about what I meant at first. For me, the idea comes very naturally to me, where we literally do things like this all the time. My intuition however comes from thinking about these models a lot (and empirical proof/evidence), and I think it may help others.\nMany people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant’Anna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.\nHowever, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like mlsynth or geolift on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantly, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory that leads them to using sub-optimal estimators or not doing basic checks. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective in the very first place. This is much more crucial because if you do not know when your methods are expected to work well, you will not know if they fail or why beyond “that does not look right”. In a sense, you are sort of blindly applying calculus and algebra without much thought as it how any of it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.\nThis is kind of a broader problem in data science in many respects. Ostensibly, there are many myths about SCM, one of which is “the myth that SCM/[Augmented] SCM[s] don’t have assumptions like parallel trends”. I do not know who believes this myth, and I am inclined to think that widespread belief of this myth… is itself a myth. After all, even your most applied economists will tell you “yeah, SCM definitely has assumptions”. They may not be able to articulate what they are as formally as they could OLS, for example, but everybody in my experience knows SCM has assumptions that need to be met. Of course, whether they verify them is another matter entirely (if you know anybody who truly holds these beliefs about SCM not having assumptions, direct them to me so I can chat with them).\nEither way, even there are not many people who confidently say/think “SCM has no assumptions”, a much better argument I think can be made that even if people do not literally think this… this is certainly the way many researchers act in practice. And that is the point of this post. Knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain. As with cars, you may not know how to build an engine, but you will be well served if you know how and when to change spark plugs or fill your tire with air.\n\nBasic SCM\nPeople often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this has been revised substantially in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or latent variable model) that often serves as a motivating data generating process. The linear factor model takes the form:\n\\[\nY_{it}(0) = \\boldsymbol{\\lambda}_t^\\top \\boldsymbol{\\mu}_i + \\varepsilon_{it}.\n\\]\nWhat does this mean? It simply means that our outcome observations are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units, plus some error term. These common factors may indeed affect each unit differently (in DID, the relationship is additive, not multiplicative), but the key idea is that we can use their similarity in terms of how they interact with the factor loadings to our advantage. Econometricians all the time tell us that SCM fundamentally is about matching our treated unit’s common factors to the common factors embedded in the donor set (also see conditons 1-6 here). This idea is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual for one unit or group using only the control units that behave similarly to the treated unit. If we get a good match (usually), we are likely also matching close to the factor loadings. This idea holds regardless of what kind of control units they are.\nA more flexible idea is the fine-grained potential outcomes model. This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit’s population.\nThe practical implication of this is that we are allowed to use different donor types (comparing cities to states, for example) on the condition that they help us learn about the trajectory of the target unit in the pre-intervention period. SCM does not care about what kind of donors you use, so long as they are informative donors.\n\n\nApplication\nUnconvinced? We can replicate some results to show this using mlsynth.\nReaders likely know the classic example of Prop 99, where California’s anti-tobacco program was compared to 38 donor states to estimate California’s counterfactual per capita cigarette consumption. But California is a ridiculously large economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units?\nIt turns out that we can do just that. Here, I compare California to the donor divisions of the United States. The outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level. California’s remains as is. Just for demonstration, I compare the standard results (around an ATT of -17) to what we get when we use other estimators, except I aggregate the outcomes as I just described. Comparing across estimators, we can visually assess how well each method captures the pre and post-treatment trajectory for California using the aggregated division data, relative to the baseline result that uses the states as donors.\n\nimport pandas as pd\nfrom mlsynth import FDID, TSSC, PDA\n\n# URL of the .dta file\nurl = \"http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta\"\n\n# Load the Stata file directly from the URL\ndf = pd.read_stata(url)\n\ndf[\"Proposition 99\"] =  ((df[\"region\"] == \"California\") & (df[\"year\"].dt.year &gt;= 1989)).astype(int)\n\n# Base configuration\nbase_config = {\n    \"df\": df,\n    \"outcome\": df.columns[2],\n    \"treat\": df.columns[-1],\n    \"unitid\": df.columns[0],\n    \"time\": df.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\", \"red\"]\n}\n\n# TSSC model\ntssc_config = base_config.copy()  # Start with base and modify if necessary\narco = TSSC(tssc_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/estimators/tssc.py:408: UserWarning: Warning: Mismatch between number of donor weights (9) and names (8) for method MSCa. Donor weights will not be populated for this method.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/estimators/tssc.py:408: UserWarning: Warning: Mismatch between number of donor weights (9) and names (8) for method MSCc. Donor weights will not be populated for this method.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nHere is TSSC estimator, where we adjust for baseline differences using an intercept.\n\n# PDA model with method 'l2'\npda_config = base_config.copy()\npda_config[\"method\"] = \"l2\"\narcol2 = PDA(pda_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n\n\n\n\n\n\n\n\n\nHere is \\(\\ell_2\\)-PDA estimator, where we allow for negative weights and an intercept.\n\n# FDID model\nfdid_config = base_config.copy()\narcofdid = FDID(fdid_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n\n\n\n\n\n\n\n\n\nHere is the predictions of the FDID estimator, where we employ Forward Selection to choose the donor pool for the DID method.\n\n\nTakeaway\nThe qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California’s pre-treatment trends, and so long as we are doing that, we may generally use as many relevant donors as we like. The issue is not that donors in general do not matter. I wrote mlsynth precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit, especially in high-dimensional settings. The key issue though is that the “right donors” do not necessarily need to be the same type of unit as the kind you are using, meaing we can be a little creative as to what we use as a donor. This is a philosophical question, not an econometric one. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.\nNote that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy. Or, you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the theoretical proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you will be able to use these methods more confidently, and with much more clarity than before.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scmo.html",
    "href": "scmo.html",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "",
    "text": "Sometimes analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may possibly predict a target/outcome variable that we care about, and plenty of other papers have commented on this fact before. Some analysts even argue for the surrogate approach, where we treat other outcomes or affected units as a kind of instrument for the counterfactual trajcectory of the target unit.\nHowever, all of this is most uncommon. As it turns out, most people in academia and industry who use synthetic controls use only a single focal outcome in their analyses. Perhaps they will adjust their unit weights by some diagonal matrix, a diagonal matrix \\(\\mathbf{V}\\) in most applications. The point of this matrix is basically to assist the main optimization in choosing the unit weights. However, even this is limited by the number of pretreatment periods you have- if you have more covariates than you have pretreatment periods, you cannot estimate the regression. Recent papers by econometricians have tried to get around this, though. This blog post covers a few recent recent papers which have advocated for this. I explain the econometric method and apply it in a simulated setting."
  },
  {
    "objectID": "scmo.html#standard-synthetic-control",
    "href": "scmo.html#standard-synthetic-control",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Standard Synthetic Control",
    "text": "Standard Synthetic Control\nBefore introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator minimizes\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThis is a constrained least squares program in which we regress the treated unit’s pre-treatment outcomes onto the control matrix under the constraint that \\(\\mathbf{w}\\) lies in the simplex \\(\\Delta^{N_0}\\). For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights \\(\\mathbf{w}^\\ast\\) are estimated, the out-of-sample estimates are obtained by applying the same weights to the control matrix\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0^{\\text{post}} \\mathbf{w}^\\ast,\n\\]\nwith the concatenation between the in and out of sample vectors corresponding to the full predictions of the model. The estimated treatment effect at each post-treatment time point is then given by the difference between observed and out-of-sample outcomes: \\(\\hat{\\tau}_{1t} = y_{1t} - \\hat{y}_{1t}\\) for \\(t \\in \\mathcal{T}_2\\)."
  },
  {
    "objectID": "scmo.html#model-averaging",
    "href": "scmo.html#model-averaging",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Model Averaging",
    "text": "Model Averaging\nWe may also model average these models together, which sometimes results in better fit than using either model alone. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have \\(\\mathbf{y}_1^{\\text{CAT}} \\in \\mathbb{R}^{T_0}\\), which denotes the pre-treatment fit from the concatenated model by Tian, Lee, and Panchenko, and on the other hand, we have \\(\\mathbf{y}_1^{\\text{AVG}} \\in \\mathbb{R}^{T_0}\\), the corresponding fit from the demeaned model by Sun, Ben-Michael, and Feller. As before, we observe the treated unit’s pre-treatment trajectory, \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\).\nTo begin, we stack the two counterfactuals into a single matrix:\n\\[\n\\mathbf{Y}^{\\text{MA}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT}} & \\mathbf{y}_1^{\\text{AVG}}\n\\end{bmatrix} \\in \\mathbb{R}^{T_0 \\times 2}.\n\\]\nWe define the model-averaged pre-treatment fit as a convex combination of the two predictions\n\\[\n\\mathbf{y}_1^{\\text{MA}}(\\boldsymbol{\\lambda}) = \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda},\n\\]\nwhere \\(\\boldsymbol{\\lambda} \\in \\Delta^2\\) is a 2-dimensional simplex weight vector\n\\[\n\\Delta^2 = \\left\\{ \\boldsymbol{\\lambda} \\in \\mathbb{R}_{\\geq 0}^2 : \\| \\boldsymbol{\\lambda} \\|_1 = 1 \\right\\}.\n\\]\nThe model averaged objective function minimizes\n\\[\n\\boldsymbol{\\lambda}^\\ast = \\underset{\\boldsymbol{\\lambda} \\in \\Delta^2}{\\operatorname{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda} \\right\\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThe interpretation of the convex hull remains the same as in the traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the global minimum and maximum of the two individual estimators\n\\[\n\\mathbf{y}_1^{\\text{MA}} \\in \\left[\n\\min\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right),\n\\max\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right)\n\\right]\n\\quad \\forall t \\in \\mathcal{T}.\n\\]\nOnce \\(\\boldsymbol{\\lambda}^\\ast\\) is found, the model-averaged out-of-sample predictions are estimated like\n\\[\n\\mathbf{Y}^{\\text{MA, post}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT, post}} & \\mathbf{y}_1^{\\text{AVG, post}}\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_1^{\\text{MA, post}} = \\mathbf{Y}^{\\text{MA, post}} \\boldsymbol{\\lambda}^\\ast.\n\\]\nEssentially, this is a mixture of both models."
  },
  {
    "objectID": "scmo.html#conformal-prediction-via-agnostic-means",
    "href": "scmo.html#conformal-prediction-via-agnostic-means",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Conformal Prediction via Agnostic Means",
    "text": "Conformal Prediction via Agnostic Means\nNow a final word on infernece. I use conformal prediction intervals to conduct inference here, developed in this paper. Precisely, I use the agnostic approach (yes, I know other approaches exist; users of mlsynth will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as \\(\\mathbf{u}_{\\text{pre}} = \\mathbf{y}_{1,\\text{pre}} - \\mathbf{y}^{\\text{SC}}_{1,\\text{pre}}\\), or just the pretreatment difference betwixt the observed values and its counterfactual. Furthermore, let \\(\\hat{\\sigma}^2 = \\frac{1}{T_0 - 1} \\left\\| \\mathbf{u}_{\\text{pre}} - \\bar{u} \\mathbf{1} \\right\\|^2\\) be the unbiased estimator of the residual variance, where \\(\\bar{u} = \\frac{1}{T_0} \\sum_{t=1}^{T_0} u_t\\) is the mean residual.\nWe aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period \\(\\mathbf{y}^{\\text{SC}}_{1,\\text{post}} \\in \\mathbb{R}^{T_1}\\) be the post-treatment SC predictions for some generic estimator. Assuming that the out-of-sample error is sub-Gaussian given the history \\(\\mathscr{H}\\) (in plain English, this just means that large errors are unlikely, which makes sense given that SC is less biased in a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via concentration inequalities. Specifically, we have \\(\\delta_\\alpha = \\sqrt{2 \\hat{\\sigma}^2 \\log(2 / \\alpha)}\\). The conformal prediction intervals are then defined as \\(\\mathbf{p}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} - \\delta_\\alpha \\mathbf{1}\\), \\(\\mathbf{u}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} + \\delta_\\alpha \\mathbf{1}\\). These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will likely be incorporated in the future."
  },
  {
    "objectID": "scmo.html#simulation",
    "href": "scmo.html#simulation",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Simulation",
    "text": "Simulation\nSuppose we are working at Airbnb, and we wish to see the causal effect of the introduction of Airbnb Experiences on Gross Booking Value (GBV), a metric which is defined as ‘’the total revenue generated by room or property rentals before any costs or expenses are subtracted’’. Airbnb Experiences connects users of the platform to local tour guides or other local attractions. It serves as a kind of competition to Travelocity, Viator and other booking/ travel services. In other words, this program may make makes this city an attraction, and we may see an increase in GBV as a result.\nWell, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy. The point of this simulation is to use the SCMO (synthetic control multiple outcomes) estimator to measure the causal impact.\nFor each unit, the observed outcome \\(\\mathbf{Y}_{jtk}\\) evolves according to an autoregressive process with latent structure for time, place, and seasonality\n\\[\n\\mathbf{Y}_{jtk} =\n\\rho_k \\mathbf{Y}_{jt-1k} +\n(1 - \\rho_k) \\left(\n\\alpha_{jk} + \\beta_{tk} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{tk} + \\mathbf{S}_{jt} + \\delta_k\n\\right) + \\varepsilon_{jtk}, \\quad \\text{for } t &gt; 1,\n\\]\nwith initial condition\n\\[\n\\mathbf{Y}_{j1k} =\n\\alpha_{jk} + \\beta_{1k} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{1k} + \\mathbf{S}_{j1} + \\delta_k + \\varepsilon_{j1k}.\n\\]\nHere, \\(\\alpha_{jk} \\sim \\mathcal{N}(0, 1)\\) and \\(\\beta_{tk} \\sim \\mathcal{N}(0, 1)\\) represent unit-outcome and time-outcome fixed effects, respectively. Each unit \\(j\\) possesses latent attributes \\(\\boldsymbol{\\phi}_j \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\), while each time-outcome pair \\((tk)\\) has associated latent loadings \\(\\boldsymbol{\\mu}_{tk} \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\). The seasonal component \\(\\mathbf{S}_{jt}\\) captures unit-specific periodicity and is defined as \\(\\gamma_j \\cos\\left( \\frac{4\\pi(t - \\tau_j)}{T_{\\text{season}}} \\right)\\), with \\(\\gamma_j \\sim \\text{Unif}(0, \\bar{\\gamma})\\) representing the amplitude and \\(\\tau_j \\sim \\text{Unif}\\{0, \\dots, T_{\\text{season}} - 1\\}\\) the phase shift. Each outcome \\(k\\) has a baseline shift \\(\\delta_k \\sim \\text{Unif}(200, 500)\\), an autocorrelation parameter \\(\\rho_k \\in (0, 1)\\), and an idiosyncratic noise component \\(\\varepsilon_{jtk} \\sim \\mathcal{N}(0, \\sigma^2)\\). One unit (Iquique, Chile in this draw) is designated as treated. To introduce selection bias, the unit with the second-largest realization on the first latent factor dimension is treated, meaning methods like difference-in-differences or interrupted time series methods will not perform well. The target unit’s GBV receives an additive treatment effect of \\(+5\\) during all post-treatment periods."
  },
  {
    "objectID": "scmo.html#results",
    "href": "scmo.html#results",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Results",
    "text": "Results\nWhen we run this estimator, we need to specify one of three estimators: TLP, SBMF, or both, where the abbreviations are obviouslyfor the surnames of the authors. We also need to supply a dictionary entry to mlsynth called addout. This is either a string or a list which lists the additional outcomes we care about in the dataframe. When we run the estimator, we get:\n\n# Run simulation\n\ndf = simulate(seed=10000, r=3)\n\nconfig = {\n    \"df\": df,\n    \"outcome\": 'Gross Booking Value',\n    \"treat\": 'Experiences',\n    \"unitid\": 'Market',\n    \"time\": 'Week',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\"], \"addout\": list(df.columns[4:]),\n    \"method\": \"BOTH\"\n}\n\narco = SCMO(config).fit()\n\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/mlsynth/estimators/scmo.py:644: UserWarning: An unexpected error occurred during SCMO plotting: plot_estimates() got an unexpected keyword argument 'df'\n  warnings.warn(f\"An unexpected error occurred during SCMO plotting: {e_plot_general}\", UserWarning)\n\n\nUsing the model averaging estimator, our pre-treatment Root Mean Squared Error is 0.276. The ATT is 5.046. The weights are also a sparse vector. The model averaged estimator returns Arequipa (0.237), Bogotá (0.232), San Salvador (0.218), Santiago de Chile (0.174), San Luis Potosí (0.081), Montevidio (0.032), and Manzanillo (0.026), as the contributing units or only 7 of the 98 donor units. The optimal mixing between the models is 0.538 for the intercept-shifted estimator and 0.461 for the concatenated method. For DID, the RMSE is 0.878 and the ATT is 5.2, meaning that the intercept adjusted average of all donor units is clearly a biased estimator.\nCompare to Forward DID, this is NOT true: we have an ATT of 5.063 and a pre-intervention RMSE of 0.289, selecting Antofagasta, Bocas del Toro, Punta del Este, San Pedro Sula, and Santiago as the optimal donor pool (this method uses no additional outcomes, only the GBV metric). When I compare to the clustered PCR method, the positively weighted donors are San Pedro Sula (0.296), Punta del Este (0.255), Santiago (0.199), Antofagasta (0.190), and La Plata (0.060)."
  },
  {
    "objectID": "shc.html",
    "href": "shc.html",
    "title": "The Synthetic Historical Control Method",
    "section": "",
    "text": "Oftentimes, we struggle with picking a donor pool for SCMs due to spillovers or the event being so massive that it is hard to argue for clean donors in principle. This blog post shows you one way we can get around that problem via using the synthetic historical control method. It is a flavor of synthetic controls, as the name suggests."
  },
  {
    "objectID": "shc.html#notations",
    "href": "shc.html#notations",
    "title": "The Synthetic Historical Control Method",
    "section": "Notations",
    "text": "Notations\nThe notation for this method (as it is presented in the paper) makes my head hurt, so I will simply quote directly from my paper that currently uses this method in hopes that I can spell it out clearly.\nLet \\(\\mathbb{R}\\) denote the set of real numbers, and let \\(\\|\\cdot\\|_1\\) denote the usual \\(\\ell_1\\) vector norm. Throughout, I denote sets using calligraphic letters (e.g., \\(\\mathcal{T}, \\mathcal{S}, \\mathcal{N}\\)) for clarity. Scalars are represented using plain lowercase letters (e.g., \\(h, t, n, m\\)), and matrices are denoted by bold uppercase letters (e.g., \\(\\mathbf{X}, \\mathbf{W}, \\mathbf{Y}\\)). Given a vector \\(\\mathbf{x} = (x_1, x_2, \\dots, x_T)^\\top \\in \\mathbb{R}^T\\) and an index set \\(\\mathcal{I} \\subseteq \\{1, \\dots, T\\}\\), I write \\(\\mathbf{x}_{\\mathcal{I}} \\coloneqq (x_t)_{t \\in \\mathcal{I}} \\in \\mathbb{R}^{|\\mathcal{I}|}\\) to denote the subvector of \\(\\mathbf{x}\\) corresponding to indices in \\(\\mathcal{I}\\), preserving their original order.\nLet \\(\\mathcal{T} = \\{1, 2, \\dots, T\\}\\) index time. Define the pre-treatment period as \\(\\mathcal{T}_1 \\coloneqq \\{t \\in \\mathcal{T} : t \\leq T_0\\}\\) and the post-treatment period as \\(\\mathcal{T}_2 \\coloneqq \\{t \\in \\mathcal{T} : t &gt; T_0\\}\\). The number of post-treatment periods is \\(n = T - T_0\\). Let \\(m \\in \\mathbb{N}\\) denote the evaluation window length, i.e., the number of periods used to construct the SHC match. Define the evaluation period as the final \\(m\\) months of the pre-treatment period:\n\\[\n\\mathcal{T}_{\\text{eval}} \\coloneqq \\{T_0 - m + 1, \\dots, T_0\\} \\subset \\mathcal{T}_1.\n\\]\nLet \\(\\mathbf{y} = (y_1, y_2, \\dots, y_T)^\\top \\in \\mathbb{R}^T\\) denote the observed outcome vector for the treated unit. Define the pre-treatment outcome vector \\(\\mathbf{y}_{\\text{pre}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_1} \\in \\mathbb{R}^{T_0}\\), the evaluation subvector \\(\\mathbf{y}_{\\text{eval}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_{\\text{eval}}} \\in \\mathbb{R}^m\\), and the post-treatment outcome vector \\(\\mathbf{y}_{\\text{post}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_2} \\in \\mathbb{R}^n\\). The evaluation subvector \\(\\mathbf{y}_{\\text{eval}}\\) is the target pattern to be reconstructed using convex combinations of earlier segments from the pre-treatment period.\nTo reconstruct the evaluation window, we compare it against earlier segments of the treated unit’s own pre-treatment trajectory. Define the donor matrix \\(\\widetilde{\\mathbf{Y}}_{\\text{pre}} \\in \\mathbb{R}^{m \\times N}\\) as a collection of \\(N\\) overlapping length-\\(m\\) subvectors extracted from the smoothed pre-treatment series. Each column is a contiguous subvector of the form \\(\\widetilde{\\mathbf{y}}_{[i, i+m-1]}\\), with \\(i\\) ranging over eligible start points in \\(\\mathcal{T}_1\\) that precede the evaluation window."
  },
  {
    "objectID": "shc.html#choosing-our-historical-segments",
    "href": "shc.html#choosing-our-historical-segments",
    "title": "The Synthetic Historical Control Method",
    "section": "Choosing Our Historical Segments",
    "text": "Choosing Our Historical Segments\nTo avoid using too many donor segments and overfit, SHC implements a forward selection algorithm. The authors in the paper sort of choose an arbitrary number of donors to use. They find that 27 donor segments about does the job, in practice. However, I did not like that they wanted to use a stepwise method with no stopping rule, since this just means that the size of the donor pool we use is kind of arbitrary. So, I write my own forward selection method. Starting with an empty set, it adds one donor at a time, each time choosing the segment that most reduces the in-sample MSE.\n\\[\n\\mathcal{S}_j = \\mathcal{S}_{j-1} \\cup \\left\\{ \\underset{i \\in \\mathcal{N} \\setminus \\mathcal{S}_{j-1}}{\\operatorname*{argmin}} \\left\\| \\widetilde{\\mathbf{y}}_{\\text{eval}} - \\widetilde{\\mathbf{Y}}_{\\text{pre}}^{(\\mathcal{S}_{j-1} \\cup \\{i\\})} \\mathbf{w}^{(j)} \\right\\|_2^2  + \\varsigma \\left\\| \\mathbf{C}_0^\\top \\mathbf{w} \\right\\|_2^2 \\right\\}, \\quad \\mathcal{S}_0 = \\emptyset,\n\\]\nTo choose when to stop adding donors, we compute a modified BIC at each step:\n\\[\n\\text{BIC}(j) = m \\cdot \\log\\left( \\text{MSE}_j \\right) + \\lambda j,\n\\]\nwhere \\(\\text{MSE}_j\\) is the in-sample mean squared error using \\(j\\) donors, and \\(\\lambda = \\log(m)\\). The algorithm stops when BIC increases for two steps in a row. Presumably, we could use a better one, so if you have suggestions let me know. I borrowed this idea from the forward selected PDA approach."
  },
  {
    "objectID": "sparsdens.html",
    "href": "sparsdens.html",
    "title": "What is a Synthetic Control?",
    "section": "",
    "text": "Introduction\nMarketers and econometricians stress the importance of location selection for geo-testing/program evaluation for the effect of advertising interventions. Inherently speaking, the control locations we use directly impacts the quality of our counterfactual predictions, and therefore the ATT, iROAS, or whatever metric we claim to care about. The reality is, you can’t simply take an average of randomly selected control regions and compare them to the places you’re increasing ad runs in. Therefore, researchers require systematic control group selection methods for the panel data setting where the randomzied controlled trial is not possible. Of course, one of these methods that’s popular is the synthetic control method. But if you’re reading this, you likely already know this. However, we need to take a step back. We need to ask ourselves more fundamentally what we think a synthetic control is as a concept. This happens in academia or industry all the time. My coworkers will ask me questions like, “Hey Jared, how do you view the whole constraints upon the weights for synthetic control methods? Why do we care, if at all, that the weights should be non-negative or add up to anything, and are there any rules regarding these ideas?” Usually people want to make some custom extension to the baseline algorithm, and want to know if they’re breaking some seemingly sacrosanct rules. My answer is usually some variant of: “Well, it depends on what you think a synthetic control is.” As it turns out, this is actually a non-trivial philosophical question that has no true answer.\nClassically, synthetic control weights are viewed as a sparse vector, and plenty of academic work has developed synthetic control methodologies from this perspective. The classic setting focuses on sparsity for the unit weights. The point of sparsity, as others (including me) argue is for the resulting positive donors to be interpretable and economically similar to the units of interest to the treated unit. More precisely, the goal is for similarity on latent factors that we cannot observe. Much work is dedicated to producing such descriptions of the SCM. Others argue however for a completely different set of standards. The main contention seems to be that while sparsity offers interpretability and simplicity, it may not always be practical for capturing the complex relationships in real economic or business phenomena. Instead, they advocate for dense coefficient vectors that distribute weights more diffusely across donors, potentially improving predictive performance.\nIn my opinion, the general form of a synthetic control problem is some objective function where we weight donor units to best approximate the treated (set of) unit(s), with the choice of constraints reflecting both the econometric goals and domain-specific beliefs. The most general expression for this is\n\\[\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\mathcal{L}\\big( \\mathbf{y}_1, \\mathbf{Y}_0 \\mathbf{w} \\big) + \\lambda \\cdot \\mathcal{R}(\\mathbf{w}) \\quad \\text{subject to} \\quad & \\mathbf{w} \\in \\mathcal{W}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{y}_1\\) is the vector of observed outcomes for the treated unit(s) during the pre-treatment period, \\(\\mathbf{Y}_0\\) is the matrix of outcomes for the donor pool units over the same period, and \\(\\mathbf{w}\\) is the vector of weights assigned to those donor units. The function \\(\\mathcal{L}(\\cdot, \\cdot)\\) represents the loss function measuring the discrepancy between the treated outcomes and the weighted combination of donors, commonly the squared error loss. The term \\(\\mathcal{R}(\\mathbf{w})\\) is a regularization function that imposes additional structure or penalties on the weights to reflect beliefs or promote certain properties like sparsity or smoothness. The scalar \\(\\lambda \\geq 0\\) controls the trade-off between fitting the data closely and respecting the regularization. Finally, the set \\(\\mathcal{W}\\) defines the feasible set of weights, encoding constraints such as non-negativity, sum-to-one, or other domain-specific restrictions. Notice that this setup is intended to be very very general. I have not yet taken a position on the nature of the weights or the specifics of the optimization problem.\nThis post shows that there is (often) no one right way to play. Many estimators may be used, and the key thing is the underlying econometric assumptions one needs to understand and make to use them effectively (sparsity vs density of course is not the only assumption of interest).\n\n\nApplication\nLet’s use an example. Consider this plot of a growth rate variable, where the outcome is the number of products purchased when some advertising campaigns went into effect. The goal of an incrementality study is to estimate how the growth of sales would have evolved absent the ads. After all, this is how we determine if the ad spend was effective or if it was wasted—by estimating how much revenue we would have generated without any advertising. In this dataset, we observe 276 control units across 176 pre-treatment time periods. The key problem of interest is that there are very many control units to choose from. We cannot simply use the average of controls as a counterfactual estimate, as this presumes that the mean of this entire control group is similar enough (in both level and trend) to the treated unit of interest. As the figure suggests though, this is likely not true. While the growth rate does indeed remove seasonality elements, there are still trend, periodic, and perhaps cyclical differences to account for as well. Given this, it’s likely the case that some reweighted average of these controls will mimic the treated unit much better than the pure average of the control units.\n\n\n\n\n\n\n\n\n\nTo this end, I fit a festival of models, some sparse, some sense. I fit the forward DID model (sparse), Forward SCM model (also sparse) and the robust PCA SCM model (sparse). For the dense models I estimate the \\(\\ell_2\\) panel data approach and the robust synthetic control (all these are documented here). I construct an ensemble of artificial counterfactual estimators by convexly combining the predictions of the base models. The goal is to produce a flexible estimator that balances the predictive accuracy of dense estimators with the interpretability and sparsity of sparse estimators. Let \\(M\\) denote the number of candidate models in the ensemble. For each model \\(m = 1, \\ldots, M\\), we obtain a predicted counterfactual series \\(\\widehat{Y}^{(m)}_{1,t}\\) for the treated unit in the pre-treatment period \\(t \\in \\mathcal{T}_0\\). We organize these into a matrix \\(\\widehat{\\mathbf{Y}}_0 \\in \\mathbb{R}^{T_0 \\times M}\\), where each column corresponds to one model’s predicted values over the pre-treatment period. Let \\(\\mathbf{Y}_{1,\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the observed outcomes for the treated unit during the same period. We solve the following convex optimization problem to learn a set of model weights \\(\\mathbf{w} \\in \\mathbb{R}^M\\):\n\\[\n\\min_{\\mathbf{w}} \\left\\| \\mathbf{Y}_{1,\\text{pre}} - \\widehat{\\mathbf{Y}}_0 \\mathbf{w} \\right\\|_2^2 + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\n\\quad \\text{subject to} \\quad \\mathbf{w} \\geq 0,\\quad \\sum_{m=1}^M w_m = 1.\n\\]\nThis is a ridge-penalized model averaging objective, constrained so that the weights are non-negative and sum to one. The penalty term \\(\\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\\) discourages over-reliance on any single model and promotes stability in the ensemble. We select the regularization parameter \\(\\lambda\\) via \\(K\\)-fold cross-validation on the pre-treatment period, minimizing the out-of-sample prediction error. The resulting weights \\(\\widehat{\\mathbf{w}}\\) define the ensemble counterfactual:\n\\[\n\\widehat{Y}^{\\text{ens}}_{1,t} = \\sum_{m=1}^M \\widehat{w}_m \\widehat{Y}^{(m)}_{1,t}, \\quad \\text{for all } t.\n\\]\nThis procedure allows flexible borrowing of information across model classes, combining the sparse structure of subset-selected synthetic controls with the improved fit of regularized or dense variants, depending on which performs better in-sample. The ensemble is constrained to lie in the convex hull of the candidate model predictions.\n\n\n/opt/hostedtoolcache/Python/3.13.6/x64/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThe results from the model averaging procedure reveal clear trade-offs between sparsity and predictive accuracy. In the sparse model average, the optimal penalty parameter is \\(\\lambda = 1.7074\\), leading to a pre-treatment RMSE of \\(0.5028\\). Within this specification, the FSCM model receives the most weight (\\(0.2085\\)), followed by FDID (\\(0.7915\\)), while RPCA is unused. In contrast, the dense model average achieves a substantially better fit (RMSE \\(= 0.3085\\)) with a much smaller penalty (\\(\\lambda = 0.0010\\)), allocating nearly all weight to PCR (\\(0.9966\\)) and very little to PDA (\\(0.0034\\)). The full model average, which includes both sparse and dense estimators, also selects \\(\\lambda = 0.0010\\) and reaches the same RMSE of \\(0.3085\\), but nearly all weight again falls on PCR (\\(0.9927\\)), with RPCA contributing marginally (\\(0.0073\\)) and all others excluded.\nThese results underscore a key econometric tension: while dense methods often achieve superior in-sample fit, they can obscure the role of individual donors and reduce interpretability. Sparse methods like FSCM and FDID provide clearer narratives but may underperform in terms of match quality. In settings where both approaches yield similar outcomes—as they do here—the choice between them ultimately depends on the econometrician’s priorities. If transparency and donor interpretability are paramount, sparse models may be preferred. If minimizing pre-treatment error is the guiding objective, dense models may be more appropriate. In this sense, model selection becomes not just a statistical exercise, but a philosophical one as well.\n\n\nFinal Thoughts\nIn some ways, this also illustrates why I wrote mlsynth. I do not claim to have the best models or know all of the secrets to solve one’s modeling needs; rather, I program models that I think are useful, in certain cases. The point of the mlsynth library is to allow researchers to compare and contrast these different models together all in one singular grammar, without needing to be bogged down in different softwares and syntaxes. Cool stuff happens all the time with these methods, and the only way for them to be used, and used more often, is by providing researchers with a simple framework by which to generate these counterfactuals in settings they care about.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "spillsynth.html",
    "href": "spillsynth.html",
    "title": "The Iterative Synthetic Control Method",
    "section": "",
    "text": "This will be a short blog post. I’ve spent the last two months doing a little industry work in the marketing realm, and in the meantime I made some substantial changes to mlsynth. This blog post simply shows one of the new key features I have implemented.\nChances are if you’re reading this, you know what I mean by the notion of SUTVA, or the stable unit treatment value assumption. It is the idea that if we care about the causal impact of a treatment on one unit, but other units are affected by the treatment or otherwise experience a similar treatment, that this exposure confounds our treatment effect with respect to the original unit we do care about. Say we wish to study the impact of German Reunification on West Germany’s GDP. We know West Germany was exposed, but what about neighboring nations like Austria or France? What if the reunification had regional effects? Analysts therefore have a problem: Austria and France may be very similar to West Germany, and therefore informative of West Germany’s counterfactual, but we are concerned they are exposed or affected by the main treatment of interest. What do we do? Before, researchers would need to drop these units or argue for their inclusion/exclusion, despite them being treated. Now, we do not need to do that, as SCM has a few approaches that deal with spillovers (this post covers just one). The approach, called iterative synthetic controls, is deceptively simple.\nSuppose Austria and France are partly treated. Step one of iSCM is to estimate a synthetic control for Austria, including France as a donor but excluding West Germany. Which SCM flavor you ask? Any one you like! For the purposes of this post, we will be using the Robsut SCM and the Robust PCA SCM methods from mlsynth. The precise details are not really important, but you may read the docs should you like. We then take the model predictions for Austria across the full pre and post period and replace the original Austria with the synthetic control values that the model predicts for Austria.\nNext, we do France: using the cleaned up Austria as a donor, we estimate the synthetic control for France, using the now-cleaned up Austria and the remaining donor pool units. As before, we replace the values for the original France (in the original dataset) with the new synthetic France. We now have cleaned up our two donors that may be exposed to the treatment.\nNow, with these two cleaned donors, we estimate the counterfactual for West Germany, with our 14 totally unexposed donors and the two now cleaned up donors that were once partially exposed.\n\nEstimation in Python\n“But Jared!”, you will say, this seems like a lot of looping and lots of donor tracking. Well fear not, that is what mlsynth is for. In order to get these results, you need Python (3.9 or greater) and mlsynth, which you may install from the Github repo. You’ll need the most recent version.\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nFirst we estimate the orignal model. You will find our handy-dandy util function iterative_scm is now imported.\n\nimport pandas as pd\nfrom mlsynth import CLUSTERSC\nfrom mlsynth.utils.spillover import iterative_scm\n\n# Load the reunification dataset\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/main/basedata/german_reunification.csv\"\ndf = pd.read_csv(url)\n\n# Define configuration for CLUSTERSC\nconfig = {\n    \"df\": df,\n    \"outcome\": \"gdp\",          # per capita GDP\n    \"treat\": \"Reunification\",     # binary treatment indicator\n    \"unitid\": \"country\",          # country name\n    \"time\": \"year\",               # time variable\n    \"display_graphs\": True,       # display counterfactual plots\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"Frequentist\": True, \"method\": \"BOTH\"\n}\n\noriginalresult = CLUSTERSC(config).fit()\n\n\n\n\n\n\n\n\nThese are the original results., You may check them against my coworker’s dissertation if you wish. Now we see how sensitive the results are to adjusting for spillover effects.\n\nfrom IPython.display import display, Markdown\n\niSCM_result = iterative_scm(CLUSTERSC(config), spillover_unit_identifiers=[\"Austria\", \"France\"], method=\"BOTH\")\n\ndef summarize_att_rmse_markdown(iSCM_result):\n    rows = []\n    for method in ['PCR', 'RPCA']:\n        sub = iSCM_result[method].sub_method_results[method]\n        att = sub.effects.additional_effects['ATT']\n        percent_att = sub.effects.additional_effects.get('Percent ATT', None)\n        t0_rmse = sub.fit_diagnostics.additional_metrics['T0 RMSE']\n\n        rows.append({\n            \"Method\": method,\n            \"ATT\": f\"{att:,.0f}\",\n            \"Percent ATT\": f\"{percent_att:.2f}%\" if percent_att is not None else \"—\",\n            \"T0 RMSE\": f\"{t0_rmse:,.1f}\",\n        })\n\n    df = pd.DataFrame(rows)\n    md_table = df.to_markdown(index=False)\n    display(Markdown(f\"### Iterative SCM \\n\\n{md_table}\"))\n\nsummarize_att_rmse_markdown(iSCM_result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIterative SCM\n\n\n\nMethod\nATT\nPercent ATT\nT0 RMSE\n\n\n\n\nPCR\n-2,111\n-7.96%\n108.2\n\n\nRPCA\n-1,477\n-5.71%\n88.1\n\n\n\n\n\nYou pass the original config to the util function, and you pass a list of strings that denote which units we believe are partly treated. Form there, the algorithm under the hood handles the donor cleaning, and returns back the results of the final SCM run with both donors cleaned. Note that if you want both PCR and RPCA cleaning, we pass it as method=\"BOTH\" to iterative_scm. We can see that the results are very similar to the original SCM. The pre-treatment fits degrade only very slightly (even more so with the Robust PCA method, highlighting its robustness to tiny tweaks in the model). Speaking of RPCA, the new weights are ‘Belgium’: 0.271, ‘Norway’: 0.552, ‘New Zealand’: 0.34, whereas before they were ‘Austria’: 0.023, ‘France’: 0.354, ‘Norway’: 0.485, ‘New Zealand’: 0.296. Austria goes away as a weighed donor, but there’s not very much change in the pre-treatment fit or the practical conclusions we draw from the analysis. When we use only Austria as the cleaned unit, RPCA’s weights are ‘UK’: 0.237, ‘France’: 0.607, ‘Norway’: 0.191, ‘New Zealand’: 0.12 with an ATT of -1536.355. When we use only France, the ATT is -1490.636 and the weights are ‘Austria’: 0.262, ‘Denmark’: 0.004, ‘Norway’: 0.517, ‘New Zealand’: 0.378. Of course, the key aspect of this procedure is knowing which units are likely to have spillover effects\n\n\nComments\nSo, this is not the only way to do this. There are plenty of other methods that people have developed for this purpose too. I likely will not program all of thse myself into mlsynth, but others who are so inclined are welcome to assist in the effort! in the future, I’ll also allow you to switch between the options for each kind of spillover management (the inclusive method versus the iterative method, for example). But, now you know how to use this for your own work. As usual, comments or suggestions are always appreciated.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "synthinter.html",
    "href": "synthinter.html",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "",
    "text": "Most data scientists who use synthetic control methods likely aim to estimate the counterfactual outcome for a treated unit if it had not been treated at all. This framework works quite neatly in the setting with a dummy treatment status (exposed or not exposed). But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, we are typically concered with the outcome under the scenario of no treatment. A research question could be how the store with the cash backprogram would have fared had it done nothing at all. In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus.\nBut what if we wish to estimate the counterfactual for a given unit as if it did another policy from what it actually did. In other words, How would Store A have performed if it had adopted Program B instead of Program A? What if City 1 had imposed a soda tax instead of banning large sodas? How would health metrics evolved if the other policy was done instead? Plenty of academic papers have addressed this idea before, but only to assess the counterfactual scenario of no tax at all.\nThis is where the Synthetic Interventions estimator is useful. SI estimates how a treated unit (or even a never treated unit) would have performed under an intervention it did not actually receive. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI substantially expands the range of counterfactual questions that can be credibly answered.\nBefore we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like this circulate that claimed to be able to estimate things like “what would NYC’s COVID rate look like had it locked down earlier than it actually did”. I had never heard of tensors, or really even matrices at the time, so I would always ask “wow, how can we even do this?” So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others."
  },
  {
    "objectID": "synthinter.html#si-in-mlsynth",
    "href": "synthinter.html#si-in-mlsynth",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "SI in mlsynth",
    "text": "SI in mlsynth\nNow I will give an example of how to use SI for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nNext we load the Proposition 99 data, using the dataset that has all of the states in the United States.\n\nimport pandas as pd\nfrom mlsynth import SI\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/jehangiramjad/tslib/refs/heads/master/tests/testdata/prop99.csv\"\ndf = pd.read_csv(url)\nlast_column = df.columns[-1]\ndf_filtered = df[df[last_column] == 2]\n\ncolumns_to_keep = list(df.columns[0:4]) + [df.columns[7]]\ndf_filtered = df_filtered[columns_to_keep]\n\nsort_columns = [df_filtered.columns[1], df_filtered.columns[2]]\ndf_filtered = df_filtered.sort_values(by=sort_columns)\n\ndf_filtered = df_filtered[df_filtered[df_filtered.columns[2]] &lt; 2000]\n\ndf_filtered['SynthInter'] = ((df_filtered[df_filtered.columns[0]] == 'LA') &\n                         (df_filtered[df_filtered.columns[2]] &gt;= 1992)).astype(int)\n\n\ntax_states = ['MA', 'AZ', 'OR', 'FL']  # Massachusetts, Arizona, Oregon, Florida abbreviations\ndf_filtered['Taxes'] = (df_filtered[df_filtered.columns[0]].isin(tax_states)).astype(int)\n\nprogram_states = ['AK', 'HI', 'MD', 'MI', 'NJ', 'NY', 'WA', 'CA']\ndf_filtered['Program'] = (df_filtered[df_filtered.columns[0]].isin(program_states)).astype(int)\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nWe begin with simple data cleaning. All we do is keep the relevant metrics, setting a treatment variable “SynthInter” to be 1 if the year is greater than or equal to 1992 and the unit of interest is Louisiana. I choose 1992 because this is the year Massachusetts passed its anti tobacco policy. Note that no policy in fact happened in Louisiana, so we will see the effect of keeping the status quo of no tobacco control policy at all. We then define as an indicator which states did the taxes. According to Abadie’s 2010 paper those states are Massachusetts, Arizona, Oregon, and Florida. We also define an indicator for states that did an anti-tobacco statewide program. This way, we can see how per capita smoking would have evolved under either policy. Under the hood, we just loop through each of the different policies the user specifies.\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nconfig = {\n    \"df\": df_filtered,\n    \"outcome\": 'cigsale',\n    \"treat\": 'SynthInter',\n    \"unitid\": 'State',\n    \"time\": 'Year',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"inters\": [\"Taxes\", \"Program\"]\n}\n\narco = SI(config).fit()\n\n\n\n\n\n\n\n\nFor Louisiana, the ATT of keeping the status quo relative to the state-wide tobacco program maintained per capita consumption 19.134 higher than what it would have been otherwise. The ATT of not passing taxes on tobacco, compared to taxes, mainatined the same at roughly 19.77 higher than what would have been otherwise. To me, these findings are not very surprising: “Anti tobacco programs/taxes reduce slightly tobacco consumption, wow, how shocking!” But now, we can actually answer these kinds of questions using econometric methods, instead of just speculating.\nWhen we look at the dictionary that the SI class returns, we see that the policies would have been pretty effective at reducing tobacco consumption. The dictionary is policy specific, one dictionary per policy the user chooses to consider. The SI estimator also only works for 1 treated unit, however analysts can easily make (say) a list of dataframes with an indicator for each synthetic treatment and loop over them. Once they have done this, we may extract the ATT per policy, per treated unit, and compute the sample average for an event-time ATT version for a given policy across units. I do not optimize the code to do this for a few reasons: one, the more policies and treated units we have, the less efficient it becomes to estimate tractably. Furthermore, the SI estimator is about personalized causal inference. So, I will likely leave this class for users to adapt to their own purposes (with some adjustments, if demand exists for something else)."
  }
]