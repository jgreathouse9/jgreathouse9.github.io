---
title: 'Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?'
date: 2025-05-12
categories: [Econometric Theory]
---

# The Importance of Theory

Many people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant'Anna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.

However, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like ```mlsynth``` or ```geolift``` on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantly, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory that leads them to using sub-optimal estimators or not doing basic checks. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective in the very first place. This is much more crucial because if you do not know when your methods are expected to work well, you will not know if they fail or why beyond "that does not look right". In a sense, you are sort of blindly applying calculus and algebra without much thought as it how any of it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.

This is kind of a broader problem in data science in many respects. [Ostensibly,](https://www.linkedin.com/posts/venkat-raman-analytics_why-advanced-difference-in-differences-did-activity-7318168653907025922-sQp_?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2Q8asBlsCYlwKJgJ488VWbcV1CX14FdOw) there are myths about SCM. One is "the myth that SCM/[Augmented] SCM[s] don't have assumptions like parallel trends". I do not know who believes this myth, and I am inclined to think that widespread belief of this myth is itself a myth. After all, even your most applied economists you know will tell you "yeah, SCM has assumptions". They may not be able to articulate them as formally, but everybody in my experience knows SCM *has* assumptions that need to be met (whether they verify them is of course another matter entirely. If you know anybody who truly holds these beliefs, [direct them to me](https://jgreathouse9.github.io/docs/consulting.html) so I can chat with them).

Either way, even there are not many people who confidently say/think "SCM has no assumptions", a much better argument can be made that even if people do not literally think this... this is certainly the way many researchers act in practice. And that is the point of this post. My main point is that knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain.

# Basic SCM

People often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this [has been](https://arxiv.org/pdf/2211.12095) [revised](https://doi.org/10.3982/QE1596) [substantially](https://arxiv.org/pdf/2108.13935) in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or [latent variable model](https://www.jmlr.org/papers/volume19/17-777/17-777.pdf)) that often serves as a motivating data generating process. The linear factor model takes the form:

$$
Y_{it}(0) = \boldsymbol{\lambda}_t^\top \boldsymbol{\mu}_i + \varepsilon_{it}.
$$

What does this mean? It simply means that our outcome observations are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units, plus some error term. These common factors may indeed affect each unit differently (in DID, the relationship is additive, not multiplicative), but the key idea is that we can use them to our advantage. Econometricians all the time tell us that SCM [fundamentally](https://osf.io/preprints/socarxiv/fc9xt_v1) is about matching our treated unit's common factors to the common factors that we believe are embedded in the donor set (also see conditons 1-6 [here](https://arxiv.org/pdf/2211.12095). In other words, this idea is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual for one unit or group using only the control units that behave similarly to the treated unit. This idea holds **regardless** of which units those happen to be. By exploiting the correlations between the donors and treated unit, we estimate counterfactuals with this idea in mind.

A more flexible idea is [the fine-grained potential outcomes model](https://proceedings.mlr.press/v151/shi22b/shi22b.pdf). This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unitâ€™s population. The practical implication of this is that when we apply these panel data models to real life settings, we are allowed to use different donor typs on the condition that they help us learn about the trajector of the target unit in the pre-intervention period. Because SCM does not care about what kind of donors you use, so long as they are informative donors.

# Application

Unconvinced? We can replicate some results to show this. We know [the classic example of Prop 99](https://en.wikipedia.org/wiki/1988_California_Proposition_99), where California's anti-tobacco program was compared to 38 donor states to predict the counterfactual per capita cigarette consumption. But California is a [ridiculously large](https://www.gov.ca.gov/2025/04/23/california-is-now-the-4th-largest-economy-in-the-world/) economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units? It turns our that we can do just that. Here, I compare California to [the donor divisions of the United States](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf), where the outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level and California's remains as is.

```{python}
#| echo: false
#| fig-align: center
import pandas as pd
from mlsynth.mlsynth import FDID, TSSC, PDA

# URL of the .dta file
url = "http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta"

# Load the Stata file directly from the URL
df = pd.read_stata(url)

df["Proposition 99"] =  ((df["region"] == "California") & (df["year"].dt.year >= 1989)).astype(int)

# Base configuration
base_config = {
    "df": df,
    "outcome": df.columns[2],
    "treat": df.columns[-1],
    "unitid": df.columns[0],
    "time": df.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue"]
}

# TSSC model
tssc_config = base_config.copy()  # Start with base and modify if necessary
arco = TSSC(tssc_config).fit()
```

Here is TSSC estimator, where we adjust for an intercept.

```{python}
#| echo: false
#| fig-align: center

# PDA model with method 'l2'
pda_config = base_config.copy()
pda_config["method"] = "l2"
arcol2 = PDA(pda_config).fit()
```
Here is $\ell_2$-PDA estimator.

```{python}
#| echo: false
#| fig-align: center
# FDID model
fdid_config = base_config.copy()
arcofdid = FDID(fdid_config).fit()
```

Here is the predictions of the FDID estimator.

# Takeaway

The qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California's pre-treatment trends, and so long as we are doing that, we may generally use as many or as little relevant donors as we like. The issue is not that donors in general do not matter. I wrote ```mlsynth``` precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit. The key issue though is that the "right donors" do not necessarily need to be the same type of unit as the kind you are using. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.

Note that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy, or you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you too can apply the theoretical implications of econometrics to your own case.
