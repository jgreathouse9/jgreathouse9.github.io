---
title: 'What is An Average?'
date: 2026-01-04
categories: [Econometric Theory]
---


# Averaging in Statistics

The average is, for most people, an ontological concept rather than the solution to a decision problem. It is more akin, mentally, to facts like "the sun rises" or "Spain is in Europe" than something to be derived or solved for.

Averages are the most basic summary statistic we are taught, aside from (I suppose) summation, and they arise quite naturally when we try to find a single value that best represents a collection of numbers. In ye olden days, Egyptians and Babylonians used summary statistics in construction and financial matters. In modern times, we use averages to summarize things like test grades, incomes, and other variables we care about.

To formalize this, suppose we have a sequence of data points $(x_1, x_2, \ldots, x_n)$ of length $n$. We want to choose a single number, $\mu$ (m-you), that is as close as possible to all of them. Here, $\mu$ is a placeholder variable representing a potential candidate for the "best" summary value. We do not know what it is in advance, and it is something we must solve for. This becomes our very first optimization problem.

## Definitions

In doing so, we will require a few rules. In the book I have a dedicated chapter on the required mathematics, but I repeat the definitions here for convenience.


<details>
<summary><strong>(optional mathematical background)</strong></summary>

::: {#def-derivative}
## The Derivative

For a scalar function $f : \mathbb{R} \to \mathbb{R}$, the *derivative* measures the instantaneous rate of change:
$$
\frac{\mathrm{d}f(x)}{\mathrm{d}x} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}.
$$

**Intuition:** How $f(x)$ changes for an infinitesimal change in $x$.
:::

Derivatives have rules attached to them. For example, one is the power rule:

::: {#def-power-rule}
## The Power Rule

If $f(x) = x^n$ for $n \in \mathbb{R}$:
$$
\frac{\mathrm{d}f(x)}{\mathrm{d}x} = n x^{n-1}.
$$

**Worked example:** $\frac{\mathrm{d}}{\mathrm{d}x} x^3 = 3x^2$.
:::

Pretty straightforward. The derivative of $x^2$ is just $2x$. For $4q^5$ it's $20q^4$. Another important rule is the chain rule.

::: {#def-chain-rule}
## The Chain Rule

For $y = f(g(x))$:
$$
\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}f}{\mathrm{d}g} \cdot \frac{\mathrm{d}g}{\mathrm{d}x}.
$$
:::

[ … rest of your content exactly as-is … ]

::: {#def-objective-function}
## Objective Function

A scalar-valued function we aim to minimize or maximize:

$$
f : \mathbb{R}^n \to \mathbb{R},
$$

$$
\min_{\mathbf{x}} f(\mathbf{x}) \quad \text{or} \quad \max_{\mathbf{x}} f(\mathbf{x}).
$$
:::

</details>

## Deriving the Arithmetic Mean

Let our observed values be $x_i$ and the number that "is as close as possible to all the values" be the Greek letter $\mu$ (m-yoo). Because we are looking for the value that minimizes the distance, we can write $x_i - \mu$ as a shorthand for this deviation. To minimize the total deviation across all points, we sum over each element of the sequence $i$: $\sum_{i=1}^{n} (x_i - \mu)$. Finally, to penalize values of $\mu$ that are further away from the observed data, we square each term. This gives us the optimization problem:

$$
\min_{\mu} \sum_{i=1}^{n} (x_i - \mu)^2.
$$


This function is convex and differentiable. We can optimize this function by taking the derivative with respect to $\mu$. We begin by applying the chain rule term by term. For each term $(x_i - \mu)^2$, there are two parts to consider: the outer function and the inner function. The outer function is $f(u) = u^2$, where $u = x_i - \mu$. Its derivative (by the power rule) is

$$
\frac{\mathrm{d}}{\mathrm{d}u} u^2 = 2u = 2(x_i - \mu)
$$

The inner function is $u(\mu) = x_i - \mu$. Its derivative with respect to $\mu$ is

$$
\frac{\mathrm{d}}{\mathrm{d}\mu} (x_i - \mu) = -1
$$

Multiplying the outer and inner derivatives gives the derivative of a single term:
$$
\frac{\mathrm{d}}{\mathrm{d}\mu} (x_i - \mu)^2 = 2(x_i - \mu) \cdot (-1) = -2(x_i - \mu)
$$

Summing over all $i$ gives the derivative of the entire objective function:

$$
\frac{\mathrm{d}}{\mathrm{d}\mu} \sum_{i=1}^{n} (x_i - \mu)^2 = \sum_{i=1}^{n} [-2(x_i - \mu)] = -2 \sum_{i=1}^{n} (x_i - \mu)
$$

Setting this derivative equal to zero gives the first-order condition for the minimum

$$
-2 \sum_{i=1}^{n} (x_i - \mu)=0.
$$


We now solve for $\mu$. First, just divide both sides by $-2$:

$$
\sum_{i=1}^{n} (x_i - \mu) = 0.
$$

Next, split the sum into two parts:

$$
\sum_{i=1}^{n} x_i - \sum_{i=1}^{n} \mu = 0.
$$

Here $\mu$ is the same number for every term in the sum and it does not depend on $i$. Why? That is exactly what we seek: *one single value* that is as close as possible to **all** the data points at once. Because $\mu$ is constant with respect to the index $i$, the second sum is simply $\mu$ added to itself $n$ times, what we call "multiplication" in the business:

$$
\sum_{i=1}^{n} \mu = n \mu
$$

So the equation simplifies to:

$$
\sum_{i=1}^{n} x_i - n\mu = 0.
$$

Now we add $n\mu$ to both sides:

$$
\sum_{i=1}^{n} x_i = n \mu.
$$


Finally, divide both sides by $n$ to isolate $\mu$:

$$
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i.
$$


::: {#def-aravg}
## Arithmetic Average

For some $(x_1, x_2, \dots, x_n) \in \mathbb{R}^n$, the arithmetic average is: $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ where $n$ is the length of the sequence and $\frac{1}{n}$ is the weight.
:::

Thus, the value of $\mu$ that minimizes the total squared deviation is the arithmetic mean of the data. The arithmetic mean is thus not an arbitrary formula: it is the unique value that minimizes the total squared distance between itself and the data. Suppose we have the data points $x = (2, 5, 7, 10)$. The arithmetic average is

$$
\bar{x} = \frac{1}{4} (2 + 5 + 7 + 10) = \frac{24}{4} = 6.
$$

This means that $\mu = 6$ is the single value that minimizes the total squared deviation

$$
\sum_{i=1}^4 (x_i - \mu)^2 = (2-6)^2 + (5-6)^2 + (7-6)^2 + (10-6)^2 = 16 + 1 + 1 + 16 = 34.
$$

No other single number would produce a smaller total squared deviation, which illustrates why the arithmetic mean is the unique "best-fitting" constant.

### So What?

Now, you may be saying "Hey Jared, this is overkill. Why bother doing all of this for something as simple as an average?" The derivation shows us that the average is the solution to an optimization problem. That single insight connects simple statistics to the machinery of modern econometrics. Variance, standard deviation, mean squared error, regression coefficients, and the synthetic control weights are all built on the same principle: finding a number/set of numbers that minimizes some measure of error. The arithmetic mean is the simplest, most familiar instance of this principle. Every estimator in this book is an application of averaging, and this fact will become crucial in later chapters when we take more sophisticated averages, including the weighted averages that power synthetic control methods.


The reason I dedicate a chapter to the arithmetic average is simple: I want to make sure we start the book on the right footing. For some reason, synthetic control methods are often described in dramatic terms, as a Frankenstein’s monster, [a mash-up of controls](https://www.linkedin.com/posts/aryma-labs_the-ultimate-test-for-synthetic-control-method-activity-7376484972360077312-Mfxe?utm_source=share&utm_medium=member_android&rcm=ACoAAB2Q8asBlsCYlwKJgJ488VWbcV1CX14FdOw), or a "fake version" of the treated unit.

For example, [Haus](https://haus.io/blog/matched-market-tests-dont-cut-it-why-haus-uses-synthetic-control-in-incrementality-experiments) and [Meta scientists](https://medium.com/@mansisharma.8.k/when-a-b-testing-isnt-an-option-real-world-lessons-from-meta-aaf246014d05) have used these kinds of analogies to frame synthetic control methods. These metaphors, though vivid, can give the impression that something exotic or mysterious is happening than it truly is. In reality, the method is a natural extension of the concept you already know: the arithmetic mean. Consider the humble average  

$$
\bar{x} = \frac{1}{n} \sum_{i=1}^n y_i,
$$  

which is simply a weighted average with *uniform weights* $w_i = 1/n$ for all $i$. It is never described as a Frankenstein or a mash-up in the same way that SCM is. In fact, this description of SCM is even more puzzling when you notice that the arithmetic mean already satisfies the classic SCM constraints: the weights are non-negative and sum to exactly 1. The simple average, thus, is the special case of SCM where every weight is $w_i = 1/n$. So when we say "SCM builds a counterfactual as a weighted average of controls with non-negative weights that sum to 1", we're describing the same mathematical structure as computing a plain old mean. The only difference is that classic SCM does not impose the uniformity of the weights. But, for some reason, the moment we allow *non-uniform weights* $w_i$ with $\sum_i w_i = 1$, the same operation suddenly is described as sophisticated or complex. If a client asked me to compute the average of sales across markets over time and I replied, “You mean you want me to make a mashed-up Frankenstein monster of your sales data?” they would rightly think I had lost the plot. Consider the simple Difference-in-Differences estimator with block assignment, which [is just](https://arxiv.org/pdf/1610.07748) the mean of controls adjusted by the baseline difference between the target cohort and control cohorts. In a simple case with four control units, each unit gets a weight of 0.25, yet nobody accuses DID of sorcery, grafting units together, or creating a Franken-market. The weights are uniform, sure, but the mathematical structure is identical: a weighted average where weights are non-negative and sum to 1.

The point is simple: every single estimator in this book uses a weighted average of controls which is derived from an optimization problem, whether we are discussing Difference-in-Differences or SCM. The arithmetic mean is merely the simplest case, and synthetic control methods are just a careful generalization of that fact.
