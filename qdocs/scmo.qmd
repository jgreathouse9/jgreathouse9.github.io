---
title: "Synthetic Control Methods With Multiple Outcomes"
date: 2025-04-14
categories: [Causal Inference, Econometrics]
---

Sometimes, analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may predict a single target variable. There are [recent](https://doi.org/10.48550/arXiv.2311.16260) [papers](https://doi.org/10.48550/arXiv.2304.02272) that have advocated for this in applied settings.

# Notation

Let $\mathbb{R}$ denote the set of real numbers. A calligraphic letter, such as $\mathcal{S}$, represents a discrete set with cardinality $S = |\mathcal{S}|$. When multiplying a matrix $\mathbf{A} \in \mathbb{R}^{T \times N_0}$ by a vector $\mathbf{b} \in \mathbb{R}^{N_0}$, the result is a new vector in $\mathbb{R}^T$, formed as a linear combination of the columns of $\mathbf{A}$:

$$
\mathbf{A} \mathbf{b} = \sum_{j \in \mathcal{N}_0} b_j \mathbf{A}_{:,j} \in \mathbb{R}^T
$$

Here, each entry in the resulting vector is a dot product between a row of $\mathbf{A}$ and the vector $\mathbf{b}$.

Let $j \in \mathbb{N}$ represent indices for a total of $N$ units and $t \in \mathbb{N}$ index time. Let $j = 1$ denote the treated unit, with the set of controls being $\mathcal{N}_0 = \mathcal{N} \setminus \{1\}$, with cardinality $N_0$. The pre-treatment period consists of the set $\mathcal{T}_1 = \{ t \in \mathbb{N} : t \leq T_0 \}$, where $T_0$ is the final period before treatment. Similarly, the post-treatment period is given by $\mathcal{T}_2 = \{ t \in \mathbb{N} : t > T_0 \}$.

The observed outcome for unit $j$ at time $t$ is $y_{jt}$, where a generic outcome vector for a given unit in the dataset is:

$$
\mathbf{y}_j \in \mathbb{R}^T, \quad \mathbf{y}_j = (y_{j1}, y_{j2}, \dots, y_{jT})^\top \in \mathbb{R}^T
$$

The outcome vector for the treated unit specifically is $\mathbf{y}_1$. The donor matrix is defined as:

$$
\mathbf{Y}_0 \coloneqq \begin{bmatrix} \mathbf{y}_j \end{bmatrix}_{j \in \mathcal{N}_0} \in \mathbb{R}^{T \times N_0}
$$

where each column indexes a donor unit and each row corresponds to a time period. We denote by $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{T_0}$ the subvector of outcomes for unit $j$ in the pre-treatment period, and by $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{T_1}$ the corresponding post-treatment vector, where $T_1 = T - T_0$. Then, we define our pre- and post-intervention analogs for the data:

$$
\mathbf{y}_1^{\text{pre}} = (y_{1t})_{t \in \mathcal{T}_1} \in \mathbb{R}^{T_0}, \quad \mathbf{y}_1^{\text{post}} = (y_{1t})_{t \in \mathcal{T}_2} \in \mathbb{R}^{T_1}
$$

$$
\mathbf{Y}_0^{\text{pre}} = \left[ \mathbf{y}_j^{\text{pre}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_0 \times N_0}, \quad \mathbf{Y}_0^{\text{post}} = \left[ \mathbf{y}_j^{\text{post}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_1 \times N_0}
$$

We aim to find weights $\mathbf{w} \in \mathbb{R}^{N_0}$ subject to

$$
\mathbf{w} \in \mathcal{W} = \left\{ \mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0} : \|\mathbf{w}\|_1 = 1 \right\}
$$

which minimize the mean squared error between the target vector and pre-intervention fit.

The exact same notations apply for the case of multiple outcomes. Let $k$ denote the number of outcomes aside from the main one being used. For each outcome $m \in \{1, 2, \dots, k\}$, we have:

$$
\mathbf{Y}_0^{(m)} \in \mathbb{R}^{T \times N_0} \quad \text{and} \quad \mathbf{y}_1^{(m)} \in \mathbb{R}^{T}
$$

where $\mathbf{Y}_0^{(m)}$ is the donor matrix for outcome $m$, and $\mathbf{y}_1^{(m)}$ is the outcome vector for the treated unit for the $m$-th outcome. Similarly, for the pre-treatment and post-treatment periods, we define:

$$
\mathbf{Y}_0^{(m), \text{pre}} \in \mathbb{R}^{T_0 \times N_0}, \quad \mathbf{Y}_0^{(m), \text{post}} \in \mathbb{R}^{T_1 \times N_0}
$$

and

$$
\mathbf{y}_1^{(m), \text{pre}} \in \mathbb{R}^{T_0}, \quad \mathbf{y}_1^{(m), \text{post}} \in \mathbb{R}^{T_1}
$$

At first, the multiple outcomes setup may seem intimidating. In every application of synthetic control I have worked with, along with most analysts I imagine, we have worked with a single outcome at a time. However, best way to think about it is that we are simply horizontally stacking matrices and vectors atop one another and using the latent factors embedded within each of those outcomes to predict the pre-intervention time series for the main metric we do care about. I define the "stacking" operation, denoted by the prime notation, as the concatenation of matrices or vectors along their rows. Specifically, the stacked version of a matrix is given by:

$$
\mathbf{A}' = \begin{bmatrix} \mathbf{A}_1 \\ \mathbf{A}_2 \\ \vdots \\ \mathbf{A}_k \end{bmatrix}
$$
where $\mathbf{A}_i$ are matrices or vectors to be stacked, and the resulting matrix $\mathbf{A}'$ has rows corresponding to each of the matrices being stacked. Similarly, for vectors we have

$$
\mathbf{v}' = \begin{bmatrix} \mathbf{v}_1 \\ \mathbf{v}_2 \\ \vdots \\ \mathbf{v}_k \end{bmatrix}
$$
where each $\mathbf{v}_i$ is a vector to be stacked, and the resulting vector $\mathbf{v}'$ is formed by stacking these vectors.

We solve the following objective function for the stacked synthetic control:

$$
\underset{\mathbf{w} \in \mathcal{W}}{\operatorname*{argmin}} \left\| \mathbf{Y}_0' \mathbf{w} - \mathbf{y}_1' \right\|^2
$$

where $\mathbf{Y}_0'$ is the stacked matrix of donor unit outcomes across all outcomes, and $\mathbf{y}_1'$ is the stacked vector of treated unit outcomes across all outcomes. More specifically:

$$
\mathbf{Y}_0' = \begin{bmatrix} \mathbf{Y}_0^{(1)} \\ \mathbf{Y}_0^{(2)} \\ \vdots \\ \mathbf{Y}_0^{(k)} \end{bmatrix}
$$

$$
\mathbf{y}_1' = \begin{bmatrix} \mathbf{y}_1^{(1)} \\ \mathbf{y}_1^{(2)} \\ \vdots \\ \mathbf{y}_1^{(k)} \end{bmatrix}
$$

To account for intercept shifts, we similarly minimize the following:

$$
\underset{\mathbf{w} \in \mathcal{W}}{\operatorname*{argmin}} \left\| \mathbf{Y}_0' \mathbf{w} - \boldsymbol{\beta} \mathbf{1}_T - \mathbf{y}_1' \right\|^2
$$

In this case, $\boldsymbol{\beta}$ represents a vector of intercepts, and $\mathbf{1}_T$ is a vector of ones with length $T$. The term $\mathbf{Y}_0'$ is the stacked matrix of donor outcomes, and $\mathbf{y}_1'$ is the stacked vector of treated unit outcomes, as defined earlier.

# Estimation in Python

```{python}

#| fig-align: center
#| echo: false

from mlsynth.mlsynth import SCMO, FSCM, FDID
import numpy as np
import pandas as pd

def simulate(
    seed=1677,
    n_units=100,
    n_periods=156,
    n_factors=15,
    rho=0.85,
    sigma=.2,
    treated_unit=-19,
    treatment_period=104,
    treatment_effect=15,
    seasonal_strength=0.6
):
    np.random.seed(seed)

    # 1. Setup

    cities = [
        "São Paulo", "Mexico City", "San Carlos de Bariloche", "Rio de Janeiro", "Ushuaia",
        "Bogotá", "Santiago", "Caracas", "Guayaquil", "Quito",
        "Brasília", "Bocas del Toro", "Asunción", "Cabo San Lucas", "Playa del Carmen",
        "Medellín", "Porto Alegre", "Placencia", "Recife", "Salvador",
        "Zihuatanejo", "San José", "Panama City", "Montevidio", "Tegucigalpa",
        "Foz do Iguaçu", "Maracaibo", "Rosario", "Maracay", "Antofagasta",
        "San Pedro Sula", "San Juan", "Chihuahua", "Cayo District", "Maturín",
        "Buzios", "Puebla", "Mar del Plata", "Arequipa", "Fernando de Noronha", "Guatemala City",
        "Zacatecas", "Mérida", "Córdoba", "Cozumel", "Trujillo",
        "Corozal Town", "Santa Cruz de la Sierra", "San Luis Potosí", "Jalapão", "Potosí",
        "Tucumán", "Neuquén", "La Plata", "Viña del Mar", "Florianópolis", "Lagos de Moreno",
        "La Paz", "Belém", "Venezuela", "Ribeirão Preto", "Valparaíso",
        "Marília", "Campinas", "Vitoria", "Sorocaba", "Santa Fe",
        "San Salvador", "Lima", "Buenos Aires", "Curitiba", "Maceió",
        "Iquique", "La Ceiba", "Puerto La Cruz", "Olinda", "Monterrey",
        "Ibagué", "Cúcuta", " Paraty", "Cancún", "Puerto Vallarta", "Chiclayo", "Ambato",
        "Pucallpa", "Santa Marta", "Villavicencio", "Paraná", "Cauca", "San Vicente",
        "Cali", "Tarija", "Manzanillo", "El Alto", "Santiago de Chile", "Cochabamba",
        "Cartagena", "Santo Domingo", "Durango", "Puerto Viejo de Talamanca"
    ][:n_units]

    # 2. Generate latent factors (AR(1) + seasonality)
    common_factors = np.zeros((n_periods, n_factors))
    common_factors[0] = np.random.normal(0, 1, n_factors)
    for t in range(1, n_periods):
        common_factors[t] = rho * common_factors[t - 1] + np.random.normal(0, 1, n_factors)

    weeks = np.arange(n_periods)
    seasonal = seasonal_strength * np.sin(4 * np.pi * weeks / 52)
    common_factors += seasonal[:, None]

    factor_loadings = np.random.normal(0, 1, (n_units, n_factors))

    # 3. Generate covariates first
    temperature = seasonal[:, None] + np.random.normal(40, 1.5, (n_periods, n_units))
    temperature = np.clip(temperature, 10, 45)  # realistic bounds

    search_volume = .75 * (temperature - 10) + np.random.normal(0, 1.5, (n_periods, n_units))
    search_volume = np.clip(search_volume, 0, None)

    arrivals = 5 + .6 * (search_volume) + np.random.normal(0, 2, (n_periods, n_units))

    exchange = np.zeros((n_periods, n_units))
    initial_rates = np.random.uniform(2, 6, size=n_units)
    for i in range(n_units):
        exchange[:, i] = initial_rates[i] + np.cumsum(np.random.normal(0, 0.02, n_periods))

    raw_employment = 10 + 0.1 * (temperature - 20) + np.random.normal(0, 0.8, (n_periods, n_units))
    employment = np.clip(raw_employment, 3, 15)

    # 4. Coefficients for covariates (unit-specific, with realistic signs)
    beta_temp = np.random.normal(0.8, 0.2, n_units)
    beta_arrivals = np.random.normal(0.4, 0.1, n_units)
    beta_search = np.random.normal(0.3, 0.1, n_units)
    beta_exchange = np.random.normal(-0.2, 0.05, n_units)  # weaker currency = more bookings
    beta_employment = np.random.normal(0.1, 0.05, n_units)

    # 5. Generate booking value using latent + observed + random offset + noise
    bookingvalue = np.zeros((n_periods, n_units))
    for i in range(n_units):
        latent_component = common_factors @ factor_loadings[i]
        covariate_component = (
            beta_temp[i] * temperature[:, i] +
            beta_arrivals[i] * arrivals[:, i] +
            beta_search[i] * search_volume[:, i] +
            beta_exchange[i] * exchange[:, i] +
            beta_employment[i] * employment[:, i]
        )
        offset = np.random.uniform(100, 250)
        noise = np.random.normal(0, sigma, n_periods)
        bookingvalue[:, i] = latent_component + covariate_component + offset + noise

    # 6. Treatment effect
    bookingvalue[treatment_period:, treated_unit] += treatment_effect

    # 7. Pack into DataFrame
    time = np.repeat(np.arange(1, n_periods + 1), n_units)
    markets = np.tile(cities, n_periods)

    df = pd.DataFrame({
        "Market": markets,
        "Time": time,
        "Gross Booking Value": bookingvalue.flatten(),
        "Experiences": np.where((markets == cities[treated_unit]) & (time >= treatment_period), 1, 0),
        "Temperature": temperature.flatten(),
        "Airport Arrivals (000s)": arrivals.flatten(),
        "Search Volume": search_volume.flatten(),
        "Exchange Rate (USD)": exchange.flatten(),
        "Hospitality Employment (%)": employment.flatten()
    }).sort_values(["Market", "Time"]).reset_index(drop=True)

    return df

# Simulate with weak seasonal factor
df = simulate(treated_unit=33, treatment_effect=20, n_factors=6)


config = {
    "df": df,
    "outcome": df.columns[2],
    "treat": df.columns[3],
    "unitid": df.columns[0],
    "time": df.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["red", "blue"], "addout": list(df.columns[4:]),
"method": "tlp"}

arco = FSCM(config).fit()


```
