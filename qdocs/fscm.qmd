---
title: 'Forward Synthetic Control Estimation'
date: 2025-03-31
categories: [Econometrics, Causal Inference, Data Science]
---
# Intro
Interpolation bias is known to be a problem with synthetic control methods. The donors we choose should be as similar to the target unit as possible in the pre-treatment period for valid counterfactual prediction. This means that selecting a donor pool is critical, but this isn't always easy in settings where many controls exist. This post introduces the [Forward Selected Synthetic Control Method](https://doi.org/10.1016/j.econlet.2024.111976). This method uses the Forward Selection method to choose the donor pool for a synthetic control model. Then, it uses that selected donor pool to estimate the out of sample predictions. 

Let $\mathbb{R}$ denote the set of real numbers. Let a caligraphic letter, say $\mathcal S$, denote a descrete set whose cardinality is $S=|\mathcal{S}|$. Let $t \in \mathbb{N}$ and $i \in \mathbb{N}$, represent the indices for $T$ time periods and  $N$ units. The pre-treatment period consists of consecutive time periods $\mathcal{T}_1 = \{1, 2, \ldots, T_0\}$ (cardinality $T_1$), while the post-treatment period is given by $\mathcal{T}_2 = \{T_0 + 1, \ldots, T\}$ (cardinality $T_2$). The treated unit is indexed by $i = 1$, while the remaining set of units, $\mathcal{N}_0 \coloneqq \{2, \ldots, N_0 + 1\}$ (cardinality $N_0$), forms the control group. Each outcome for all units is denoted by $y_{it}$. Denote the outcome vector for the treated unit as $\mathbf{y}_1 \coloneqq \begin{bmatrix} y_{11} & y_{12} & \cdots & y_{1T} \end{bmatrix}^\top \in \mathbb{R}^T$, where each entry corresponds to the outcome of the treated unit at time $t$. The donor pool matrix, similarly, is defined as $\mathbf{Y}_0$, or the concatenation of the control unit vectors, with $N_0$ columns. The corresponding space of synthetic control weights is the $(N_0)$-dimensional probability simplex:

$$
\Delta^{N_0} \coloneqq \left\{ \mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0} : \|\mathbf{w}\|_1 = 1 \right\}.
$$

Now, consider a restricted donor pool chosen by forward selection. This consists of a strict subset of control units, $\mathcal{S} \subseteq \mathcal{N}_0$, with cardinality $|\mathcal{S}| = k$, where $k < N_0$. This subset induces a subsimplex of the original probability simplex, defined as:  

$$
\Delta^{k - 1}(\mathcal{S}) \coloneqq \left\{ \mathbf{w}' \in \mathbb{R}_{\geq 0}^{k} : \|\mathbf{w}'\|_1 = 1 \right\}.
$$

Since any feasible weight vector $\mathbf{w}' \in \Delta^{k-1}(\mathcal{S})$ can be embedded in $\Delta^{N_0-1}$ by setting the weights of all units outside $\mathcal{S}$ to zero, we have the natural inclusion:

$$
\Delta^{k - 1}(\mathcal{S}) \subseteq \Delta^{N_0 - 1}.
$$

Now, we describe the iterative donor selection process. At each iteration, we identify the optimal subset of control units that minimizes the mean squared error (MSE) in the pre-treatment period. First, each donor $j \in \mathcal{N}_0$ is evaluated independently by solving:

$$
\underset{\mathbf{w} \in \Delta^0(\{j\})}{\operatorname*{argmin}} \, \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w} \|_2^2.
$$

This computes one MSE per donor unit (essentially identifying the nearest neighbor in terms of pre-treatment fit). The donor with the lowest MSE, denoted as $j^\ast$, is selected as the initial control unit:

$$
 j^\ast = \underset{j \in \mathcal{N}_0}{\operatorname*{argmin}} \, \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w}^{(j)} \|_2^2.
$$

Next, we iteratively expand the donor pool by adding an additional control unit at each step. Specifically, for each $j \in \mathcal{N}_0 \setminus \mathcal{S}$, we update the synthetic control model and select the unit that minimizes the MSE over the expanded subsimplex:

$$
 j^\ast = \underset{j \in \mathcal{N}_0 \setminus \mathcal{S}}{\operatorname*{argmin}} \, \underset{\mathbf{w} \in \Delta^{k}(\mathcal{S} \cup \{j\})}{\operatorname*{min}} \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w} \|_2^2.
$$

This represents the greedy step where the next donor $j^\ast$ is chosen based on the best pre-treatment fit within the expanded donor pool. 

To generalize, let $\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_K$ denote the donor sets selected at each iteration, with corresponding MSE values $MSE(\mathcal{S}_1), MSE(\mathcal{S}_2), \dots, MSE(\mathcal{S}_K)$. The final optimal donor set $\mathcal{S}^\ast$ is then:

$$
\mathcal{S}^\ast = \underset{\mathcal{S}_i \in \{\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_K\}}{\operatorname*{argmin}} \, MSE(\mathcal{S}_i).
$$

## Estimation in ```mlsynth```

As ususal, we begin by installing ```mlsynth```.

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

And then we load the Prop 99 data and fit the model in the ususal ```mlsynth``` fashion.

```{python}
#| fig-align: center

import pandas as pd
from IPython.display import display, Markdown
from mlsynth.mlsynth import FSCM
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"
data = pd.read_csv(url)
config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": "red"}

arco = FSCM(config).fit()
```

After estimating, we can get the weights like

```{python}
weights_dict = arco['Weights'][0]
df = pd.DataFrame(list(weights_dict.items()), columns=['State', 'Weight'])
display(Markdown(df.to_markdown(index=False)))
```
These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the synthetic control, with only 6 being assigned positive weight.
