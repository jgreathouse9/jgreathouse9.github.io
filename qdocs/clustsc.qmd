---
title: 'On the Use of Clustering for Synthetic Controls Methods, using mlsynth'
date: 2025-02-03
categories: [Causal Inference, Machine Learning]
---

Picking a donor pool is very important with the synthetic control method. The vanilla method suffers from interpolation biases in the case when the donor pool is not suitable for the target unit. This is farily common with applied SCM in real life, as oftentimes we may have a high-dimensional donor pool and not know what the right control units would be _a priori_. The standard advice given in this instance is to limit our control group to units that are already similar to the target unit. But how do we do this? One way is to use clustering methods to select the control group. Consider [the classic Basque Country example](https://economics.mit.edu/sites/default/files/publications/The%20Economic%20Costs%20of%20Conflict.pdf) below, where the Basque Country undergoes a wave of terrorism in the mid-1970s which is thought to impact their GDP per Capita relative to other areas of Spain.

```{python}
#| fig-align: center
#| echo: false

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

def plot_donors_and_treated(donor_matrix, treated_vector, pre_periods, title):
    plt.figure(figsize=(10, 6))

    # Plot the donor units
    for i in range(donor_matrix.shape[1]):
        plt.plot(donor_matrix[:, i], color='gray', linewidth=0.5, alpha=0.8, label='_nolegend_')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Treated Unit')

    # Plot the normalized average of controls
    plt.plot(donor_matrix.mean(axis=1), color='blue', linewidth=2, label='Average of Controls')

    # Add vertical line for the pre-periods (waterline)
    plt.axvline(x=pre_periods, color='red', linestyle='--', linewidth=1.5, label='Terrorism')

    # Add labels and title
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.legend()

    plt.show()

# Set up theme for Matplotlib
def set_theme():
    theme = {
        "axes.grid": True,
        "grid.linestyle": "-",
        "grid.color": "black",
        "legend.framealpha": 1,
        "legend.facecolor": "white",
        "legend.shadow": True,
        "legend.fontsize": 12,
        "legend.title_fontsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        "axes.labelsize": 10,
        "axes.titlesize": 12,
        "figure.dpi": 100,
        "axes.facecolor": "white",
        "figure.figsize": (10, 6),
    }
    matplotlib.rcParams.update(theme)

set_theme()


file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)
treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

basque_prepped = dataprep(df, unitid, time, outcome, treat)

plot_donors_and_treated(basque_prepped['donor_matrix'], basque_prepped['y'], basque_prepped['pre_periods'], 'Basque vs. Controls')

```

Here, we plot the Basque versus the average of its controls as well as the individual donor outcome vectors themselves. We can see that the Basque Country is one of the wealthiest areas of Spain, up there with Madrid, Cataluna, and the Balearic Islands. We have other donors too, which by comparison are less wealthy. The key, then, is to ask which donors we should select for the synthetic control algorithm to consider in the first place, by exploiting pre-intervention similarities between the treated unit and control group. With "better" pre-policy donors, there is a higher chance that our out-of-sample predictions would be closer to the actual counterfactual.

# Time-Series Clustering for Synthetic Controls

To begin, we use SVD over the outcomes matrix $\mathbf{Y}_0 \in \mathbb{R}^{N_0 \times T}$. SVD decomposes it into $\mathbf{Y}_0 = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$, where $\mathbf{U} \in \mathbb{R}^{N_0 \times T}$ contains the left singular vectors, $\mathbf{\Sigma} \in \mathbb{R}^{T \times T}$ is a diagonal matrix of singular values, and $\mathbf{V} \in \mathbb{R}^{T \times T}$ contains the right singular vectors. Using singular value thresholding or taking the spectral rank, keeping only the top $r$ singular values, we have a low-dimensional representation of the outcomes matrix, $\mathbf{M} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$. The transformed representation is then defined as $\tilde{\mathbf{U}} = \mathbf{U} \mathbf{\Sigma}_r$, where $\mathbf{\Sigma}_r$ is the truncated diagonal matrix containing only the retained singular values.

We then perform k-means clustering over the the rows of $\tilde{\mathbf{U}}$, or the singular vectors. We seek to partition the units into $k$ clusters by minimizing the within-cluster variance, solving $\underset{P}{\operatorname*{argmin}} \sum_{i=1}^{N_0} \min_{j \in [k]} \|\tilde{\mathbf{U}}_i - P_j\|^2$, where $P_j$ represents the cluster centers. ``mlsynth`` uses the silhouette method to choose the number of clusters. Each unit is assigned a cluster, and we use the cluster that contains the treated unit as the donor pool.

We now may apply the SCM estimator. The weight vector $\mathbf{w}$ is obtained by solving $\underset{\mathbf{w} \in \mathbb{R}^{n_A}}{\operatorname*{argmin}} \lVert \mathbf{y}_1^- - \mathbf{w}^\top \mathbf{A}^- \rVert_2^2$, where $n_A$ is the cardinality of the selected subset of controls. For the FPCA method, see [the documentation](https://mlsynth.readthedocs.io/en/latest/clustersc.html) for ``mlsynth``, which essentially does a functional data version of this exact method.

# Estimating the Causal Impact

We may now estimate the impact with either PCR or another robust matrix factorization method similar to how Mani did in [his dissertation](https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=6069&context=gc_etds). We use an iterative algorithm to extract the low-rank structure of our new donor pool. Mani uses what we would call Principal Componenet Pursuit, but another such method is non-convex half-quadratic regularization (HQF), which I implemented for Python from [the original MATLAB code](https://github.com/bestzywang/Robust-PCA-via-Non-convex-Half-quadratic-Regularization/blob/main/RPCA_HQF.m). HQF is a form of Robust PCA, a general class of methods which aims to decompose the observed noisy matrix $\mathbf{Y}_0$ into a low-rank matrix $\mathbf{L}$ and a sparse matrix $\mathbf{S}$, such that $\mathbf{Y}_0 = \mathbf{L} + \mathbf{S}$, where $\mathbf{L} = \mathbf{U} \mathbf{V}^{top}$ represents the low-rank approximation, and $\mathbf{S}$ models the sparse noise. The optimization proceeds iteratively by updating $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{S}$. The matrices $\mathbf{U}$ and $\mathbf{V}$ are initialized randomly with dimensions $m \times r$ and $n \times r$, respectively. The matrix $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ is initialized as $\mathbf{Y}_0$, assuming that $\mathbf{S}$ is initially zero. The scale $\sigma$ is initialized based on the median absolute deviation of the residuals, and $\mathbf{S}$ is updated later using this noise model. For all of the super technical details of this, see the [original paper](https://doi.org/10.1016/j.sigpro.2022.108816).

The low-rank approximation $\mathbf{L}$ is computed as the product $\mathbf{U} \mathbf{V}^{\top}$. The update equations for $\mathbf{U}$ and $\mathbf{V}$ are:
  $$
  \mathbf{U} = \left( \mathbf{D} \mathbf{V}^{\top} - \lambda_1 \mathbf{U}_p \right) \left( \mathbf{V} \mathbf{V}^{\top} - \lambda_1 \mathbf{I} \right)^{-1}
  $$
  where $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ and $\mathbf{U}_p$ is the previous value of $\mathbf{U}$ and
  $$
  \mathbf{V} = \left( \mathbf{U}^{\top} \mathbf{U} - \lambda_2 \mathbf{I} \right)^{-1} \left( \mathbf{U}^{\top} \mathbf{D} - \lambda_2 \mathbf{V}_p \right)
  $$
where $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ and $\mathbf{V}_p$ is the previous value of $\mathbf{V}$. These updates minimize the approximation error in the low-rank structure of $\mathbf{Y}_0$. In our case, both lambdas are chosen as $\lambda = \frac{1}{\sqrt{\max(m, n)}}$ where $m$ is the number of rows and $n$ is the number of columns for the selected donor matrix.

After updating $\mathbf{U}$ and $\mathbf{V}$, the next step is to update the sparse noise matrix $\mathbf{S}$. This update is based on the residual $\mathbf{T} = \mathbf{Y}_0 - \mathbf{L}$, identifying large residuals that are likely to represent noise. The scale $\sigma$ is computed using the median absolute deviation of the residuals:
$$
\sigma = 10 \times 1.4815 \times \text{median}(\left| \mathbf{T} - \text{median}(\mathbf{T}) \right|).
$$
Then, $\mathbf{S}$ is updated by retaining only the large residuals:
$$
\mathbf{S} = \mathbf{T} \circ \mathbf{Q}_1
$$
where $\mathbf{Q}_1$ is a binary matrix that marks large residuals. The entries of $\mathbf{Q}_1$ are set to 1 if the absolute deviation exceeds $\sigma$, and 0 otherwise. After updating $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{S}$, the algorithm checks for convergence by monitoring the change in the Frobenius norm:
$$
\text{RMSE}[t] = \frac{\| \mathbf{Y}_0 - \mathbf{L} \|_F}{\sqrt{m \cdot n}}
$$
If the change in RMSE between consecutive iterations is smaller than a predefined threshold (in our case, $1 \times 10^{-6}$), the algorithm terminates early. Alright, that's the explanation, just an iterative algorithm to extract the low rank structure. With that, now we do the SCM optimization

$$
\begin{split}\begin{align}
    \underset{w}{\text{argmin}} & \quad ||\mathbf{y}_{1} - \mathbf{L} \mathbf{w}^{\top}||_{2}^2 \\
    \text{s.t.} \: & \mathbf{w}: w_{j} \in \mathbb{R}_{\geq 0}
\end{align}\end{split}
$$
As ususal, we take the dot product between the solution vector and the low-rank structure, and those are our counterfactual predictions.

## Plotting Our Selected Donors


```{python}
#| fig-align: center
#| echo: false

# Substrings to match
substrings = ['Cata', 'Madrid', 'Balear']

# Filter column names that contain any of the substrings
filtered_columns = [col for col in basque_prepped["Ywide"].columns if any(substring in col for substring in substrings)]

# Extract the matrix of the filtered columns
Y0sub = basque_prepped["Ywide"][filtered_columns].to_numpy()

plot_donors_and_treated(Y0sub, basque_prepped['y'], basque_prepped['pre_periods'], 'Basque vs. Subset of Controls')

```
Here we plot the Basque Country versus its selected donors. We can see that these donors (Cataluna, the very closest grey line in Euclidean distnace to the Basque Country), Madrid, and the Balearic Islands are much more similar to the Basque Country than the other 13 control units. Both fPCA-clustering implemented in Mani's dissertation and the clustering over the right singular vectors choose the same donor pool when the pretreatment period extends up to 1975.

#@ The Results from the Models

Here are the estimated synthetic controls with this setup (note that again, we're only using these three chosen donors, not the full 16 unit donor pool).

```{python}
#| fig-align: center

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1975)).astype(int)


treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "Robust": "HQF",
    "Frequentist": False
}

model = CLUSTERSC(config)

arco = model.fit()
```

As we can see the results between HQF-SYNTH and PCR generally agree (note I'm using the Bayesian Ridge regression that [Amjad and co](https://jmlr.org/papers/volume19/17-777/17-777.pdf) advocate for in their paper). You the reader may say "well okay Jared, these results seem to agree. Why do we care about one versus the other?" The _truly interesting thing_, of course, happens when we do in-time placebos. Here is the result for 1973

```{python}
#| fig-align: center
#| echo: false

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)
arco = model.fit()
```
and the next one for 1970

```{python}
#| fig-align: center
#| echo: false

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1970)).astype(int)
arco = model.fit()
```

As we can see from these figures, the original Robust SCM (Bayesian or not!), the model seems to overfit to the pre-intervention period. The fit becomes terrible from the 1970 period to the 1975 period with the RSC/PCR method with the treatment year of 1970, leading to an inflated effect size. Also, the clustering over the right singular vectors, with the 1970 treatment date, selects only Cataluna and Madrid as the donors. The weights given to both are 0.516, which is almost equuivalent to the DID method which assigns proportional weights to the control units. In contrast, the HQF-SCM method doesn't change very much at all when we use the alternate treatment dates. This suggests that more sophisticated robust matrix factorization like Robust PCA/HQF methods plus clustering routines are competitive, at least, to the PCR method, since their effect sizes (in this case) is quite robust to the in-time placebo test. The practical conclusions of the HQF-SCM method remains the same as the original SCM which studied this question.

## Takeaways for Practitioners

The meaning of these results are quite simple: donor selection matters for SCM studies. In fact, simple machine-learning donor selection methods can oftentimes give the same or similar answers to classical studies which oftentimes used covariates (in the original SCM paper, Abadie and his advisor used 13 covariates to construct the counterfactual, returning Cataluna and Madrid as the weighted donors). I say oftentimes because these results are predicated on both assumptions and tuning parameters. We assume, for example, some low rank approximation exists that can fit the pre-intervention time series of the treated unit. The tuning parameters matter too-- the lambda parameters control the sparisty of our results, for example. I use a simple herustic to tune it, but it may be more reasonable to use methods such as cross-validation to select the number of clusters or the values lambda should take, but this would demand simulation evidence.

Either way, the main benefit for policy analysts is that these two methods offer ways to select a donor poor for synthetic control methods, on top of not needing to collect auxilary covariates that the original paper used to obtain very similar counterfactual predictions, and more work should be done on these estimators to see when and why they'd agree.



