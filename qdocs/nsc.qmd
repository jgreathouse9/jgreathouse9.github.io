---
title: "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation"
date: 2025-05-19
categories: [Causal Inference, Econometrics]
---

Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where [the non-linear synthetic control method](https://arxiv.org/abs/2306.01967) comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works.


# Non-Linear SC

Let $\mathbf{y} \in \mathbb{R}^T$ denote the outcome vector of the treated unit over $T$ pre-treatment periods. Let $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$ denote the matrix of outcomes for $J$ control units over the same pre-treatment periods. We seek to find a vector of weights $\mathbf{w} \in \mathbb{R}^J$ that forms an affine combination of the control units' outcomes to best approximate the treated unit.

We define the vector of pairwise discrepancies $\boldsymbol{\delta} \in \mathbb{R}^J$ as
$$
\delta_j = \|\mathbf{y} - \mathbf{Y}_0^{(:,j)}\|_2, \quad j = 1, \dots, J,
$$
which captures the $L_2$ distance between the treated unit and each control unit in the pre-treatment period. The objective is to solve the following constrained optimization problem:
$$
\begin{aligned}
\min_{\mathbf{w} \in \mathbb{R}^J} \quad & \|\mathbf{y} - \mathbf{Y}_0 \mathbf{w}\|_2^2 + a \sum_{j=1}^J \delta_j |w_j| + b \|\mathbf{w}\|_2^2 \\
\text{s.t.} \quad & \mathbf{1}^\top \mathbf{w} = 1
\end{aligned}
$$

The first term, $\|\mathbf{y} - \mathbf{Y}_0 \mathbf{w}\|_2^2$, is a standard least squares loss that penalizes the squared discrepancy between the treated unit's outcomes and the weighted combination of control units. This term ensures that the synthetic control closely approximates the treated unit in the pre-treatment period. The second term, $a \sum_{j=1}^J \delta_j |w_j|$, introduces a discrepancy-weighted $\ell_1$ penalty on the weights. The scalar parameter $a > 0$ controls the strength of this penalty. This term discourages assigning large weights to control units whose outcomes differ substantially from the treated unit, thereby promoting selection of more similar controls. The third term, $b \|\mathbf{w}\|_2^2$, is an $\ell_2$ regularization term (or Tikhonov regularization) that shrinks the weights toward zero in order to stabilize the solution and prevent overfitting. The scalar parameter $b > 0$ determines the degree of shrinkage. The constraint $\mathbf{1}^\top \mathbf{w} = 1$ ensures that the resulting synthetic control is an affine combination of the control units, maintaining scale comparability with the treated unit.

To select appropriate values of the regularization parameters $a$ and $b$, we perform $k$-fold cross-validation over a grid of candidate values. The default CV method that Wei argues for in his paper frankly takes forever (on my end anyways), so I default to `sklearn`'s cross validation which seems to both be quicker and fit about as well. Similar to his approach, I treat each control unit in turn as a "pseudo-treated" unit and attempt to reconstruct its pre-treatment outcomes using a weighted combination of the remaining controls. These reconstructions are repeated over several validation folds.

In each fold, we partition the control units into a training set $\mathbf{Y}_{\text{train}}$ and a validation set $\mathbf{Y}_{\text{val}}$. For each unit $\mathbf{y}^{(j)}$ in the validation set, we solve the following optimization problem:
$$
\begin{aligned}
\min_{\mathbf{w} \in \mathbb{R}^J} \quad & \|\mathbf{y}^{(j)} - \mathbf{Y}_{\text{train}} \mathbf{w}\|_2^2 + a \sum_{l=1}^J \delta_l |w_l| + b \|\mathbf{w}\|_2^2 \\
\text{s.t.} \quad & \mathbf{1}^\top \mathbf{w} = 1,
\end{aligned}
$$
where $\delta_l = \|\mathbf{y}^{(j)} - \mathbf{Y}_{\text{train}}\|_2$ measures the pairwise discrepancy between the pseudo-treated unit and each donor unit in the training set. Let $\widehat{\mathbf{y}}^{(j)} = \mathbf{Y}_{\text{train}} \mathbf{w}^{(j)}$ be the predicted outcome for unit $j$. The validation error is given by the mean squared error:
$$
\text{MSE}^{(j)} = \frac{1}{T} \|\mathbf{y}^{(j)} - \widehat{\mathbf{y}}^{(j)}\|_2^2.
$$

Averaging over all validation units across all folds, we compute the total cross-validated prediction error for a given $(a, b)$. The optimal parameters are then defined as
$$
(a^\ast, b^\ast) = \arg\min_{a \in \mathcal{A},\, b \in \mathcal{B}} \; \text{CVError}(a, b),
$$
where $\mathcal{A}$ and $\mathcal{B}$ denote the candidate grids and $\text{CVError}(a, b)$ is the average mean squared error across folds.

## NSC in ```mlsynth```

Now I will give an example of how to use NSC for your own applied work. As ususal, in order to properly implement this, we begin by installing ```mlsynth``` from my Github

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

And then we load the Proposition 99 dataset and fit the model in the ususal ```mlsynth``` fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.

```{python}
#| fig-align: center

import pandas as pd
from mlsynth.mlsynth import NSC

url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"

# Feel free to change "smoking" with "basque" above in the URL

data = pd.read_csv(url)

# Our method inputs

config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": "blue"}

arco = NSC(config).fit()
```

Here is our observed California versus the counterfactual. The weights shown define an affine combination of control units used to approximate California’s per capita cigarette consumption prior to 1989. The dominant contributor is Nebraska, which carries a weight of 0.540, indicating that it serves as the single most influential donor in replicating California's pre-treatment trajectory. Other notable positive contributors include Montana (0.171), Nevada (0.168), Colorado (0.148), New Mexico and Connecticut (each at 0.127), Idaho (0.122), and Alabama (0.110). This suggests that their historical smoking trends most closely mirrored California’s prior to the intervention. Several states contribute small positive weights, such as Kansas (0.082), Arkansas (0.029), Minnesota (0.013), and Illinois (0.006).

Some control units—North Dakota (-0.250), Tennessee (-0.213), Ohio (-0.195), Indiana (-0.083), and Iowa (-0.035) get negative weight. These negative weights (again bear in mind that this is allowed in affine but not convex combinations) indicate that we are extrapolating beyond the support of the donor pool. Practically, the model is subtracting the influence of these states because their smoking trends diverged from California’s in ways that could distort the synthetic match if left uncorrected.

Many remaining states receive weights that are exactly zero or numerically negligible. These states did not contribute to the synthetic California because either their historical trajectories were too dissimilar from California’s, or the regularization in the model penalized their inclusion due to high discrepancy or redundancy with other controls.
