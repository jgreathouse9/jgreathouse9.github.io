---
title: 'On the Use of Clustering for Synthetic Controls Methods, using mlsynth'
date: 2025-02-03
categories: [Causal Inference, Machine Learning]
---

Picking a donor pool is very important with the synthetic control method. The vanilla method suffers from interpolation biases in the case when the donor pool is not suitable for the target unit. This is farily common with applied SCM in real life, as oftentimes we may have a high-dimensional donor pool and not know what the right control units would be _a priori_. The standard advice given in this instance is to limit our control group to units that are already similar to the target unit. But how do we do this? One way is to use clustering methods to select the control group. Consider [the classic Basque Country example](https://economics.mit.edu/sites/default/files/publications/The%20Economic%20Costs%20of%20Conflict.pdf) below, where the Basque Country undergoes a wave of terrorism in the mid-1970s which is thought to impact their GDP per Capita relative to other areas of Spain.

```{python}
#| fig-align: center
#| echo: false

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

def plot_donors_and_treated(donor_matrix, treated_vector, pre_periods, title):
    plt.figure(figsize=(10, 6))

    # Plot the donor units
    for i in range(donor_matrix.shape[1]):
        plt.plot(donor_matrix[:, i], color='gray', linewidth=0.5, alpha=0.8, label='_nolegend_')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Treated Unit')

    # Plot the normalized average of controls
    plt.plot(donor_matrix.mean(axis=1), color='blue', linewidth=2, label='Average of Controls')

    # Add vertical line for the pre-periods (waterline)
    plt.axvline(x=pre_periods, color='red', linestyle='--', linewidth=1.5, label='Terrorism')

    # Add labels and title
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.legend()

    plt.show()

# Set up theme for Matplotlib
def set_theme():
    theme = {
        "axes.grid": True,
        "grid.linestyle": "-",
        "grid.color": "black",
        "legend.framealpha": 1,
        "legend.facecolor": "white",
        "legend.shadow": True,
        "legend.fontsize": 12,
        "legend.title_fontsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        "axes.labelsize": 10,
        "axes.titlesize": 12,
        "figure.dpi": 100,
        "axes.facecolor": "white",
        "figure.figsize": (10, 6),
    }
    matplotlib.rcParams.update(theme)

set_theme()


file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)
treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

basque_prepped = dataprep(df, unitid, time, outcome, treat)

plot_donors_and_treated(basque_prepped['donor_matrix'], basque_prepped['y'], basque_prepped['pre_periods'], 'Basque vs. Controls')

```

Here, we plot the Basque versus the average of its controls as well as the individual donor outcome vectors themselves. We can see that the Basque Country is one of the wealthiest areas of Spain, up there with Madrid, Cataluna, and the Balearic Islands. We have other donors too, which by comparison are less wealthy. The key, then, is to ask which donors we should select for the synthetic control algorithm to consider in the first place, by exploiting pre-intervention similarities between the treated unit and control group. With "better" pre-policy donors, there is a higher chance that our out-of-sample predictions would be closer to the actual counterfactual.

# Clustering for SCM

To begin, we use SVD over the outcomes matrix $\mathbf{Y}_0 \in \mathbb{R}^{N_0 \times T}$. SVD decomposes it into $\mathbf{Y}_0 = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$, where $\mathbf{U} \in \mathbb{R}^{N_0 \times T}$ contains the left singular vectors, $\mathbf{\Sigma} \in \mathbb{R}^{T \times T}$ is a diagonal matrix of singular values, and $\mathbf{V} \in \mathbb{R}^{T \times T}$ contains the right singular vectors. Using singular value thresholding or taking the spectral rank, keeping only the top $r$ singular values, we have a low-dimensional representation of the outcomes matrix, $\mathbf{M} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$. The transformed representation is then defined as $\tilde{\mathbf{U}} = \mathbf{U} \mathbf{\Sigma}_r$, where $\mathbf{\Sigma}_r$ is the truncated diagonal matrix containing only the retained singular values.

We then perform k-means clustering over the the rows of $\tilde{\mathbf{U}}$, or the singular vectors. We seek to partition the units into $k$ clusters by minimizing the within-cluster variance, solving $\underset{P}{\operatorname*{argmin}} \sum_{i=1}^{N_0} \min_{j \in [k]} \|\tilde{\mathbf{U}}_i - P_j\|^2$, where $P_j$ represents the cluster centers. ``mlsynth`` uses the silhouette method to choose the number of clusters. Each unit is assigned a cluster, and we use the cluster that contains the treated unit as the donor pool.

We now may apply the SCM estimator. The weight vector $\mathbf{w}$ is obtained by solving $\underset{\mathbf{w} \in \mathbb{R}^{n_A}}{\operatorname*{argmin}} \lVert \mathbf{y}_1^- - \mathbf{w}^\top \mathbf{A}^- \rVert_2^2$, where $n_A$ is the cardinality of the selected subset of controls. For the FPCA method, see [the documentation](https://mlsynth.readthedocs.io/en/latest/clustersc.html) for ``mlsynth``.

# Estimating the Causal Impact

We may now estimate the impact with either PCR or another robust matrix factorization method similar to how Mani did in [his dissertation](https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=6069&context=gc_etds). We use an iterative algorithm to exrtact the low-rank matrix of our new donor pool. Mani uses what we would call Principal Componenet Pursuit, but another such method is non-convex half-quadratic regularization (HQF). HQF is a form of Robust PCA, a general class of methods which aims to decompose the observed noisy matrix $\mathbf{Y}_0$ into a low-rank matrix $\mathbf{L}$ and a sparse matrix $\mathbf{S}$, such that $\mathbf{Y}_0 = \mathbf{L} + \mathbf{S}$, where $\mathbf{L} = \mathbf{U} \mathbf{V}^T$ represents the low-rank approximation, and $\mathbf{S}$ models the sparse noise. The optimization proceeds iteratively by updating $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{S}$. The matrices $\mathbf{U}$ and $\mathbf{V}$ are initialized randomly with dimensions $m \times r$ and $n \times r$, respectively. The matrix $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ is initialized as $\mathbf{Y}_0$, assuming that $\mathbf{S}$ is initially zero. The scale $\sigma$ is initialized based on the median absolute deviation of the residuals, and $\mathbf{S}$ is updated later using this noise model. For all of the super technical details of this, see the [original paper](https://doi.org/10.1016/j.sigpro.2022.108816).

The low-rank approximation $\mathbf{L}$ is computed as the product $\mathbf{U} \mathbf{V}^T$. The update equations for $\mathbf{U}$ and $\mathbf{V}$ are:
  $$
  \mathbf{U} = \left( \mathbf{D} \mathbf{V}^T - \lambda_1 \mathbf{U}_p \right) \left( \mathbf{V} \mathbf{V}^T - \lambda_1 \mathbf{I} \right)^{-1}
  $$
  where $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ and $\mathbf{U}_p$ is the previous value of $\mathbf{U}$ and
  $$
  \mathbf{V} = \left( \mathbf{U}^T \mathbf{U} - \lambda_2 \mathbf{I} \right)^{-1} \left( \mathbf{U}^T \mathbf{D} - \lambda_2 \mathbf{V}_p \right)
  $$
where $\mathbf{D} = \mathbf{Y}_0 - \mathbf{S}$ and $\mathbf{V}_p$ is the previous value of $\mathbf{V}$. These updates minimize the approximation error in the low-rank structure of $\mathbf{Y}_0$.

After updating $\mathbf{U}$ and $\mathbf{V}$, the next step is to update the sparse noise matrix $\mathbf{S}$. This update is based on the residual $\mathbf{T} = \mathbf{Y}_0 - \mathbf{L}$, identifying large residuals that are likely to represent noise. The scale $\sigma$ is computed using the median absolute deviation of the residuals:
$$
\sigma = 10 \times 1.4815 \times \text{median}(\left| \mathbf{T} - \text{median}(\mathbf{T}) \right|).
$$
Then, $\mathbf{S}$ is updated by retaining only the large residuals:
$$
\mathbf{S} = \mathbf{T} \circ \mathbf{Q}_1
$$
where $\mathbf{Q}_1$ is a binary matrix that marks large residuals. The entries of $\mathbf{Q}_1$ are set to 1 if the absolute deviation exceeds $\sigma$, and 0 otherwise. After updating $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{S}$, the algorithm checks for convergence by monitoring the change in the Frobenius norm:
$$
\text{RMSE}[t] = \frac{\| \mathbf{Y}_0 - \mathbf{L} \|_F}{\sqrt{m \cdot n}}
$$
If the change in RMSE between consecutive iterations is smaller than a predefined threshold (in our case, $1 \times 10^{-6}$), the algorithm terminates early.

```{python}
#| fig-align: center

import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.mlsynth import dataprep
from mlsynth.mlsynth import CLUSTERSC
import matplotlib
import numpy as np

file = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
# Load the CSV file using pandas
df = pd.read_csv(file)

df["Terrorism"] = (df["regionname"].str.contains("Basque", case=False, na=False) & (df["year"] >= 1973)).astype(int)


treat = "Terrorism"
outcome = "gdpcap"
unitid = "regionname"
time = "year"

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "Robust": "HQF",
    "Frequentist": False
}

model = CLUSTERSC(config)

arco = model.fit()
```

Here we have the Basque dataset.
