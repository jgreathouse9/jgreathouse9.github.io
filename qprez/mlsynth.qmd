---
title: "Counterfactual Estimation via ```mlsynth```: A Practical Guide"
author: "Jared Greathouse"
date: today
format: 
  revealjs:
    scrollable: true
theme: simple
highlight-style: github
---


## Counterfactual Estimation

- Maximizing value for business services and crafting effective public policy depends on knowing whether policies, promotions, new web features, taxes, or other interventions _meaningfully_ affect metrics that we care about.

- Researchers commonly use Difference-in-Differences designs and Synthetic Control methods (DID and SCM) to measure causal impacts.

- Both approaches use weighted averages of control groups from the pre-treatment period to estimate counterfactuals, or the post-treatment values of the outcome we'd see absent treatment.

---

## DID and SCM

Both methods optimize over pre-treatment periods.

- For DID, we have  
  $$
  \begin{aligned}
      \underset{\mathbf{w} \in \mathbb{R}^{N_0}}{\operatorname*{argmin}} &\quad \lVert \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} - \boldsymbol{\beta}_0 \rVert_2^2, \\
      \text{s.t.} &\quad \mathbf{w} = \frac{1}{N_0} \mathbf{1}_{N_0}.
  \end{aligned}
  $$

- SCM imposes $\Delta \operatorname{:=} \{\lVert \mathbf{w} \rVert_1 = 1  \mid \mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0}\}.$ We learn the weights via  
  $$
  \underset{\mathbf{w} \in \Delta}{\operatorname*{argmin}} \lVert \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \rVert_2^2.
  $$

Note there's no intercept, and we presume the convex combination of the donor pool will be well-fitting.

---

## Potential Problems

However, SCM and DID may struggle sometimes.

::: {.panel-tabset}

### DID

- DID requires $\mathbb{E}[y_{1t}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t}(0)] = \mathbb{E}[y_{1t-1}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t-1}(0)]$, or parallel trends. Does not hold when the mean of controls is dissimilar to the trend of the treated unit/group.

```{python}
#| echo: false

import pandas as pd
from mlsynth.mlsynth import dataprep
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Custom Matplotlib theme (Uber-style)
ubertheme = {
    "figure.facecolor": "white",
    "figure.dpi": 100,
    "figure.titlesize": 16,
    "figure.titleweight": "bold",
    "lines.linewidth": 1.2,
    "patch.facecolor": "#0072B2",  # Blue shade for patches
    "xtick.direction": "out",
    "ytick.direction": "out",
    "font.size": 16,
    "font.family": "sans-serif",
    "font.sans-serif": ["DejaVu Sans"],
    "axes.grid": True,
    "axes.facecolor": "white",
    "axes.linewidth": 0.1,
    "axes.titlesize": "large",
    "axes.titleweight": "bold",
    "axes.labelsize": "medium",
    "axes.labelweight": "bold",
    "axes.spines.top": False,
    "axes.spines.right": False,
    "axes.spines.left": False,
    "axes.spines.bottom": False,
    "axes.titlepad": 25,
    "axes.labelpad": 20,
    "grid.alpha": 0.1,
    "grid.linewidth": 0.5,
    "grid.color": "#000000",
    "legend.framealpha": 0.5,
    "legend.fancybox": True,
    "legend.borderpad": 0.5,
    "legend.loc": "best",
    "legend.fontsize": "small",
    "figure.figsize": (11,5)
}

matplotlib.rcParams.update(ubertheme)

def plot_treated_vs_controls(donor_matrix, treated_vector, pre_periods, title):
    """
    Plots a single treated unit against the control group.

    Parameters:
    - donor_matrix (numpy.ndarray): A 2D array where each column represents a control unit.
    - treated_vector (numpy.ndarray): A 1D array representing the treated unit.
    - pre_periods (int): The cutoff time index for the pre-treatment period.
    - title (str): The title of the plot.
    """

    # Indicate pre-treatment period cutoff
    plt.axvline(x=pre_periods, color='blue', linestyle='--', linewidth=1.5, label='Treatment Date')

    # Plot control group trajectories
    plt.plot(donor_matrix, color='gray', linewidth=0.2, alpha=0.35, label='_nolegend_')

    # Plot the average of control units
    average_controls = donor_matrix.mean(axis=1)
    plt.plot(average_controls, color='red', linewidth=1, label='Mean of Controls')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Sayulita')

    # Labels and legend
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.ylabel('New Drivers')
    plt.legend()

    plt.show()

np.random.seed(3111)

# Parameters
n_units = 100
n_periods = 104
n_factors = 12
rho = 0.85
sigma = 1.25

treated_unit = 54
treatment_period = 76
treatment_effect = 15

# List of Cities, for Monte Carlo Could simply be generalized to "Cities + n"
cities = [
    "São Paulo", "Mexico City", "San Carlos de Bariloche", "Rio de Janeiro", "Ushuaia",
    "Bogotá", "Santiago", "Caracas", "Guayaquil", "Quito",
    "Brasília", "Montevideo", "Asunción", "El Paso", "Playa del Carmen",
    "Medellín", "Porto Alegre", "Placencia", "Recife", "Salvador",
    "Zihuatanejo", "San José", "Panama City", "Barranquilla", "Tegucigalpa",
    "Foz do Iguaçu", "Maracaibo", "Rosario", "Maracay", "Antofagasta",
    "San Pedro Sula", "San Juan", "Chihuahua", "Cayo District", "Maturín",
    "Buzios", "Puebla", "Miami", "Arequipa", "Fernando de Noronha", "Guatemala City",
    "Zacatecas", "Mérida", "Córdoba", "San Miguel", "Trujillo",
    "Corozal Town", "Santa Cruz de la Sierra", "San Luis Potosí", "Jalapão", "Potosí",
    "Tucumán", "Neuquén", "La Plata", "Sayulita", "Florianópolis", "Lagos de Moreno",
    "La Paz", "Belém", "Venezuela", "Ribeirão Preto", "Valparaíso",
    "Marília", "Campinas", "Vitoria", "Sorocaba", "Santa Fe",
    "San Salvador", "Lima", "Buenos Aires", "Curitiba", "Maceió",
    "Los Angeles", "La Ceiba", "Puerto La Cruz", "Olinda", "Monterrey",
    "Ibagué", "Cúcuta", " Paraty", "Cancún", "Puerto Vallarta", "Chiclayo", "Ambato",
    "Pucallpa", "Santa Marta", "Villavicencio", "Paraná", "Cauca", "San Vicente",
    "Cali", "Tarija", "Manzanillo", "El Alto", "Santiago de Chile", "Cochabamba",
    "Cartagena", "Santo Domingo", "Durango", "Puerto Viejo de Talamanca"
]

# Setting the common factors
common_factors = np.zeros((n_periods, n_factors))

# Setting the common factors for period 1
common_factors[0, :] = np.random.normal(0, 1, n_factors)

for t in range(1, n_periods):
    common_factors[t, :] = rho * common_factors[t-1, :] + np.random.normal(0, 1, n_factors)

# Factor Loadings with error terms
factor_loadings = np.random.normal(0, 1, (n_units, n_factors))

# Error term for model
error_terms = np.random.normal(0, sigma, (n_periods, n_units))


data = np.zeros((n_periods, n_units))

for i in range(n_units):
    data[:, i] = common_factors @ factor_loadings[i, :] + error_terms[:, i]+np.random.randint(40, 80)


data[treatment_period:, treated_unit] += treatment_effect


market_names = cities
time = np.repeat(np.arange(1, n_periods+1), n_units)
markets = np.tile(market_names, n_periods)

# Reshape data into the correct format for DataFrame
sales = data.flatten()

indicator = np.where((markets ==cities[treated_unit]) & (time >= treatment_period), 1, 0)

# Create DataFrame
df = pd.DataFrame({
    'Market': markets,
    'Time': time,
    'Drivers': sales,
    'Uber Green': indicator
})

treat = "Uber Green"
time = "Time"
outcome = "Drivers"
unitid = "Market"


prepped = dataprep(df, unitid, time, outcome, treat)

plot_treated_vs_controls(prepped["donor_matrix"], prepped["y"], prepped["pre_periods"], "Sayulita vs. Controls")
```

### SCM

- SCM struggles when $N_0>>T_0$, and is prone to overfitting in this setting. Also, smaller donor pools are preferable to mitigate interpolation biases. But picking a good donor pool can be challenging when there are very many controls.

- Convex SCM also favors sparse solutions. But what if the true vector of weights is _not_ mostly 0? What if the DGP is actually dense instead of sparse?

:::

---

## Potential Solutions

- Fortunately, econometricians have crafted methods which address these issues.

- Many of these methods are re-formulations or relaxations of the standard toolkit.

---

## Potential Solutions, cont.

For example...

::: {.panel-tabset}

### Forward DID

```{python}
#| echo: false
#| fig-align: center

import qrcode
from PIL import Image
import matplotlib.pyplot as plt

# Generate QR code
url = "https://doi.org/10.1287/mksc.2022.0212"

# Display the QR code in Quarto
plt.imshow(qrcode.make(url), cmap="gray")
plt.axis("off")  # Hide axes
plt.show()

```

- Forward DID uses forward selection to choose a donor pool/control group for a single treated unit, using the DID estimator.

- However, its only existing implementations are either in MATLAB and R.

- MATLAB naturally is not free! And it, as well as the R code, requires users to have a wide dataframe and manually specify the number of pre-treatment periods.

- This may work fine for one study, but what about if we do not have 44 pretreatment periods?


```{python}

#| eval: false
#| echo: true

data = read.csv("GDP.csv", sep=",", header = TRUE)

t=dim(data)[1]
no_control=dim(data)[2]-1
control_ID=1:no_control
t1=44           # t1 is the pretreatment sample size
y=data[,1]
y1=data[1:t1,1]
y2=data[(t1+1):t,1]}


```

### RSC

```{python}
#| echo: false
#| fig-align: center

import qrcode
from PIL import Image
import matplotlib.pyplot as plt

# Generate QR code
url = "https://jmlr.org/papers/v19/17-777.html"

# Display the QR code in Quarto
plt.imshow(qrcode.make(url), cmap="gray")
plt.axis("off")  # Hide axes
plt.show()

```

- Robust Synthetic Control uses PCA to denoise the donor pool and use the low-rank approximation to learn the weights. An upshot of this method is that, in their empirical applications at least, the method appears to not rely on additional covariate metrics that standard SCMs need to attain acceptable fit.

- However, one [existing implementation](https://github.com/sdfordham/pysyncon/blob/main/examples/robust/basque_robust.ipynb) requires the user to manually specify the number of principal components/singular values as well as the lambda parameter. [Another](https://github.com/deshen24/panel-data-regressions/blob/master/case_study.py) requires an even more complex setup.

### And...

- The existing implementations (arguably) presume a more advanced knowledge of machine-learning, econometrics, and/or programming than the modal marketing scientist, policy analyst/economist, or data scientist is **likely** to have.

- Many of the steps critical to the analysis are not automated, leaving room for small errors.

- This combination makes it harder/more cumbersome for the applied scientist to use these methods in practice. 

:::

---

## Enter ```mlsynth```...

- The Python library ```mlsynth``` is meant to further democratize causal inference and econometrics, making its benefits available to a wider class of researchers.

- ```mlsynth``` automates these estimators (and more!).

- Its syntax is very simple, and the steps are totally automated.

- This presentation will cover ```mlsynth``` and how to use it in practice. I will describe a few of the algorithms and give examples of how to use them.

## Installing ```mlsynth```

To install ```mlsynth```, we need:

- Python 3.9 or later

- ```cvxpy, matplotlib, numpy, pandas, scipy, scikit-learn, statsmodels```

- And then, we can install this from the command line

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

---

## Using ```mlsynth```

- A ```df```.

- A Time variable (numeric or date-time are allowed).

- A unit variable (a string, to know which units are which).

- A numeric outcome variable.

- A dummy variable denoting treatment, 1 when the treatment is active and the unit is treated, else 0 for all other units and times.

---

## Hubei Lockdown

```{python}
#| echo: false
#| fig-align: center

from mlsynth.mlsynth import CLUSTERSC, dataprep
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib

ubertheme = {
    "figure.facecolor": "white",
    "figure.figsize": (11, 5),
    "figure.dpi": 100,
    "figure.titlesize": 16,
    "figure.titleweight": "bold",
    "lines.linewidth": 1.2,
    "patch.facecolor": "#0072B2",  # Blue shade for patches
    "xtick.direction": "out",
    "ytick.direction": "out",
    "font.size": 14,
    "font.family": "sans-serif",
    "font.sans-serif": ["DejaVu Sans"],
    "axes.grid": True,
    "axes.facecolor": "white",
    "axes.linewidth": 0.1,
    "axes.titlesize": "large",
    "axes.titleweight": "bold",
    "axes.labelsize": "medium",
    "axes.labelweight": "bold",
    "axes.spines.top": False,
    "axes.spines.right": False,
    "axes.spines.left": False,
    "axes.spines.bottom": False,
    "axes.titlepad": 25,
    "axes.labelpad": 20,
    "grid.alpha": 0.1,
    "grid.linewidth": 0.5,
    "grid.color": "#000000",
    "legend.framealpha": 0.5,
    "legend.fancybox": True,
    "legend.borderpad": 0.5,
    "legend.loc": "best",
    "legend.fontsize": "small",
}

matplotlib.rcParams.update(ubertheme)

df = pd.read_csv('https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/HubeiSCM.csv')

unitid = df.columns[1]
time = df.columns[-1]
outcome = df.columns[2]
#df[outcome] = np.log(df[outcome])

treat = df.columns[-2]

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True, "cluster": True #, "objective": "SIMPLEX"
}

prepdict = dataprep(df, unitid, time, outcome, treat)

Nmean = np.mean(prepdict["donor_matrix"],axis=1)

y = prepdict["y"]

plt.figure(figsize=(10, 6))
# Extract the dates from Ywide's index
time_index = prepdict["Ywide"].index

plt.axvline(x=time_index[prepdict["pre_periods"] - 1], color="blue", linestyle="--", linewidth=2, label="Lockdown Date")


# Plot control average in red
plt.plot(time_index, Nmean, color="red", linewidth=2, label="Control Avg")

# Plot treated unit in black
plt.plot(time_index, y, color="black", linewidth=2, label="Treated Unit")

# Format x-axis labels
plt.xticks(rotation=45)
plt.xlabel("Date")
plt.ylabel("GDP per Capita")
plt.legend()
plt.show()


```
