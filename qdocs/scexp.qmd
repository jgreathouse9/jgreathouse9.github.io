---
title: 'Synthetic Controls for Marketing Experiments'
date: 2025-09-20
categories: [Experiments, Econometrics]
---

# The Difficulties of Market Experimentation

[Suppose the government of Curaçao](https://www.travelpulse.com/news/destinations/curacao-takes-proactive-steps-toward-sustainability-amid-global-overtourism) wishes to implement a new ["Green Stay" initiative](https://www.businesstravelnewseurope.com/Accommodation/HRS-Green-Stay-updates-are-set-to-increase-hotel-participation) that encourages hotels to adopt sustainability measures such as reducing water usage, improving waste management, and shifting toward renewable energy. From a methodological standpoint, the most rigorous way to measure the impact of such a program would be through a randomized controlled trial, where some neighborhoods are randomly assigned to implement the policy while others serve as controls. Randomization ensures that, in expectation, treated and control groups are balanced, and that differences in outcomes can be attributed to the policy. It also provides a clear framework for statistical inference and lends credibility to the findings.

Despite these advantages, there are serious obstacles to conducting a true RCT in this context. Ethically, assigning certain neighborhoods to receive the benefits while denying them to others may be perceived as unfair, particularly if the policy boosts reputation, attracts eco-conscious travelers, or leads to financial advantages. Guests might also experience different standards unknowingly, raising questions about fairness and transparency.

Politically, implementing randomization across hundreds of neighborhoods would require coordination among the government, hotel associations, and local businesses, all of whom may resist being “experimented on.” Because tourism is central to the island’s economy, public perception would be sensitive, and randomized assignment could easily be framed as a risk to one of Curaçao’s most important industries.

The logistical burden is equally daunting: monitoring and enforcing different sustainability requirements neighborhood by neighborhood would be complex and costly, with significant overhead required for compliance and enforcement. In practice, coordinating hundreds of independent hotels is unlikely to succeed. In other words, while an RCT is desirable in theory, ethical, political, and feasibility barriers make it impractical in the context of Curaçao’s hotel sector.

Given these challenges, a more practical approach is to consider the market at a localized level, grouping neighborhoods into clusters that share similar characteristics, such as geographic location, customer demographics, or historical patterns of tourist activity. By focusing on clusters rather than individual units, we mitigate ethical concerns of treating some neighborhoods differently than others, since selection occurs within naturally similar groups rather than arbitrarily across the whole market.

Cluster analysis also reduces logistical complexity, because interventions can be coordinated at the cluster level rather than needing to monitor and enforce policies across hundreds of disparate hotels. Within each cluster, we can select treated neighborhoods that closely resemble the local market and control neighborhoods that reflect the cluster’s underlying dynamics. This ensures that observed effects of the intervention are not driven by unusual outliers or idiosyncratic behaviors, but rather capture the typical response within a given market segment.

But how would we select which neighborhoods to treat and which to use as controls? Even within a single cluster, we cannot treat everybody for practical reasons. To address this, we can use synthetic control methods, which construct “synthetic neighborhoods” that mimic the characteristics of treated clusters, allowing analysts to experiment with different treatment assignments and evaluate their likely impact rigorously.

## Notation and Synthetic Control Designs

In our Curaçao Green Stay example, the unit of treatment is the neighborhood, not individual hotels. That is, entire neighborhoods may adopt sustainability measures, while others remain untreated. Let $\mathcal{J} = \{1, \dots, J\}$ denote neighborhoods observed over $T_0$ pre-treatment periods $\mathcal{T}_0 = \{1, \dots, T_0\}$, nested within $K$ clusters $\mathcal{K} = \{1, \dots, K\}$ that group neighborhoods with similar characteristics (e.g., location, tourist demographics, historical occupancy). Let $I_k \subseteq \mathcal{J}$ denote neighborhoods in cluster $k$, with $j \in I_{k(j)}$ indicating cluster membership. In our case, we have 21 units of interest, where some will be treated and others will not. We have 128 pre-treatment periods, and 28 post periods. Observed outcomes, like occupancy rates or energy use, are collected in $\mathbf{Y} \in \mathbb{R}^{J \times T_0}$, with $y_{jt}$ representing the outcome for neighborhood $j$ at time $t$. Potential outcomes are $y_{jt}^I$ if the neighborhood participates in Green Stay and $y_{jt}^N$ if not. The cluster-weighted average treatment effect is

$$
\tau_t = \sum_{j=1}^J f_j \, (y_{jt}^I - y_{jt}^N), \quad t > T_0,
$$

where $f_j \ge 0$ and $\sum_{j=1}^J f_j = 1$. Predictor vectors $\mathbf{x}_j \in \mathbb{R}^r$ capture pre-treatment characteristics of neighborhoods, such as the average energy use, hotel occupancy, or typical guest demographics, with cluster mean

$$
\mathbf{\bar{x}}_k = \frac{\sum_{j \in I_k} f_j \mathbf{x}_j}{\sum_{j \in I_k} f_j}.
$$

Distances are defined as

$$
D_{1,j,k} = \|\mathbf{x}_j - \mathbf{\bar{x}}_k\|_2^2, \quad
D_{2,j,j,k} = \|\mathbf{x}_j - \mathbf{x}_{j}\|_2^2.
$$

We define synthetic treated and control neighborhoods via weights $w_j \ge 0$ and $v_{i,j} \ge 0$, and neighborhoods are either treated or not via $z_j \in \{0,1\}$. Optional costs $\mathbf{c}$ and cluster budgets $B_k$ constrain feasible assignments. The feasible set is

$$
\begin{aligned}
\mathcal{F} = \Big\{ (w,v) \,\Big| \;
& w_j, v_j \ge 0, \quad \sum_{j \in I_k} w_j = \sum_{j \in I_k} v_j = 1, \\
& w_j \le z_j, \; v_j \le 1 - z_j, \quad \sum_{j \in I_k} z_j \in [m_{\text{min},k}, m_{\text{max},k}], \\
& \sum_{j \in I_k} c_j w_j \le B_k, \quad \sum_{k=1}^K \sum_{j \in I_k} c_j w_j \le B_{\text{total}}, \\
& \sum_{k=1}^K z_{j,k} \le 1
\Big\}.
\end{aligned}
$$

Intuitively, in the Curaçao context:

1. **Non-negativity and normalization ($w_j, v_j \ge 0$, $\sum w_j = \sum v_j = 1$):**  
   Synthetic neighborhoods are weighted averages of real neighborhoods. Negative weights would imply “subtracting” a neighborhood’s influence. Summing to one ensures each synthetic neighborhood is a proper mixture of real neighborhoods.

2. **Treatment assignment ($z_j \in \{0,1\}, w_j \le z_j, v_j \le 1 - z_j$):**  
   Each neighborhood is either treated or not. A treated neighborhood contributes to the synthetic treated unit but not the control unit, and vice versa, avoiding “double-counting.”

3. **Cardinality constraints ($\sum z_j \in [m_{\text{min},k}, m_{\text{max},k}]$):**  
   Ensures at least one neighborhood is treated per cluster to observe effects, but not every neighborhood, respecting ethical, financial, or logistical limits.

4. **Budget constraints ($\sum c_j w_j \le B_k$, $\sum c_j w_j \le B_{\text{total}}$):**  
   Neighborhoods may have different costs. Cluster- and global-level budgets ensure the program remains financially feasible.

5. **Cluster exclusivity ($\sum_k z_{j,k} \le 1$):**  
   Each neighborhood can only be treated in one cluster to avoid overlapping treatments that could confound the effect.

The clustered synthetic control estimator is obtained by solving

$$
\min_{(\mathbf{w},\mathbf{v}) \in \mathcal{F}} \; \mathcal{L}(\mathbf{w},\mathbf{v}),
$$

where $\mathcal{F}$ is the feasible set defined by nonnegativity, within–cluster normalization, exclusivity, and any budget constraints. Each cluster $k \in \{1,\dots,K\}$ has member indices $I_k$, cluster mean $\mathbf{\bar{x}}_k$, and outcomes $\mathbf{X}_{I_k}$. 

The optimization problem chooses synthetic treated and control neighborhoods by minimizing a loss function subject to feasibility constraints. At its core, the method constructs weighted averages of units to be treated and untreated, such that both averages resemble the cluster as a whole and each other.

The base estimator is the most straightforward. For each cluster $k$, it tries to make the synthetic treated and synthetic control neighborhoods look like the cluster mean:

$$
\mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v}) = \sum_{k=1}^K \Big( \mathbf{f}_{I_k}^\top \mathbf{1} \Big) \Big[
\|\bar{\mathbf{x}}_k - \mathbf{X}_{I_k}^\top \mathbf{w}_{I_k}\|_2^2
+ \|\bar{\mathbf{x}}_k - \mathbf{X}_{I_k}^\top \mathbf{v}_{I_k}\|_2^2
\Big].
$$

This guarantees that both sides of the experiment are anchored to the same benchmark—the average of their cluster.

The weakly penalized version extends this idea by also encouraging the treated and control groups to resemble each other directly:

$$
\mathcal{L}_{\text{Weak}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \beta \|\mathbf{X}_{I_k}^\top \mathbf{w}_{I_k} - \mathbf{X}_{I_k}^\top \mathbf{v}_{I_k}\|_2^2.
$$

In practice, this prevents the optimization from drifting toward two very different synthetic groups that both happen to match the cluster mean.

The penalized estimator takes things further by adding distance-based penalties. These ensure that the selected neighborhoods are not only good statistical matches but also geographically or demographically close to the cluster mean:

$$
\mathcal{L}_{\text{Penalized}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \lambda_1 \mathbf{w}_{I_k}^\top D_{1,k} \mathbf{w}_{I_k}
+ \lambda_2 \mathbf{v}_{I_k}^\top D_{1,k} \mathbf{v}_{I_k}.
$$

Here, the matrices $D_{1,k}$ encode distances, so neighborhoods that are far from the cluster center are penalized more heavily.

Finally, the unit-level estimator introduces the most granular controls. It not only enforces closeness to cluster means but also requires that each treated neighborhood has a tight match with its synthetic control, and that pairs of neighborhoods within the cluster do not diverge too much:

$$
\mathcal{L}_{\text{Unit}}(\mathbf{w},\mathbf{v}) = \mathcal{L}_{\text{Base}}(\mathbf{w},\mathbf{v})
+ \xi \, \big(\mathbf{w}_{I_k}^\top \, \mathrm{diag}(\|\mathbf{X}_{I_k} - \mathbf{X}_{I_k} \mathbf{V}_k\|_F^2)\big)
+ \lambda_{2,\text{unit}} \, \mathrm{tr}(\mathbf{W}_{I_k}^\top D_{2,k} \mathbf{V}_{I_k}).
$$

This is the most restrictive design. It is best suited for cases where heterogeneity across neighborhoods matters, and analysts want each treated area to have a carefully matched control counterpart.

Together, these estimators form a hierarchy. The base version establishes a simple anchor; the weak estimator prevents divergence; the penalized estimator incorporates geographic or demographic realism; and the unit-level estimator enforces fine-grained one-to-one balance. Analysts can experiment across this ladder, choosing the design that best matches the goals and constraints of their study.


## Intuition

What’s particularly interesting about this setup is that we’re not necessarily estimating treatment effects yet. Instead, the synthetic control framework is acting as a decision-making tool: it tells us which neighborhoods to treat and why, based on the trade-offs encoded in the objective function. Analysts can tweak the penalties, change clustering schemes, or adjust budget and cardinality constraints to see how the proposed treatment assignments change.

For example, increasing $\beta$ encourages the treated neighborhoods to be closer to the synthetic controls, effectively prioritizing representativeness of the treated units. Adjusting $\lambda_1$ and $\lambda_2$ emphasizes fidelity to cluster averages, penalizing outlier neighborhoods from being selected. Unit-level penalties $\xi$ and $\lambda_{2,j}$ allow analysts to ensure micro-level balance, focusing on individual neighborhoods with unique characteristics.

Because the framework is fully flexible, it becomes a sandbox for experimental design: you can simulate different cost and budget allocations, understand which neighborhoods are likely to be chosen under various priorities, and make informed decisions before the business/government does any treatment. In other words, this lets analysts answer: "Given our objectives and constraints, which neighborhoods should be treated, and why?" before any policy is implemented, making the framework extremely valuable for planning ethical, practical, and representative market experiments.


## Inference

We now evaluate the treatment effects implied by the synthetic control design. The design phase determines which neighborhoods are treated versus control and how clusters are formed; the inference phase estimates the treatment effects and quantifies their uncertainty. Specifically, we (1) compute unit-level differences between synthetic treated and synthetic control outcomes, (2) aggregate these differences to obtain cluster-weighted and overall effects, and (3) use blank pre-treatment periods to quantify uncertainty via split-conformal confidence intervals and permutation tests.

### Time Periods

To ensure clarity, we define the time periods explicitly. The full pre-treatment period is $\mathcal{T}_0 = \{1, \dots, T_0\}$, with $T_0 = 128$ periods in the Curaçao example. This is divided into the fitting period $\mathcal{T}_1 = \{1, \dots, T_0 - |\mathcal{B}|\}$, used to compute the synthetic control weights ($\mathbf{w}_j$, $\mathbf{v}_j$) and treatment assignments ($z_j$), and the blank period $\mathcal{B} = \{T_0 - |\mathcal{B}| + 1, \dots, T_0\}$, withheld for placebo tests, with $|\mathcal{B}| = 28$ periods. The post-treatment period is $\mathcal{T}_2 = \{T_0 + 1, \dots, T_0 + 28\}$, covering 28 periods. Thus, $\mathcal{T}_0 = \mathcal{T}_1 \cup \mathcal{B}$.

### Unit- and Cluster-Level Effects

We estimate treatment effects by comparing synthetic treated and control outcomes. For neighborhood $j \in \mathcal{J}$, the synthetic treated outcome at time $t$ is constructed using the weights $\mathbf{w}_j$ from the design phase:

$$
y_{j \in \mathcal{N}_1t} = \sum_{j \in I_k : z_{j} = 1} \mathbf{w}_{j} y_{jt},
$$

and the synthetic control outcome is:

$$
y_{j \in \mathcal{N}_0t} = \sum_{j \in I_k : z_{j} = 0} \mathbf{v}_{j} y_{jt},
$$

where $y_{jt}$ is the observed outcome for neighborhood $j$ at time $t$, and $k$ is the cluster containing $j$ (i.e., $j \in I_k$). The unit-level treatment effect for neighborhood $j$ at post-treatment time $t \in \mathcal{T}_2$ is:

$$
\hat{\tau}_{j \in \mathcal{N}_1t} = y_{j \in \mathcal{N}_1t} - y_{j \in \mathcal{N}_0t}, \quad j \in \mathcal{J}, \, t \in \mathcal{T}_2.
$$

This represents the difference between the synthetic treated and control outcomes for neighborhood $j$. We aggregate unit-level effects for treated neighborhoods ($z_j = 1$) within each cluster $k \in \mathcal{K}$ using pre-specified weights $f_j \ge 0$, where $\sum_{j \in I_k} f_j = 1$ within each cluster, to form the cluster-level effect:

$$
\hat{\tau}_{k t} = \sum_{j \in I_k : z_j = 1} f_j \hat{\tau}_{j \in \mathcal{N}_1t}, \quad k \in \mathcal{K}, \, t \in \mathcal{T}_2.
$$

The overall effect across all clusters at time $t \in \mathcal{T}_2$ is:

$$
\hat{\tau}_{t} = \sum_{k=1}^K \hat{\tau}_{k t} = \sum_{j \in \mathcal{J} : z_j = 1} f_j \hat{\tau}_{j \in \mathcal{N}_1t}, \quad t \in \mathcal{T}_2,
$$

where $\sum_{j \in \mathcal{J}} f_j = 1$ ensures proper weighting across all neighborhoods.

### Placebo Effects

Placebo effects provide a reference distribution for treatment effects under the null hypothesis of no treatment, using the blank pre-treatment periods $\mathcal{B}$. We compute placebo effects using the same synthetic control weights ($\mathbf{w}_j$, $\mathbf{v}_j$) and treatment assignments ($z_j$) derived from the fitting period $\mathcal{T}_1$. For neighborhood $j$ at time $t \in \mathcal{B}$, the unit-level placebo effect is:

$$
\hat{\tau}_{j \in \mathcal{N}_1t}^{\mathcal{B}} = y_{j \in \mathcal{N}_1t} - y_{j \in \mathcal{N}_0t}, \quad j \in \mathcal{J}, \, t \in \mathcal{B},
$$

where $y_{j \in \mathcal{N}_1t}$ and $y_{j \in \mathcal{N}_0t}$ are defined as above, using the observed outcomes in $\mathcal{B}$ and the weights from $\mathcal{T}_1$. Cluster-level placebo effects are:

$$
\hat{\tau}_{k t}^{\mathcal{B}} = \sum_{j \in I_k : z_j = 1} f_j \hat{\tau}_{j \in \mathcal{N}_1t}^{\mathcal{B}}, \quad k \in \mathcal{K}, \, t \in \mathcal{B}.
$$

The overall placebo effect is:

$$
\hat{\tau}_{t}^{\mathcal{B}} = \sum_{k=1}^K \hat{\tau}_{k t}^{\mathcal{B}} = \sum_{j \in \mathcal{J} : z_j = 1} f_j \hat{\tau}_{j \in \mathcal{N}_1t}^{\mathcal{B}}, \quad t \in \mathcal{B}.
$$

These placebo effects quantify the variability of synthetic differences in the absence of treatment, serving as a benchmark for statistical inference.

### Split-Conformal Confidence Intervals

To quantify uncertainty, we construct split-conformal confidence intervals using the placebo effect distribution. These intervals ensure that the true effect lies within the bounds with probability $1-\alpha$ in repeated samples, based on the variability observed in $\mathcal{B}$. For the overall effect at $t \in \mathcal{T}_2$, we compute the $(1-\alpha)$-quantile of the absolute overall placebo effects:

$$
q_{1-\alpha} = \text{Quantile}_{1-\alpha}\!\big(|\hat{\tau}_{t}^{\mathcal{B}}|\big), \quad t \in \mathcal{B}.
$$

The confidence interval for $\hat{\tau}_t$ is:

$$
\mathrm{CI}_{t} = \big[\hat{\tau}_{t} - q_{1-\alpha}, \, \hat{\tau}_{t} + q_{1-\alpha}\big], \quad t \in \mathcal{T}_2.
$$

For cluster-level effects, we compute:

$$
q_{k,1-\alpha} = \text{Quantile}_{1-\alpha}\!\big(|\hat{\tau}_{k t}^{\mathcal{B}}|\big), \quad t \in \mathcal{B},
$$

yielding the cluster-level interval:

$$
\mathrm{CI}_{k t} = \big[\hat{\tau}_{k t} - q_{k,1-\alpha}, \, \hat{\tau}_{k t} + q_{k,1-\alpha}\big], \quad k \in \mathcal{K}, \, t \in \mathcal{T}_2.
$$

These intervals provide interpretable bounds on the treatment effects, calibrated using the placebo distribution.

### Permutation Testing

To test whether the post-treatment effects are statistically significant, we perform permutation tests at both the overall and cluster levels. The observed global statistic is the average absolute effect over post-treatment periods:

$$
S_{\text{obs}} = \frac{1}{|\mathcal{T}_2|} \sum_{t \in \mathcal{T}_2} \big|\hat{\tau}_{t}\big|.
$$

We form a permutation distribution by pooling the placebo effects $\{\hat{\tau}_{t}^{\mathcal{B}} : t \in \mathcal{B}\}$ and post-treatment effects $\{\hat{\tau}_{t} : t \in \mathcal{T}_2\}$, then repeatedly sampling subsets of size $|\mathcal{T}_2|$ to compute:

$$
S_{\text{perm}}^{(b)} = \frac{1}{|\mathcal{T}_2|} \sum_{t \in \text{sample}} \big|\hat{\tau}_{t}\big|, \quad b = 1, \dots, B.
$$

The global p-value is:

$$
p_{\text{global}} = \frac{1}{B} \sum_{b=1}^B \mathbf{1}\{S_{\text{perm}}^{(b)} \ge S_{\text{obs}}\}.
$$

For cluster-level tests, we apply the same procedure using $\{\hat{\tau}_{k t}^{\mathcal{B}} : t \in \mathcal{B}\}$ and $\{\hat{\tau}_{k t} : t \in \mathcal{T}_2\}$ for each cluster $k$, yielding $p_{k,\text{global}}$. These tests assess whether the observed effects are unusually large compared to the null distribution from placebo periods.


# A Simple Example

I applied the experimental synthetic control method to the Curaçao Green Stay example, estimating treatment effects by constructing synthetic treated and control units. The analysis used two optimization formulations: Mixed Integer Quadratic Programming (MIQP) and its respective Quadratic Programming (QP) relaxation. I selected 2 to 3 treated units per cluster, with pre-treatment fit assessed via root mean squared error (RMSE). Below, I summarize results for two clustering setups (two clusters and one cluster) and different penalty settings.

For the two-cluster setup (the first two plots below), both MIQP and QP formulations produced identical outcomes, suggesting that the integer constraints in MIQP did not alter the solution, making QP a computationally efficient alternative. In Cluster 1 (non-Willemstad), the pre-treatment RMSE was 0.1961, indicating an excellent fit. The control weights were distributed across six neighborhoods—Barber (0.1922), Lagún (0.1583), Oostpunt (0.1377), Santa Rosa (0.1940), Tera Corá (0.1293), and Westpunt (0.1885)—while the treated weights were assigned to Sint Willibrordus (0.3819), Soto (0.3003), and Spaanse Water (0.3179), summing to 1. In Cluster 2, Willemstad, the RMSE was slightly higher at 0.2185. Nine neighborhoods contributed to the control weights—Brievengat (0.1107), Groot Kwartier (0.1126), Hato (0.0633), Koraal Partir (0.1013), Otrobanda (0.1024), Pietermaai (0.1486), Piscadera Bay (0.1268), Sint Michiel (0.1240), and Steenrijk (0.1103), while the treated weights went to Groot Piscadera (0.2195), Saliña (0.3298), and Scharloo (0.4508), with Scharloo having the largest influence.

```{python}

#| echo: false

from mlsynth import MAREX
import pandas as pd
from mlsynth.utils.exputils import plot_cluster_full


df = pd.read_csv("https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/Curacao_EXP_922.csv")
df_pre = df[df["time"] <= 120].copy()
#df_pre["Region"]=0

config = {"df": df,
          "unitid": "town",
          "time": "time",
          "outcome": "Y_obs",
          "T0": 128,
          "m_min": 2,
          "m_max": 3,
          "cluster": "Region",
          "program_type": "MIQP", "blank_periods": 28}

design = MAREX(config).fit()


plot_cluster_full(df,design)

```

```{python}

#| echo: false

from mlsynth import MAREX
import pandas as pd
import matplotlib.pyplot as plt
from mlsynth.utils.exputils import plot_cluster_full


df = pd.read_csv("https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/Curacao_EXP_922.csv")
df_pre = df[df["time"] <= 120].copy()
df["Region"]=0

config = {"df": df,
          "unitid": "town",
          "time": "time",
          "outcome": "Y_obs",
          "T0": 128,
          "m_min": 1,
          "m_max": 5,
          "cluster": "Region",
          "program_type": "MIQP", "blank_periods": 28}

design = MAREX(config).fit()

plot_cluster_full(df,design)
```

When all 21 neighborhoods were treated as a single cluster using MIQP with 2 to 3 treated units, the RMSE was 0.1999, comparable to the two-cluster setup and indicating excellent fit. The control weights were spread across 18 neighborhoods—Barber (0.0721), Brievengat (0.0739), Groot Kwartier (0.0435), Groot Piscadera (0.0401), Hato (0.0690), Koraal Partir (0.0439), Oostpunt (0.0279), Pietermaai (0.0559), Saliña (0.0451), Santa Rosa (0.0620), Scharloo (0.0686), Sint Michiel (0.0608), Sint Willibrordus (0.1004), Soto (0.0309), Spaanse Water (0.0671), Steenrijk (0.0647), Tera Corá (0.0603), and Westpunt (0.0137), with each contributing 1–10% to the synthetic control. The treated weights were assigned to Lagún (0.3335), Otrobanda (0.3730), and Piscadera Bay (0.2935). In a separate one-cluster setup using MIQP with a cluster-penalized design, where penalties $\lambda_1 = 0.2$ and $\lambda_2 = 0.2$ were applied to encourage proximity to the cluster mean, the RMSE increased to 1.52. The fit quality degrades slighly, but is still acceptable. The control weights were concentrated on three neighborhoods—Santa Rosa (0.0415), Scharloo (0.7368), and Tera Corá (0.2217)—with Scharloo dominating the synthetic control. The treated weights were assigned to two neighborhoods, Pietermaai (0.5161) and Piscadera Bay (0.4839), summing to 1, reflecting a sparser solution driven by the penalties.

The results show that the unpenalized MIQP and QP formulations consistently achieved excellent pre-treatment fit, with RMSEs ranging from 0.1961 to 0.2185 in the two-cluster setup and 0.1999 in the unpenalized one-cluster setup, supporting reliable counterfactual estimation. The penalized design, however, compromised fit quality, likely due to the restrictive $\lambda_1 = \lambda_2 = 0.2$ penalties, which prioritized sparsity and proximity to the cluster mean but led to a high RMSE. Ofcourse, in reality we can choose these penalties via cross validation, instead of setting them manually, these were just chosen for the sake of example.

Treated weights were balanced across 2–3 units in all setups, ensuring no single unit dominated, while control weights varied from moderate (6–19% in two-cluster) to highly diversified (1–10% in one-cluster unpenalized) to sparse (Scharloo at 0.7368 in penalized). The identical results from MIQP and QP in the two-cluster setup suggest that the QP relaxation is a viable alternative for computational efficiency. Also, these data were simualted [from Wikipedia](https://en.wikipedia.org/wiki/List_of_populated_places_in_Cura%C3%A7ao), in theory [we could get both population and density data](https://www.citypopulation.de/en/curacao/admin/) on a wider variety of places in Curaçao, with a more sophisitcated hierarchical linear factor model, to see how the estimator would play with even more clusters/units.

## Takeaways for Practice

What this shows is that the synthetic design is robust: whether we divide Curaçao into clusters or treat it as a single unit, whether we enforce integer treatment assignments or relax them, the method generally produces stable synthetic matches. Notice that we have not yet discussed the ATT/ATE or the uncertainty of estimated treatment effects. This is because the framework above is focused on experimental design. The main goal is to decide *who should be treated* and *why* before any real-world intervention. In other words, we can be confident in the design itself—who is treated, who is not, and why—before estimating treatment effects or reporting uncertainty.

Contrast this with a traditional RCT. In an RCT, randomization occurs once in the field. Designing an RCT is costly and requires coordination across multiple actors. We cannot experiment with different randomization schemes beforehand, since the quality of the randomization can only be assessed *after* it occurs. Once units are assigned, the study must proceed, with all associated costs, risks, and constraints.

By contrast, the synthetic control design can be iterated rapidly on a laptop. We can explore different cluster groupings, penalties, cost constraints, or solver formulations and observe how stable the resulting design is before committing any resources. Crucially, in settings with few treatable units, synthetic control designs can achieve balance analogous to an RCT, even when only a handful of units are treated. This allows interventions to be targeted strategically, saving resources while maintaining credible comparisons.

This flexibility makes the synthetic experimentation method a powerful planning tool. It enables researchers and practitioners to prototype experiments cheaply and safely, ensuring that the data support a credible design before any real-world action. Large-scale RCTs remain the gold standard, but they are expensive and logistically demanding. Synthetic design exercises open the door to an iterative, exploratory phase of experimental planning—helping ensure that when we do invest in a full study, it is targeted, efficient, and built on a solid foundation.
