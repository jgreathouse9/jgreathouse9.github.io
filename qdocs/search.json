[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nArtificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nJan 24, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nClustered Synthetic Controls: ClusterSC from mlsynth\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nFeb 3, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scdense.html",
    "href": "scdense.html",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "",
    "text": "Plenty of posts have been done in the last decade on the synthetic control method and related approaches. It is a flavor of artificial counterfactual estimator for the panel data setting. Folks from Microsoft, Databricks, Uber, Amazon, Netflix, Gainwell Technologies, and elsewhere have written about/covered it, detailing its implementation, use cases, and econometric theory.\nMany (not all) of these cover the standard SCM, developed originally to study terrorism in the Basque Country and conduct comparative case studies more broadly. Standard SCM tends to favor, under certain technical conditions, a sparse set of control units being assigned weights. These weights aim to reconstruct the factor loadings/observed values of the treated unit, pre-intervention. Sparsity, or the true coefficient vector being mostly 0, has appealing properties; for example, a sparse vector allows us to interpret the synthetic control a lot easier, facilitating the estimation of leave-one-out placebos and other sensitivity checks. In some cases though, the sparsity notion is unfounded.\nThe the \\(\\ell_2\\) panel data approach is an econometric methodology developed by Zhentao Shi and Yishu Wang. The \\(\\ell_2\\)-PDA is a form of synthetic control estimation that accommodates sparse or dense data generation processes. A dense DGP is when the true vector of coefficients is mostly not zero. Sometimes, a sparse model is too simplistic, especially in settings where we have very many predictors. This is especially true when we have lots of multicollinearity among our predictors, which may be very plausible in settings with a lot of control units. The LASSO and the convex hull SCM (for different reasons and in different cases) generally struggle with this, whereas the Ridge or \\(\\ell_2\\)-PDA accomodate multicollinearity as a feature. In this post, I demonstrate the method as implemented in my library mlsynth. The Python code for these results may be found here."
  },
  {
    "objectID": "scdense.html#ell_2-relaxation",
    "href": "scdense.html#ell_2-relaxation",
    "title": "Synthetic Controls in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "\\(\\ell_2\\) Relaxation",
    "text": "\\(\\ell_2\\) Relaxation\nThe \\(\\ell_2\\) relaxation estimates the model \\(\\mathbf{y}_1 = \\boldsymbol{\\alpha}+ \\mathbf{Y}_0\\mathbf{w}+\\mathbf{\\epsilon}\\), where \\(\\boldsymbol{\\alpha}\\) is some unconstrained intercept we estimate after solving for the weights. The main innovation with \\(\\ell_2\\) relaxation is the way Zhentao and Yishu propose to estimate the weights, which I think is pretty ingenious. They essentially advocate to literally exploit the pre-treatment relationships between the treated unit and donor pool, and the relationships between the donors themselves. Here is the optimization:\n\\[\\begin{split}\n\\begin{aligned}\n&\\min_{\\mathbf{w}} \\frac{1}{2} \\|\\mathbf{w}\\|_2^{2}, \\quad \\forall t \\in \\mathcal{T}_{1}, \\\\\n&\\text{subject to } \\|\\mathbf{\\eta} - \\mathbf{\\Sigma} \\mathbf{w}\\|_\\infty \\leq \\tau\n\\end{aligned}\n\\end{split}\\]\nUnderneath the scary matrix algebra, we have two terms to focus on here: eta (\\(\\boldsymbol{\\eta} \\in \\mathbb{R}^{N_0 \\times 1}=\\frac{1}{T_1} \\mathbf{Y}_0^\\top \\mathbf{y}_1\\)) and sigma (\\(\\boldsymbol{\\Sigma}\\in \\mathbb{R}^{N_0 \\times N_0} = \\frac{1}{T_1} \\mathbf{Y}_0^\\top \\mathbf{Y}_0\\)). The first term simply is a projection which measures the relation between treated unit and each of the control units. The greater the absolute value of the number, the more/less similar the donor is to the treated unit. The latter is the same, except for the control group, capturing how similar the control units are to each other. The optimization itself just finds the values of beta that bound the infinity norm (or, the maximum absolute value) of the difference between these two, such that the difference is below some constant, tau \\(\\tau\\). As for the weights themselves, they are shrank towards zero, but never actually equal 0. The weights may be any real number.\nCritical to this process is the selection of tau. In the original paper, the authors do a 70-30 train test split over the pre-intervention period, where they take the first 70% of the pre-treatment data and supply some values of tau. Then, the next step is to estimate the regression model run the optimization and calculate out-of-sample validation predictions up until time point \\(T_0\\). We then check the RMSE of the validation error across all these values of tau and select the value of with the lowest validation RMSE. A lower tau corresponds to an estimator closer to pure least squares, and too high a value of tau results in drastic underfitting (trust me). The way mlsynth does this, however, is by splitting the pre-treatment period in half, but the performance is pretty close to what Zhentao and Yishu do for their empirical example (see the code at the documentation).\n\\(\\ell_2\\) relaxation offers a counterfactual prediction for dense settings. Oftentimes, a sparse model is too simplistic, especially in settings where we have very many predictors. This is especially true when we have lots of multicollinearity among our predictors (our donor pool in this case), which may be very plausible in settings with a lot of control units. The LASSO and the convex hull SCM generally struggle with this, whereas the Ridge or \\(\\ell_2\\)-relaxer accomodate multicollinearity as a feature. Okay, now with that out of the way, let’s apply this to two real-life empirical examples, shall we?"
  },
  {
    "objectID": "scdense.html#plotting-our-data",
    "href": "scdense.html#plotting-our-data",
    "title": "Synthetic Controls in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "Plotting Our Data",
    "text": "Plotting Our Data\n\nNote:\nThese data are freely available from my Github and can be viewed at a dashboard on Streamlit. They were scraped together using Python- the code for this scrape is private though. Someday, I may do a blog post employing a simple example of how I scraped it together, since public policy scholars can take advantage of scraping for their own purposes.\n\nBefore we get into the fancy estimation though, let’s just plot our data. These daily data come from Songstats, a platform which keeps data on Spotify and Apple (and lots of other sources) music/interaction trends. For example, here is Tyla’s Apple performance and the same for Spotify. For Spotify, the metric is “Playlist Reach” which Songstats defines as\n\nThe combined follower number of all Spotify playlists with 200+ followers your music is currently on.\n\nI keep the data from Janaury 1 2023 to June 1 2024. There are 59 donors in the Spotify data, after we drop artists with unbalanced panel/missing data. For Apple, our metric is “Playlists”, which Songstats defines as\n\nThe total number of distinct playlists your music has ever been on.\n\nThese data are collected from January 1 of 2022 to June 1 2024. Also, not all artists had complete data, so I kept only the artist who were fully observed over the course of the time series. This means there are 24 donors for Apple, yet this could easily change assuming I began the training period with the same time point as the Spotify data.\n\n\n\n\n\n\n\n\n\nI normalized all the metrics to 100 at August 17, 2023, a few days before the treatment occured (Tyla performing in Kigali, Rwanda). Zhentao and Yishu recommend mean-standardization and then backing out the coefficients in the paper, but I haven’t programmed this for the \\(\\ell_2\\) PDA just yet. Either way, I think it helps us to see what’s going on in better detail when we normalize the outcome trends as we see here, since now we’re practically comparing everybody in terms of relative trends. The plot says that Tyla generally had less relative popularity compared to the donors for the majority of the time seies pre treatment. We can see both her metrics balloon in the post-intervention period. We can also see quite clearly that parallel trends with respect to the donor pool likely does not hold; the trend of the average of controls is quite flat relatve to Tyla’s.\n\nSongstats Spotify\nI begin with the Spotify data. When we estimate the \\(\\ell_2\\) relaxer, comparing Tyla to her 59 controls, we get the following plot (note the red line is the counterfactual Tyla, or how her Playlist Reach would look absent her going viral) :\n\n\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/cvxpy/problems/problem.py:1481: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n|    | Metric              | Value                                                            |\n|---:|:--------------------|:-----------------------------------------------------------------|\n|  0 | ATT                 | 1325.835                                                         |\n|  1 | Standard Error      | 76.59708007821426                                                |\n|  2 | t-stat              | 294.25658232644037                                               |\n|  3 | Confidence Interval | (np.float64(1175.0739327938988), np.float64(1476.5960672061012)) |\n|  4 | RMSE (T0)           | 5.226                                                            |\n|  5 | R-Squared           | 0.978                                                            |\n\n\n&lt;Figure size 1000x550 with 0 Axes&gt;\n\n\n\n\nSongstats Apple\nHere is the same plot with the Apple data, where the outcome is Playlist Count:\n\n\n\n\n\n\n\n\n\n|    | Metric              | Value                                                           |\n|---:|:--------------------|:----------------------------------------------------------------|\n|  0 | ATT                 | 412.589                                                         |\n|  1 | Standard Error      | 22.423240653018407                                              |\n|  2 | t-stat              | 312.80104015901196                                              |\n|  3 | Confidence Interval | (np.float64(368.4547890403085), np.float64(456.72321095969147)) |\n|  4 | RMSE (T0)           | 4.343                                                           |\n|  5 | R-Squared           | 0.982                                                           |\n\n\n&lt;Figure size 1000x550 with 0 Axes&gt;\n\n\nThis is unsurprising, and the practical results speak for themselves. Going viral clearly contributed a lot to Tyla’s success across both platforms, as the estimated counterfactuals spell out very plainly that her reach would be a lot flatter compared to what it in fact was/is. As we know from the above, every artist in the donor pool contributes to Tyla’s synthetic control, and the \\(\\ell_2\\) relaxer will not run ito problems of multicollinearity as LASSO might in higher dimensions. This method can also be used with multiple treated units, but I haven’t written this just yet."
  },
  {
    "objectID": "scdense.html#a-review-of-synthetic-controls",
    "href": "scdense.html#a-review-of-synthetic-controls",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "A Review of Synthetic Controls",
    "text": "A Review of Synthetic Controls\nLet \\(\\mathbb{R}\\) denote the set of real numbers. Denote the natural numbers and unit interval respectively as \\[\n\\mathbb{N} \\coloneqq \\{1, 2, 3, \\dots\\}, \\quad \\mathbb{I} \\coloneqq \\{ w \\in \\mathbb{R} : 0 \\leq w \\leq 1 \\}.\n\\] Let a caligraphic letter, say \\(\\mathcal A\\), denote a descrete set whose cardinality is \\(A=|\\mathcal{A}|\\). The sup norm of a vector \\(\\mathbf{y} \\in \\mathbb{R}^N\\) is defined as the maximum absolute value of its components, \\(\\|\\mathbf{y}\\|_\\infty = \\max_{j = 1, \\ldots, N} |y_j|\\). The floor function of a real number \\(x \\in \\mathbb{R}\\), denoted as \\(\\lfloor x \\rfloor\\), returns \\(\\lfloor x \\rfloor = \\max \\{k \\in \\mathbb{N} : k \\leq x\\}\\). Let \\(t \\in \\mathbb{N}\\) and \\(i \\in \\mathbb{N}\\), represent the indices for \\(T\\) time periods and \\(N\\) units. The pre-treatment period consists of consecutive time periods \\(\\mathcal{T}_1 = \\{1, 2, \\ldots, T_0\\}\\) (cardinality \\(T_1\\)), while the post-treatment period is given by \\(\\mathcal{T}_2 = \\{T_0 + 1, \\ldots, T\\}\\) (cardinality \\(T_2\\)). The treated unit is indexed by \\(i = 1\\), while the remaining set of units, \\(\\mathcal{N}_0 \\coloneqq \\{2, \\ldots, N_0 + 1\\}\\) (cardinality \\(N_0\\)), forms the control group. Each outcome for all units is denoted by \\(y_{it}\\). Denote the outcome vector for the treated unit as \\(\\mathbf{y}_1 \\coloneqq \\begin{bmatrix} y_{11} & y_{12} & \\cdots & y_{1T} \\end{bmatrix}^\\top \\in \\mathbb{R}^T\\), where each entry corresponds to the outcome of the treated unit at time \\(t\\). The donor pool matrix, similarly, is defined as\n\\[\n\\mathbf{Y}_0 = \\begin{bmatrix}\n    y_{21} & y_{22} & \\cdots & y_{2T} \\\\\n    y_{31} & y_{32} & \\cdots & y_{3T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    y_{(N_0+1)1} & y_{(N_0+1)2} & \\cdots & y_{(N_0+1)T}\n  \\end{bmatrix} \\in \\mathbb{R}^{N_0 \\times T}.\n\\]\nThe key challenge of causal inference is that we only observe the factual/realized outcome, expressed as:\n\\[\ny_{it} = y_{it}^1 d_{it} + \\left(1 - d_{it}\\right)y_{it}^0.\n\\] Outcomes in this framework are a function of treatment status, \\(d_{it} \\in \\{0, 1\\}\\). While \\(d_{it} = 1\\), a unit is treated and while \\(d_{it} = 0\\) a unit is untreated. Thus, \\(y_{it}^1\\) is the potential outcome under treatment, \\(y_{it}^0\\) is the counterfactual (or potential) outcome under no treatment. The objective is to estimate the counterfactual outcomes \\(y_{1t}^0\\). SCMs and panel data approaches are weighting based estimators. In this setup, some weight vector \\[\n\\mathbf{w} \\coloneqq \\begin{bmatrix} w_2 & w_3 & \\cdots & w_{N_0+1} \\end{bmatrix}^\\top, \\: \\text{where } w_i \\: \\forall \\: i \\in \\mathcal{N}_0.\n\\] is assigned across the \\(N_0\\) control units to approximate the treated unit’s pre-intervention outcome. SCM solves for \\(\\mathbf{w}\\) via the optimization:\n\\[\n\\underset{\\mathbf{w} \\in \\mathbb{I}^{N_0}}{\\operatorname*{argmin}} \\lVert \\mathbf{y}_1 - \\mathbf{w}^\\top \\mathbf{Y}_0 \\rVert_2^2,  \\: \\forall \\: t \\in \\mathcal{T}_{1}, \\: \\text{subject to } \\lVert \\mathbf{w} \\rVert_1 = 1.\n\\]\nHere, \\(\\mathbf{w}^\\top \\mathbf{Y}_0\\) represents the dot product of predictions, also known as a weighted average. The constraint of the convex hull disallows extrapolation of any form. SCMs are predicated on good pretreatment fit, such that \\(\\mathbf{w}^\\top \\mathbf{Y}_0 \\approx \\mathbf{y}_1\\). Speaking generally, the better pre-treatment fit we have over a long time series, the better the out-of-sample predictions will be for the synthetic control."
  },
  {
    "objectID": "scdense.html#implementing-ell_2-relaxation-in-mlsynth.",
    "href": "scdense.html#implementing-ell_2-relaxation-in-mlsynth.",
    "title": "Synthetic Controls in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "Implementing \\(\\ell_2\\) relaxation in mlsynth.",
    "text": "Implementing \\(\\ell_2\\) relaxation in mlsynth.\nIn order to get these results, you need Python (3.8 or greater) and mlsynth, which you may install from the Github repo.\n$ pip install -U git+https://github.com/jgreathouse9/mlsynth.git\nI taught Policy Data Analysis last semester at Georgia State. It was an undergrad course, but I still wanted to cover a little bit of causal infernece, not because they will all go on to be grad students or policy researchers, but because the causal mindset is such an important part of how we think about and interact with the world. We managed to get through Difference-in-Differences, after we covered regression and basic stats. The very final topic we covered on the very last day of class was SCM. I prepared for class as I usually do, by playing music. The lyics were in Zulu, so I did not expect anybody who was not South African or a fan of Tyla to know the song, which they did not when I asked them. I then played Tyla’s song “Water”, the one that went viral in 2023, which everybody knew about when I asked.\nI used this to transition into the lecture for the day. “Oftentimes,” I began, “we are concerned about the impact of specific events or interventions. As we’ve discussed, the DID method does this on a parallel trends assumption, or that absent the intevention, the trends of the treated group would, on average, move the same as the control group. But what do we do if this breaks down? What if we cannot get parallel trends to hold, for some reason? All semester, we’ve discussed academic examples, but I want to show you how this may look in a semi-real life setting. So the question we wish to answer here is”How would Tyla’s musical success have looked absent her going viral?” Maybe she would’ve made it big anyways, but how can we tell?”\n\nPlotting Our Data\n\nNote:\nThese data are freely available from my Github and can be viewed at a dashboard on Streamlit. They were scraped together using Python- the code for this scrape is private though. Someday, I may do a blog post employing a simple example of how I scraped it together, since public policy scholars can take advantage of scraping for their own purposes.\n\nBefore we get into the fancy estimation though, let’s just plot our data. These daily data come from Songstats, a platform which keeps data on Spotify and Apple (and lots of other sources) music/interaction trends. For example, here is Tyla’s Apple performance and the same for Spotify. For Spotify, the metric is “Playlist Reach” which Songstats defines as\n\nThe combined follower number of all Spotify playlists with 200+ followers your music is currently on.\n\nI keep the data from Janaury 1 2023 to June 1 2024. There are 59 donors in the Spotify data, after we drop artists with unbalanced panel/missing data. For Apple, our metric is “Playlists”, which Songstats defines as\n\nThe total number of distinct playlists your music has ever been on.\n\nThese data are collected from January 1 of 2022 to June 1 2024. Also, not all artists had complete data, so I kept only the artist who were fully observed over the course of the time series. This means there are 24 donors for Apple, yet this could easily change assuming I began the training period with the same time point as the Spotify data.\n\n\n\n\n\n\n\n\n\nI normalized all the metrics to 100 at August 17, 2023, a few days before the treatment occured (Tyla performing in Kigali, Rwanda). Zhentao and Yishu recommend mean-standardization and then backing out the coefficients in the paper, but I haven’t programmed this for the \\(\\ell_2\\) PDA just yet. Either way, I think it helps us to see what’s going on in better detail when we normalize the outcome trends as we see here, since now we’re practically comparing everybody in terms of relative trends. The plot says that Tyla generally had less relative popularity compared to the donors for the majority of the time seies pre treatment. We can see both her metrics balloon in the post-intervention period. We can also see quite clearly that parallel trends with respect to the donor pool likely does not hold; the trend of the average of controls is quite flat relatve to Tyla’s.\n\n\nSongstats Spotify\nI begin with the Spotify data. When we estimate the \\(\\ell_2\\) relaxer, comparing Tyla to her 59 controls, we get the following plot (note the red line is the counterfactual Tyla, or how her Playlist Reach would look absent her going viral) :\n\n\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/cvxpy/problems/problem.py:1481: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n|    | Metric              | Value                                                            |\n|---:|:--------------------|:-----------------------------------------------------------------|\n|  0 | ATT                 | 1325.835                                                         |\n|  1 | Standard Error      | 76.59708007821426                                                |\n|  2 | t-stat              | 294.25658232644037                                               |\n|  3 | Confidence Interval | (np.float64(1175.0739327938988), np.float64(1476.5960672061012)) |\n|  4 | RMSE (T0)           | 5.226                                                            |\n|  5 | R-Squared           | 0.978                                                            |\n\n\n&lt;Figure size 1000x550 with 0 Axes&gt;\n\n\n\n\nSongstats Apple\nHere is the same plot with the Apple data, where the outcome is Playlist Count:\n\n\n\n\n\n\n\n\n\n|    | Metric              | Value                                                           |\n|---:|:--------------------|:----------------------------------------------------------------|\n|  0 | ATT                 | 412.589                                                         |\n|  1 | Standard Error      | 22.423240653018407                                              |\n|  2 | t-stat              | 312.80104015901196                                              |\n|  3 | Confidence Interval | (np.float64(368.4547890403085), np.float64(456.72321095969147)) |\n|  4 | RMSE (T0)           | 4.343                                                           |\n|  5 | R-Squared           | 0.982                                                           |\n\n\n&lt;Figure size 1000x550 with 0 Axes&gt;\n\n\nThis is unsurprising, and the practical results speak for themselves. Going viral clearly contributed a lot to Tyla’s success across both platforms, as the estimated counterfactuals spell out very plainly that her reach would be a lot flatter compared to what it in fact was/is. This method can also be used with multiple treated units, as we see in the paper, but I haven’t written this for mlsynth just yet."
  },
  {
    "objectID": "scdense.html#selecting-tau",
    "href": "scdense.html#selecting-tau",
    "title": "Artificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer",
    "section": "Selecting \\(\\tau\\)",
    "text": "Selecting \\(\\tau\\)\nCritical to \\(\\ell_2\\)-PDA is the selection of tau. The reason for this is because while \\(\\tau \\to 0\\), the estimator converges to OLS, which we know will overfit to the pre-intervention data in most cases unless the data are exactly low-rank and fully observed. Too high a value of \\(\\tau\\) results in drastic underfitting (trust me) of the pre-treatment data. The way mlsynth tunes tau is by cross-validation over the pre-treatment period. mlsynth invokes a log-space over the interval \\(\\tau \\in \\left[ 10^{-4}, \\tau_{\\text{init}} \\right]\\), where \\(\\tau_{\\text{init}} = \\|\\boldsymbol{\\eta}\\|_\\infty\\). These correspond to the values of tau we supply. We first create a training and validation period, \\(\\mathcal{T}_1 = \\mathcal{T}_1^{\\text{train}} \\cup \\mathcal{T}_1^{\\text{val}}, \\quad\n\\mathcal{T}_1^{\\text{train}} \\cap \\mathcal{T}_1^{\\text{val}} = \\emptyset.\\) Denote the training series as \\(\\mathcal{T}_1^{\\text{train}} = \\{1, 2, \\ldots, k\\}\\). Denote the validation series as \\(\\mathcal{T}_1^{\\text{val}} = \\{k+1, \\ldots, T_0\\}\\), where \\(k = \\left\\lfloor \\frac{T_1}{2} \\right\\rfloor\\). We then estimate the model for the training period, and compute our predictions from \\(\\{k+1, \\ldots, T_0\\}\\). The tau we select is\n\\[\\begin{aligned}\n\\tau^{\\ast} = \\operatorname*{argmin}_{\\tau} \\left( \\frac{1}{|\\mathcal{T}_1^{\\text{val}}|} \\| \\mathbf{y}^{\\ell_2} - \\mathbf{y}_1 \\|_2^2 \\right),\n\\end{aligned}\\]\nor the one that minimizes the validation Root Mean Squared Error. The performance is pretty close to what Zhentao and Yishu do for their empirical example (see the code at the documentation). In the original paper, they do a 70-30 train test split over the pre-intervention period, where they take the first 70% of the pre-treatment data and supply some values of tau.\nOkay hard part done. Let’s apply this to a real-life example, shall we?"
  },
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, lots of the datasets we tend to work with come in pretty csv files. And while that’s great… oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we’d need to ask AAA and pay thousands of dollars for an extended time series… but now we don’t need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is then structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA’s website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is “https://gasprices.aaa.com/?state=MA”. For Florida, the URL is “https://gasprices.aaa.com/?state=FL”. See the pattern? There’s a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python’s requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we’ve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year’s worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I’ve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data or some such thing. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we’re scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it’s just “scrape”, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that’s the language I use, but I’m certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don’t add them and then the job ends (this is what happens if I try to run the action after it’s ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I’m updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I’m scraping (unless you’re a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I’m home to personally oversee it. The value here is that I’ve manually gotten my computer to do a specific task every single day, the correct way (assuming you’ve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA’s site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it’s worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you’re a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we’ve automated our tasks correctly."
  },
  {
    "objectID": "clustsc.html",
    "href": "clustsc.html",
    "title": "Clustered Synthetic Controls: ClusterSC from mlsynth",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom mlsynth.mlsynth import dataprep\nfrom mlsynth.mlsynth import FDID, CLUSTERSC, PDA\nimport matplotlib\nimport numpy as np\n# Set up theme for Matplotlib\ndef set_theme():\n    theme = {\n        \"axes.grid\": True,\n        \"grid.linestyle\": \"-\",\n        \"grid.color\": \"black\",\n        \"legend.framealpha\": 1,\n        \"legend.facecolor\": \"white\",\n        \"legend.shadow\": True,\n        \"legend.fontsize\": 12,\n        \"legend.title_fontsize\": 12,\n        \"xtick.labelsize\": 10,\n        \"ytick.labelsize\": 10,\n        \"axes.labelsize\": 10,\n        \"axes.titlesize\": 12,\n        \"figure.dpi\": 100,\n        \"axes.facecolor\": \"white\",\n        \"figure.figsize\": (10, 6),\n    }\n    matplotlib.rcParams.update(theme)\n\ndef plot_donors_and_treated(donor_matrix, treated_vector, pre_periods, title, ax):\n    for i in range(donor_matrix.shape[1]):\n        ax.plot(donor_matrix[:, i], color='gray', linewidth=0.5, alpha=0.8, label='_nolegend_')\n    ax.plot(treated_vector, color='black', linewidth=2, label='Treated Unit')\n    average_controls = donor_matrix.mean(axis=1)\n    ax.plot(average_controls, color='red', linewidth=2, label='Normalized Average of Controls')\n    ax.axvline(x=pre_periods, color='blue', linestyle='--', linewidth=1.5, label='Water')\n    ax.set_title(title)\n    ax.set_xlabel('Time Periods')\n\ndef clean_cargo_data(url, reference_date=\"March 2017\"):\n    # Load the dataset\n    df = pd.read_csv(url)\n\n    # Identify airports where column 3 ever has a \"-\"\n    airports_with_dash = df.loc[df.iloc[:, 2] == \"-\", df.columns[0]].unique()\n\n    # Drop all rows where the airport is in the list\n    df_cleaned = df[~df.iloc[:, 0].isin(airports_with_dash)].copy()\n\n    # Convert column 3 values based on suffix\n    def convert_values(value):\n        if isinstance(value, str):\n            value = value.replace(\",\", \"\").strip()  # Remove commas & spaces\n\n            if value.endswith(\"k\"):\n                return float(value[:-1]) * 1_000\n            elif value.endswith(\"m\"):\n                return float(value[:-1]) * 1_000_000\n            elif value.replace(\".\", \"\").isdigit():  # Handle numbers safely\n                return float(value)\n\n        return value  # Return as-is if conversion fails\n\n    # Apply transformation to column 3\n    df_cleaned.iloc[:, 2] = df_cleaned.iloc[:, 2].astype(str).apply(convert_values)\n\n    df_cleaned.iloc[:, 1] = df_cleaned.iloc[:, 1].str.strip()\n\n    # Convert column 2 (dates) to real datetime format\n    df_cleaned.iloc[:, 1] = pd.to_datetime(df_cleaned.iloc[:, 1], format=\"%B %Y\",  errors='coerce')  # Let pandas infer format\n\n    # Normalize values based on March 2017\n    reference_date_dt = pd.to_datetime(reference_date)\n    reference_values = {}\n\n    for airport in df_cleaned.iloc[:, 0].unique():\n        airport_data = df_cleaned[df_cleaned.iloc[:, 0] == airport]\n        march_2017_data = airport_data[airport_data.iloc[:, 1] == reference_date_dt]\n\n        if not march_2017_data.empty:\n            # Get the value for March 2017\n            reference_value = march_2017_data.iloc[0, 2]  # Value in column 3\n            reference_values[airport] = reference_value\n\n    # Normalize the values\n    def normalize_by_reference(row):\n        airport = row.iloc[0]  # Column 1 has airport name\n        if airport in reference_values:\n            reference_value = reference_values[airport]\n            row.iloc[2] = (row.iloc[2] / reference_value)*100  # Normalize to 100 at March 2017\n        return row\n\n    df_normalized = df_cleaned.apply(normalize_by_reference, axis=1)\n    df_normalized = df_normalized.groupby(df_normalized.columns[0]).filter(lambda x: len(x) == 212)\n\n    return df_normalized\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/jgreathouse9/GSUmetricspolicy/refs/heads/main/data/RawData/cargo_data_parallel.csv\"\n\n# Call the function to clean the data\ndf = clean_cargo_data(url)\n\ndf.rename(columns={'freight_mail_value': 'Weight'}, inplace=True)\n\n# Add the 'treated' column: 1 for CVG from April 2017, else 0\ndf['Amazon Air'] = ((df.iloc[:, 0] == \"Cincinnati, OH: Cincinnati/Northern Kentucky International (CVG)\") & (df.iloc[:, 1] &gt;= pd.to_datetime(\"April 2017\"))).astype(int)\ndf.iloc[:, 0] = df.iloc[:, 0].replace(\"Cincinnati, OH: Cincinnati/Northern Kentucky International (CVG)\", \"Connecticut Airport\")\n\ndf['Time'] = df.groupby(df.columns[0]).cumcount() + 1\ndf = df[df['Time'] &gt;= 38]\ndf['Time'] = df['Time']-37\ndf = df[df['Time'] &lt; 109]\n\nset_theme()\n\ndf.rename(columns={'month_year': 'Date'}, inplace=True)\n\n\n\nprepped = dataprep(df, 'airport', 'Date', 'Weight', 'Amazon Air')\n\nprepped[\"donor_matrix\"] = prepped[\"donor_matrix\"][:, ~np.isnan(prepped[\"donor_matrix\"]).any(axis=0)]\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Plot each donor as a thin grey line (columns are different units)\nfor donor in prepped[\"donor_matrix\"].T:  # .values.T to iterate over columns (units)\n    plt.plot(range(1, len(prepped[\"y\"]) + 1), donor, color='grey', linewidth=0.5)\n\n# Plot the treated vector (y) in blue\nplt.plot(range(1, len(prepped[\"y\"]) + 1), prepped[\"y\"], color='blue', linewidth=2, label='Treated')\n\n# Formatting\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.title('Treated vs Donor Matrix')\nplt.grid(True)\n# Limit y-axis between 0 and 200\nplt.ylim(0, 400)\n\n# Remove the legend (as requested)\nplt.legend().set_visible(False)\n\n# Show plot\nplt.show()\n\n\namazonconfig = {\n    \"df\": df,\n    \"treat\": \"Amazon Air\",\n    \"time\": \"Date\",\n    \"outcome\": 'Weight',\n    \"unitid\": \"airport\",\n    \"counterfactual_color\": \"red\",\n    \"treated_color\": \"black\",\n    \"display_graphs\": True,\n    \"cluster\": False,\n    \"Frequentist\": \"False\"\n}\n\nhub_model = CLUSTERSC(amazonconfig)\nARCO_results = hub_model.fit()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nArtificial Counterfactuals in Dense Settings: the \\(\\ell_2\\) relaxer\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nJan 24, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPosts\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  }
]