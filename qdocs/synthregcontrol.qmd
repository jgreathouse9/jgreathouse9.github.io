---
title: 'A Novel Two Stage Synthetic Control Method'
date: 2025-04-06
categories: [Causal Inference, Econometrics]
---

This blog post covers [the synthetic regression control method](https://arxiv.org/pdf/2306.02584). It is a new SCM that was once only available in ```R```. The paper itself is so notation dense that it's honestly hard for me to get through, and only after reading it and its notations many times did I understand how the estimator was supposed to work.

# Notation

Let $\mathbb{R}$ denote the set of real numbers. A calligraphic letter, such as $\mathcal{S}$, represents a discrete set with cardinality $S = |\mathcal{S}|$. Let $\mathbf{C}_T \in \mathbb{R}^{T \times T}$ represent the centering matrix defined as:

$$
\mathbf{C}_T \coloneqq \mathbf{I}_T - \frac{1}{T} \mathbf{1}_T \mathbf{1}_T^\top
$$

Where $\mathbf{I}_T$ is the identity matrix of size $T$, and $\mathbf{1}_T \in \mathbb{R}^T$ is the vector of ones of length $T$. For any vector $\mathbf{x} \in \mathbb{R}^T$, the demeaned version is:

$$
\widetilde{\mathbf{x}} \coloneqq \mathbf{C}_T \mathbf{x}
$$ 

For any pair of vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{N_0}$, we define the Hadamard (elementwise) product as $(\mathbf{a} \odot \mathbf{b})_j \coloneqq a_j b_j \quad\forall j \in \mathcal{N}_0$. Depending on orientation, there are two dot products we work with. For column vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{N_0}$,
  $$
  \mathbf{a}^\top \mathbf{b} \coloneqq \sum_{j \in \mathcal{N}_0} a_j b_j \in \mathbb{R}
  $$

This is equivalent to viewing $\mathbf{a}^\top \in \mathbb{R}^{1 \times N_0}$ as a row vector and multiplying it by the column vector $\mathbf{b} \in \mathbb{R}^{N_0}$. The result is a scalar:

  $$
  \mathbf{a}^\top \mathbf{b} \in \mathbb{R}
  $$

More generally, when multiplying a matrix $\mathbf{A} \in \mathbb{R}^{T \times N_0}$ by a vector $\mathbf{b} \in \mathbb{R}^{N_0}$, the result is a new vector in $\mathbb{R}^T$, formed as a linear combination of the columns of $\mathbf{A}$:
$$
\mathbf{A} \mathbf{b} = \sum_{j \in \mathcal{N}_0} b_j \mathbf{A}_{:,j} \in \mathbb{R}^T
$$

Here, each entry in the resulting vector is a dot product between a row of $\mathbf{A}$ and the vector $\mathbf{b}$. Let $j \in \mathbb{N}$ represent indices for a total of $N$ units and $t \in \mathbb{N}$ index time. Let $j = 1$ be the treated unit, with the set of controls being $\mathcal{N}_0 = \mathcal{N} \setminus \{1\}$, with cardinality $N_0$. The pre-treatment period consists of the set $\mathcal{T}_1 = \{ t \in \mathbb{N} : t \leq T_0 \}$, where $T_0$ is the final period before treatment. Similarly, the post-treatment period is given by $\mathcal{T}_2 = \{ t \in \mathbb{N} : t > T_0 \}$.

The observed outcome for unit $j$ at time $t$ is $y_{jt}$, where a generic outcome vector for a given unit in the dataset is $\mathbf{y}_j \in \mathbb{R}^T$, with $\mathbf{y}_j = (y_{j1}, y_{j2}, \dots, y_{jT})^\top \in \mathbb{R}^{T}$. The outcome vector for the treated unit specifically is $\mathbf{y}_1$. The donor matrix is defined as $\mathbf{Y}_0 \coloneqq \begin{bmatrix} \mathbf{y}_j \end{bmatrix}_{j \in \mathcal{N}_0} \in \mathbb{R}^{T \times N_0}$, where each column indexes a donor unit and each row is indexed to a time period. We denote by $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{T_0}$ the subvector of outcomes for unit $j$ in the pre-treatment period, and by $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{T_1}$ the corresponding post-treatment vector, where $T_1 = T - T_0$. Then, we define our pre and post intervention analogs for the data:

$$
\mathbf{y}_1^{\text{pre}} = (y_{1t})_{t \in \mathcal{T}_1} \in \mathbb{R}^{T_0}, \quad \mathbf{y}_1^{\text{post}} = (y_{1t})_{t \in \mathcal{T}_2} \in \mathbb{R}^{T_1}
$$

$$
\mathbf{Y}_0^{\text{pre}} = \left[ \mathbf{y}_j^{\text{pre}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_0 \times N_0}, \quad \mathbf{Y}_0^{\text{post}} = \left[ \mathbf{y}_j^{\text{post}} \right]_{j \in \mathcal{N}_0} \in \mathbb{R}^{T_1 \times N_0}
$$

## Synthetic Controls

Before I get to the SRC, I review basic synthetic controls. Our main goal is to estimate the counterfactual outcomes in the post-treatment period (i.e., for $t > T_0$). We define the synthetic control as a weighted average of the donor units. The goal is to find the weight vector $\mathbf{w}$ that best approximates the outcome vector of the treated unit during the pre-treatment period. We minimize

$$
\underset{\mathbf{w} \in \mathcal{W}}{\operatorname*{argmin}} \left\| \mathbf{y}_{1,\text{pre}} - \mathbf{Y}_{0,\text{pre}} \mathbf{w} \right\|_2^2
$$

where $\mathbf{y}_{1,\text{pre}} \in \mathbb{R}^{T_0}$ is the pre-treatment outcome vector for the treated unit and $\mathbf{Y}_{0,\text{pre}} \in \mathbb{R}^{T_0 \times N_0}$ is the pre-treatment outcome matrix for the donor units. The entries for the weight vector live on the simplex:

$$
\mathcal{W} = \left\{ \mathbf{w} \in \mathbb{R}^{N_0} : \mathbf{w} \geq 0, \|\mathbf{w}\|_1 = 1 \right\}
$$

Once the weights $\mathbf{w}$ are estimated, the counterfactual outcome for the treated unit during the post-treatment period is given by

$$
\mathbf{y}^{\text{SC}}_1 = \mathbf{Y}_0 \mathbf{w}, \quad \text{for } t > T_0
$$

Thus, the counterfactual outcome for each time period in the post-treatment period is the weighted sum of the donor units' outcomes, with the weights derived from the pre-treatment period.

# Synthetic Regression Control

The SRC estimation procedure consists of three stages: estimation of alignment coefficients, estimation of the noise variance, and penalized optimization of donor weights. To account for level differences, we first demean both the treated and donor series using the centering matrix. The demeaned treated outcome vector is given by
$$
\widetilde{\mathbf{y}}_1 = \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}}, \quad \text{and} \quad \widetilde{\mathbf{Y}}_0 = \mathbf{C}_{T_0} \mathbf{Y}_0^{\text{pre}}.
$$

For $\forall \: j \in \mathcal{N}$, an alignment coefficient $\hat{\theta}_j$ is computed by projecting $\widetilde{\mathbf{y}}_1$ onto the corresponding column $\widetilde{\mathbf{Y}}_{0,j}$ of the demeaned donor matrix. In plain English, this amounts to running $N_0$ separate OLS regressions, each using one donor to predict the treated unit:
$$
\hat{\theta}_j = \frac{\widetilde{\mathbf{y}}_1^\top \widetilde{\mathbf{Y}}_{0,j}}{\| \widetilde{\mathbf{Y}}_{0,j} \|^2}, \quad \text{for each } j \in \mathcal{N}_0.
$$
These coefficients collectively form the vector $\hat{\boldsymbol{\theta}} \in \mathbb{R}^{N_0}$. In plain English, these coefficients tell us how well each donor unit matches the treated unit in terms of specific trends (not levels, trends) in the data, after adjusting for broader trends via the original demeaning. If the coefficient for a donor is high, it means that unit's behavior is closely aligned with the treated unit in this adjusted space. it is the equivalent of running univariate regression for each donor unit.

We then define the alignment-adjusted donor matrix (which we will later use in optimization) as
$$
\mathbf{Y}_{\boldsymbol{\theta}} = \mathbf{Y}_0^{\text{pre}} \cdot \mathrm{diag}(\hat{\boldsymbol{\theta}}),
$$
which rescales each donor column by its alignment with the treated unit. 

The SRC method needs to estimate the residual noise variance of the donor projection (or, our donor matrix multiplied by the theta values) with the target unit's demeaned values. We call this variance term $\hat{\sigma}^2$. Using the centering matrix $\mathbf{C}_{T_0}$, we define
$$
\mathbf{G} = \mathbf{Y}_0^{\text{pre}^\top} \mathbf{C}_{T_0} \mathbf{Y}_0^{\text{pre}}.
$$
$\mathbf{G}$ tells us how similar each donor unit's behavior is to the others, but with the broad, common time trends removed. It is similar to a demeaned correlation matrix showing how the donor unitsâ€™ patterns align with each other in terms of their unique behaviors in the pre-treatment period. We extract its diagonal to normalize the donor projection, forming the operator
$$
\mathbf{Z} = \mathbf{Y}_0^{\text{pre}} \cdot \mathrm{diag}\left( \mathrm{diag}(\mathbf{G})^{-1} \right) \cdot \mathbf{Y}_0^{\text{pre}^\top}.
$$
is like a weighted version of the donor data, where the "importance" of each donor unit is based on its relationship to the others (measured by $\mathbf{G}$). Donors whose behavior is more idiosyncratic (in the demeaned space) are weighted more heavily, reflecting their unique explanatory power for the treated unit. We care about how well these demeaned adjusted donors are related to the target unit. we measure this by estimating a residual, which is constructed like
$$
\mathbf{r} = \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}} - \mathbf{C}_{T_0} \mathbf{Z} \mathbf{C}_{T_0} \mathbf{y}_1^{\text{pre}},
$$
and the estimated noise variance is
$$
\hat{\sigma}^2 = \| \mathbf{r} \|^2.
$$

The closer the residual is to 0, the better fitting the demeaned adjusted donors will be to the target unit. The more dissimilar they are, we penalize their discrepancies with the estimated noise variance $\hat{\sigma}^2$. With $\hat{\boldsymbol{\theta}}$ and $\hat{\sigma}^2$ in hand, we solve a penalized least squares problem to obtain donor weights. Specifically, we minimize the objective
$$
\underset{\mathbf{w} \in \mathcal{W}}{\operatorname*{argmin}} \left\| \mathbf{y}_1^{\text{pre}} - \mathbf{Y}_{\boldsymbol{\theta}} \mathbf{w} \right\|^2 + 2 \hat{\sigma}^2 \cdot \mathbf{1}_{N_0}^\top \mathbf{w},
$$
The solution $\mathbf{w}$ balances goodness-of-fit with regularization based on the estimated noise. Finally, we construct predictions. Let
$$
\bar{y}_1 = \frac{1}{T_0} \mathbf{1}_{T_0}^\top \mathbf{y}_1^{\text{pre}}, \quad \text{and} \quad \bar{\mathbf{y}}_0 = \frac{1}{T_0} \mathbf{Y}_0^{\text{pre}^\top} \mathbf{1}_{T_0}
$$
be the mean of the treated unit and the vector of donor means, respectively, in the pre-treatment period. Let $\mathbf{Y}_0^{\text{post}} \in \mathbb{R}^{T_1 \times N_0}$ denote the donor outcomes in the post-treatment period. Our SRC predictions take the form of

$$
\mathbf{y}^{\text{SRC}}_1 = \begin{bmatrix} \hat{\mathbf{y}}_1^{\text{pre}} \\ \hat{\mathbf{y}}_1^{\text{post}} \end{bmatrix}
= \begin{bmatrix} 
\mathbf{Y}_0^{\text{pre}} (\hat{\boldsymbol{\theta}} \odot \mathbf{w}) \\ 
\bar{y}_1 \cdot \mathbf{1}_{T_1} + \left( \mathbf{Y}_0^{\text{post}} - \mathbf{1}_{T_1} \bar{\mathbf{y}}_0^\top \right)(\hat{\boldsymbol{\theta}} \odot \mathbf{w})
\end{bmatrix}
$$

## Estimating SRC in ```mlsynth```

Now I will give an example of how to use SRC for your own applied work. We begin by installing ```mlsynth``` from my Github

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

For comparison sake, I use the Basque data since that's what the author uses.

```{python}

import pandas as pd
from mlsynth.mlsynth import SRC, FSCM
import matplotlib.pyplot as plt
import matplotlib

# matplotlib theme
jaredplot = {'axes.grid': True,
              'grid.linestyle': '--',
              'legend.framealpha': 1,
              'legend.facecolor': 'white',
              'legend.shadow': True,
              'legend.fontsize': 14,
              'legend.title_fontsize': 16,
              'xtick.labelsize': 14,
              'ytick.labelsize': 14,
              'axes.labelsize': 16,
              'axes.titlesize': 20,
              'figure.dpi': 100}

matplotlib.rcParams.update(jaredplot)

# URL to fetch the dataset
url = 'https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv'
df = pd.read_csv(url)

treat = df.columns[-1]
time = df.columns[1]
outcome = df.columns[2]
unitid = df.columns[0]

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": False,
    "counterfactual_color": "blue"
}

# Create instances of each model with the config dictionary
models = [SRC(config), FSCM(config)]


results = []
for model in models:
    result = model.fit()
    results.append((type(model).__name__, result))  # Store the model name and result as a tuple

plt.figure(figsize=(10, 6))
plt.axvline(x=20, color='red', linestyle=':', label="Terrorism")

for model_name, result in results:
    counterfactual = result["Vectors"]["Counterfactual"]
    # Plot the Counterfactual vs Observed Unit with dynamic labels
    plt.plot(counterfactual, label=f"{model_name} Basque", linestyle='--')
observed_unit = result["Vectors"]["Observed Unit"]
plt.plot(observed_unit, label=f"Basque", color='black', linewidth=2)

plt.xlabel('Time Periods')
plt.ylabel('GDP per Capita')
plt.title('Synthetic Regression Control Versus Forward SCM')
plt.legend()

plt.show()
```
