---
title: 'Synthetic Controls in Dense Settings'
date: 2025-01-24
permalink: /posts/2025/01/scdense/
---

Plenty of posts have been done in the last decade on [the synthetic control method](https://doi.org/10.1198/jasa.2009.ap08746) and related approaches; folks from [Microsoft](https://medium.com/data-science-at-microsoft/causal-inference-using-synthetic-controls-d96a890c83a7), [Databricks](https://towardsdatascience.com/how-to-do-causal-inference-using-synthetic-controls-ab435e0228f1), [Uber](https://youtu.be/j5DoJV5S2Ao?si=RWUYFjFEWpvkl8x1), [Amazon](https://towardsdatascience.com/causal-inference-with-synthetic-control-in-python-4a79ee636325), [Netflix](https://netflixtechblog.com/round-2-a-survey-of-causal-inference-applications-at-netflix-fd78328ee0bb), [Gainwell Technologies](https://andrewpwheeler.com/2019/12/06/using-regularization-to-generate-synthetic-controls-and-conformal-prediction-for-significance-tests/), and [else](https://rudrendupaul.medium.com/causal-inference-part-7-synthetic-control-methods-a-powerful-technique-for-inferring-causality-in-3ec5dbe26038)[where](https://henamsingla.medium.com/synthetic-control-method-a-z-d28099c56edb) have gone over it, detailing different aspects of the method. Many (not of course [not all](https://peerunreviewed.blogspot.com/2019/11/a-short-tutorial-on-robust-synthetic.html)) of these go over the standard SCM. Broadly, the original SCM tends to favor, under certain technical conditions, a sparse set of control units being the underlying weights. And while sparsity has plenty of appealing properties, such as our ability to interpret the synthetic control, sometimes this advice simply breaks down because [in some cases](https://ceistorvergata.it/public/files/RFCS/Giannone_illusion4-2.pdf) the sparsity notion is wrong. In other words, most of the coefficeints being 0 is a notion that cannot be defended. So in this post, I demonstrate [the](10.13140/RG.2.2.11670.97609) $\ell_2$ panel data approach, an econometric methodology by [Zhentao Shi](https://zhentaoshi.github.io/) and [Yishu Wang](https://ishwang1.github.io/) which accommodates dense data generation processes, or when the true vector of coefficient is mostly not zero.

> **Note:**  
This post goes over an extended empirical example. To see the simple and fast way to use this method, you can go to [the Python documentation](https://mlsynth.readthedocs.io/en/latest/pda.html#ell-2-relaxation) of this method in my packiage ``mlsynth`` which is both slightly more technical and a lot shorter than I'll be here.

# A Review of Synthetic Controls

Let $t \in \mathbb{N}$ and $i \in \mathbb{N}$ be the time unit indices for our outcomes, $y_{it}$. Let $i = 1$ represent the treated unit, and let the remaining units, $N_0$ in total, form the control group, which remains untreated. The key challenge of causal inference is that we only have factual outcomes, which are defined as $y_{it} = y_{it}^1 d_{it} + \left(1 - d_{it}\right)y_{it}^0$, where $d_{it}$ is an indicator for whether the unit is treated ($d_{it} = 1$) or untreated ($d_{it} = 0$). An intervention occurs at time $T_0$, such that $t = \\{1, 2, \ldots, T_0\\}$ corresponds to the pre-treatment period, and $t = \\{T_0 + 1, \ldots, T\\}$ corresponds to the post-treatment period. The objective is to estimate the counterfactual $y_{1t}^0$ for the post-treatment period, or the values we would see if the treatment did not happen.

The original SCM proposes to  use least-squares regression to estimate a convex combination of weights, $\mathbf{w}^{N_0} \operatorname*{:=} \\{2, \ldots N\\}$. The weights are constrained to lie in the simplex, meaning $\mathbf{w} \in [0, 1]$ and $\lVert \mathbf{w} \rVert_1 = 1$. Here, the columns of the donor pool matrix, denoted like $\mathbf{Y}_0 \in \mathbb{R}^{N_0 \times T}$, are our independent variables. Naturally, the pre-treatment outcome vector of our treated unit, $\mathbf{y}_1 \in \mathbb{R}^{1 \times T}$, is the dependent variable. A good synthetic control satisfies $\mathbf{w}^\top \mathbf{Y}_0 \approx \mathbf{y}_1$, or good pre-treatment fit with respect to the treated unit vector.

## $\ell_2$ Relaxation

The $\ell_2$ relaxation estimates the model $\mathbf{y}_1 = \boldsymbol{\alpha}+ \mathbf{Y}_0\mathbf{w}+\mathbf{\epsilon}$, where $\boldsymbol{\alpha}$ is some unconstrained intercept. We can see that $\mathbf{Y}_0\mathbf{w}$ is the prediction of our model, where the weights are pretty much regression coefficients. However, the way Zhentao and Yishu propose to estimate the weights is pretty ingenious: by literally using the pre-treatment relationships between the treated unit and donor pool, bounded by some positive constant. Here is how they do that:

\[
\begin{split}
\begin{aligned}
&\min_{\mathbf{w}} \frac{1}{2} \|\mathbf{w}\|_{2}^{2}, \quad \forall t \in \mathcal{T}_{1}, \quad \text{subject to } \|\boldsymbol{\eta} - \boldsymbol{\Sigma} \mathbf{w}\|_\infty \leq \tau \\
&\boldsymbol{\eta} = \frac{1}{T_1} \mathbf{Y}_0^\top \mathbf{y}_1, \\
&\boldsymbol{\Sigma} = \frac{1}{T_1} \mathbf{Y}_0^\top \mathbf{Y}_0, \quad \boldsymbol{\Sigma} \in \mathbb{R}^{N_0 \times N_0}.
\end{aligned}
\end{split}
\]


Underneath the scary matrix algebra, we have two terms to focus on here (technically three if we couint the tuning parameter): _eta_ ($\boldsymbol{\eta}$) nad _sigma_ ($\boldsymbol{\sigma}$). The first term simply is the measure of relation between treated unit and each of the control units. That's literally it, the greater the absolute value of the number, the more/less similar the donor is to the treated unit. The latter is the same, except it captures how similar the control units are to each other. So, the optimization itself just finds the values of beta that bound the infinity norm (or, the maximum absolute value) of this difference below some constant, _tau_ $\eta$. As for the weight themselves, they are shrank towards zero, but never actually equal 0, as we would (practically) get from the original convex hull weights and would literally get from the LASSO.

Critical to this process is the selection of tau. In the original paper, the authors fo a 70-30 train test split over the pre-intervention period, where they take the first 70% of the pre-treatment data and supply some values of tau. We then check the RMSE of the validation error (up until the final pre-treatment point) and select the value of tau with the lowest validation RMSE. A lower tau corresponds to an estimator closer to least squares, and too high a value of tau results in drastic underfitting (trust me, you should have seen how I wrote the Python code for this and tried to tune this, tau matters a lot).
