---
title: 'Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?'
date: 2025-05-12
categories: [Econometric Theory]
---

Many people do not understand the math behind the methods they use. If you asked most DS employes who work with causal infernece to formally prove when parallel trends holds like Pedro Sant'Anna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that's okay, that's natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.

However, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like ```mlsynth``` or ```geolift``` on you machine (or whatever you run your code in), but how do you know which tool to use, when,. and why? More importantyl, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory. And this fact is usually obvious from the problems they run into. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective.

[Ostensibly,](https://www.linkedin.com/posts/venkat-raman-analytics_why-advanced-difference-in-differences-did-activity-7318168653907025922-sQp_?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2Q8asBlsCYlwKJgJ488VWbcV1CX14FdOw) there are myths about SCM and its efficacy relative to DID, specifically "the myth that SCM/[Augmented] SCM don't have assumptions like parallel trends". I do not know who believes this myth, and if you do, [direct them to me](https://jgreathouse9.github.io/docs/consulting.html). Still, even there are not many people who confidently say "SCM has no assumptions", this is certainly the way many researchers act in practice. And that is the point of this post.

# Basic SCM

People often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this [has been](https://arxiv.org/pdf/2211.12095) [revised](https://doi.org/10.3982/QE1596) [substantially](https://arxiv.org/pdf/2108.13935) in recent years, but there's another one that people do not comment on enough: the idea of the linear factor model (or [latent variable model](https://www.jmlr.org/papers/volume19/17-777/17-777.pdf))

$$
Y_{it}(0) = \boldsymbol{\lambda}_t^\top \boldsymbol{\mu}_i + \varepsilon_{it}.
$$

What does this mean? It simply means that our outcome are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units. These factors may indeed affect each unit differently... but they are common across units. SCM [fundamentally](https://osf.io/preprints/socarxiv/fc9xt_v1) is about matching our treated unit's common factors to the common factors that we believe are embedded in the donor set (see conditons 1-6 [here](https://arxiv.org/pdf/2211.12095), such that unobservable heteorgeneity goes away and we have a good enough match between our donor pool an the target unit. The linear factor model assumes that all units (treated and donors) share the same common factors, and that the treated unit’s factor loadings can be expressed as a combination of the donors’ loadings.

A more flexible idea is [the fine-grained potential outcomes model](https://proceedings.mlr.press/v151/shi22b/shi22b.pdf), which shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit’s population. This model also allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit’s population. The practical implication of this is that when we apply these panel data models to real life settings, we are allowed to use different donor typs on the condition that they help us learn about the trajector of the target unit in the pre-intervention period. Because SCM does not care about what kind of donors you use, so long as they are informative donors.

# Proof

Don't believe me? Let's replicate some results shall we.

```{python}
#| echo: false
#| fig-align: center
import pandas as pd
from mlsynth.mlsynth import FDID, TSSC, PDA

# URL of the .dta file
url = "http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta"

# Load the Stata file directly from the URL
df = pd.read_stata(url)

df["Proposition 99"] =  ((df["region"] == "California") & (df["year"].dt.year >= 1989)).astype(int)

# Base configuration
base_config = {
    "df": df,
    "outcome": df.columns[2],
    "treat": df.columns[-1],
    "unitid": df.columns[0],
    "time": df.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue"]
}

# TSSC model
tssc_config = base_config.copy()  # Start with base and modify if necessary
arco = TSSC(tssc_config).fit()
```

Here is TSSC.

```{python}
#| echo: false
#| fig-align: center

# PDA model with method 'l2'
pda_config = base_config.copy()
pda_config["method"] = "l2"
arcol2 = PDA(pda_config).fit()
```
Here is l2 pda.

```{python}
#| echo: false
#| fig-align: center
# FDID model
fdid_config = base_config.copy()
arcofdid = FDID(fdid_config).fit()
```

Here is FDID.
