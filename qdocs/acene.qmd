---
title: 'Data Science for Policy Analysts: On Importance of Data Generating Processes'
date: 2025-02-14
categories: [Econometrics, Causal Inference]
---

# All Casual Estimates Are Not Created Equal

I just began my dissertation last week. The first chapter studies the causal impact of Texas' repeal of the tampon tax on demand as well as consumer price savings, naturally, using synthetic control methods as one may have guessed. I was doing the literature review, [my favorite part](https://andrewpwheeler.com/2020/12/16/lit-reviews-are-almost-functionally-worthless/) of the research process, and upon doing some digging, I _had to_ write a post about this.

I found [a paper](https://doi.org/10.1038/s41562-024-01996-4) called "Why current menstrual policies do not work". Much of the framing of the paper is fine, until we get to the part about evidence regarding the tampon tax. King writes

> Similarly, a recent UK campaign to abolish a 5% ‘tampon tax’ unintentionally boosted the profits of disposable product manufacturers by about £15 million per year, without substantially reducing the cost of products for consumers,

citing [this paper](https://taxpolicy.org.uk/wp-content/assets/tampon_tax_report.pdf) by Tax Policy Associates, with the web version being at [this link](https://taxpolicy.org.uk/2022/11/10/tampontax/). I read this and though "Hmm, this is an empirical claim, I wonder what their causal methodology was. So I went to look the paper up as a bit of professional curiosity.

The paper studies the pass through rate of abolishing the tax on savings to consumers. To quote the paper directly,

> We used Office for National Statistics data to analyse tampon price changes around 1 January 2021, the date that the “tampon tax” was abolished. We were able to do this because the ONS includes tampons (but not other menstrual products) in the price quotes it samples every month to compile the consumer prices index. Since 2017, the ONS has published the full datasets for its price sampling.

Okay fine. No problems so far. The euthors find "Overall, the average price for [tampons in] the period after the VAT abolition is about 1.5% less than it was beforehand." Still no issues so far. But then we check the methodology, that they [link to](https://github.com/DanNeidle/tampontax/blob/main/tampon_tax_indexes_all_goods.py) at their Github, and the results were less than exciting, putting it politely. Why? The authors do a simple t-test. That is, a simple pre-post test which compares the mean difference of tampon prices before the abolition of the tax and after the abolition of the tax. Precisely, they write

> Apply [sic] statistical techniques to these datasets is not straightforward given the limited number of datapoints and very high degree of volatility. It was, however, thought appropriate to run an unequal variance one-sided t-test (using the python SciPy library) to compare the pricing datasets for the six months before 1 January 2021 with those for the subsequent six months.

Appropriate? By who? Who said this was a good idea? Look, I know that real data are often messy and that we have to take steps to compensate for noise, corruption, and overall lack of cleanliness, but I want to be clear about something: _no matter what_ the extant difficulties are, this is not at all the correct way to do things, and the point of this post is to explain why. To really understand what's wrong here, we have to discuss data generating processes.

## Data Generating Processes

### DID

The parallel trends assumption in Difference-in-Differences follows directly from the two-way fixed effects model. In this setup, the outcome $y_{jt}$ follows the data-generating process  

$$
y_{jt} = \boldsymbol{\lambda}_j^\top \boldsymbol{\delta}_t + \boldsymbol{\epsilon}_{jt},
$$

where the unit-specific fixed effect is given by $\boldsymbol{\lambda}_j = \begin{bmatrix} a_j \\ 1 \end{bmatrix}$ and the time-specific fixed effect is given by $\boldsymbol{\delta}_t = \begin{bmatrix} 1 \\ b_t \end{bmatrix}$. This simplifies to  

$$
y_{jt} = a_j + b_t + \epsilon_{jt},
$$

which captures an additive unit effect $a_j$, a time effect $b_t$, and an idiosyncratic error $\epsilon_{jt}$. For the purposes of DID, we restrict our attention to the pre-treatment period. Since no treatment occurs in the pre-treatment period, the observed outcome is the untreated potential outcome (the sum of the unit and time effect). Taking expectations removes the error term, giving us:  

$$
\mathbb{E}[y_{jt}(0)] = a_j + b_t.
$$

Consider two periods $t$ and $t'$ in the pre-treatment period. The expected untreated outcome at $t$ is $a_j + b_t$, and at $t'$ it is $a_j + b_{t'}$. Taking the difference, we obtain  

$$
\mathbb{E}[y_{jt}(0)] - \mathbb{E}[y_{jt'}(0)] = (a_j + b_t) - (a_j + b_{t'}) = b_t - b_{t'}.
$$

The unit fixed effect $a_j$ cancels out, implying that changes in expected untreated outcomes depend only on the time effects. This result applies both to the treated unit and to the control group. For the treated unit, denoted as $j = 1$, the expected difference is  

$$
\mathbb{E}[y_{t1}(0)] - \mathbb{E}[y_{t'1}(0)] = b_t - b_{t'}.
$$

For the control group, taking the average over all units $j \in \mathcal{N}_0$, we obtain  

$$
\mathbb{E}[y_{\mathcal{N}_0 t}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t'}(0)] = b_t - b_{t'}.
$$

Since the time difference is identical for both groups, this means that absent the treatment, the difference between the treatment unit and the control unit would be some constant, let's call it $\alpha$. Note what this means: the groups do NOT need to be the same in terms of levels. They just need to be identical in terms of _average trends_, proxied by the average scalar difference of their trajectories.

### T Test

We can follow the exact same logic for t-test, under an even simpler setup. In the pre-treatment period, the expected outcome for unit $j$ is:

$$
\mathbb{E}[y_{jt} \mid d_{jt} = 0] = \mathbb{E}[\lambda_j + \delta_t + \epsilon_{jt} \mid d_{jt} = 0]
$$

Since $\lambda_j$ and $\delta_t$ are both constant for a given unit and time, the expected outcome simplifies to:

$$
\mathbb{E}[y_{jt} \mid d_{jt} = 0] = \lambda_j + \delta_t + \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]
$$

Similarly, in the post-treatment period, the expected outcome for unit $j$ is:

$$
\mathbb{E}[y_{jt} \mid d_{jt} = 1] = \mathbb{E}[\lambda_j + \delta_t + \epsilon_{jt} \mid d_{jt} = 1]
$$

Since $\lambda_j$ and $\delta_t$ are again constant, this simplifies to:

$$
\mathbb{E}[y_{jt} \mid d_{jt} = 1] = \lambda_j + \delta_t + \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]
$$

Now, we compute the difference in expected outcomes between the pre-treatment and post-treatment periods for the treated unit

$$
\underbrace{\mathbb{E}[y_{jt} \mid d_{jt} = 1]}_{\text{Post-Period}} -
\underbrace{\mathbb{E}[y_{jt} \mid d_{jt} = 0]}_{\text{Pre-Period}}
$$

This is just the t-test estimator expressed in terms of expectations. Substituting the expressions above into this, we get:

$$
\left( \lambda_j + \delta_t + \underbrace{\mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]}_{\text{Post-Period}} \right)
-
\left( \lambda_j + \delta_t + \underbrace{\mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]}_{\text{Pre-Period}} \right)
$$

Since both $\lambda_j$ (unit fixed effect) and $\delta_t$ (time fixed effect) are present in both terms, they cancel each other out:

$$
\underbrace{\mathbb{E}[y_{jt} \mid d_{jt} = 1]}_{\text{Post-Period}} -
\underbrace{\mathbb{E}[y_{jt} \mid d_{jt} = 0]}_{\text{Pre-Period}}
=
\underbrace{\mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]}_{\text{Post-Period}} -
\underbrace{\mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]}_{\text{Pre-Period}}
$$

For the pre-post t-test to be valid, we essentially must assume that the error terms $\epsilon_{jt}$ are independent and identically distributed (i.i.d.) between the pre-treatment and post-treatment periods. This means that any difference in outcomes should be due solely to the treatment effect, not due to other unobserved factors.

The assumption is that the expected difference in error terms is zero:

$$
\mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1] - \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0] = 0
$$

Thus, the difference in expected outcomes for the treated unit simplifies to:

$$
\mathbb{E}[y_{jt} \mid d_{jt} = 1] - \mathbb{E}[y_{jt} \mid d_{jt} = 0] = 0
$$

This result implies that any observed difference in outcomes between the pre- and post-treatment periods must be entirely due to the treatment effect, assuming no confounding factors or time-specific shocks, which basically is _never_ true in real life. In fairness, the authors do recognize these shortcomings, writing

> The prices of tampons and the other consumer goods considered in this paper will be affected by numerous factors – the supply of raw materials, energy costs, and sheer random happenstance. It is therefore unsurprising that we see a large amount of month-to-month variation in prices; this makes it difficult to identify separate real trends from noise. More sophisticated statistical methods than a t-test are therefore not helpful (difference-in-difference and synthetic control methods were attempted, but did not produce meaningful results).

I agree that there are plenty of confounding factors (mostly unobserved) and noise, but in situations like that, we should use modern advances in econometrics to deal with this instead of shoehorning the data for a given method.

## A Re-Analysis with ```mlsynth```

Here is a re-analysis of this question using ```mlsynth```. I use the $\ell_2$ relaxer.

```{python}
#| fig-align: center
#| echo: false

import requests
import pandas as pd
from io import StringIO
from mlsynth.mlsynth import PDA
import os
import matplotlib

jared_theme = {
    "axes.grid": False,
    "grid.linestyle": "-",
    "grid.color": "black",
    "legend.framealpha": 1,
    "legend.facecolor": "white",
    "legend.shadow": True,
    "legend.fontsize": 14,
    "legend.title_fontsize": 16,
    "xtick.labelsize": 11,
    "ytick.labelsize": 14,
    "axes.labelsize": 14,
    "axes.titlesize": 20,
    "figure.dpi": 120,
    "axes.facecolor": "white",
    "figure.figsize": (10, 5.5),
}

matplotlib.rcParams.update(jared_theme)

def fetch_and_combine_github_csvs(owner, repo, directory):
    """
    Fetches all CSV files in the given GitHub directory that start with 'upload',
    combines them into a single DataFrame, and processes it.

    Parameters:
        owner (str): GitHub username or organization name.
        repo (str): Repository name.
        directory (str): Directory path within the repository.

    Returns:
        pd.DataFrame: Combined and processed DataFrame.
    """
    # GitHub API URL to list contents of the directory
    api_url = f'https://api.github.com/repos/{owner}/{repo}/contents/{directory}'

    # Get the directory contents
    response = requests.get(api_url)
    files = response.json()

    # Filter for CSV files that start with 'upload'
    csv_files = [file for file in files if file['name'].startswith('upload') and file['name'].endswith('.csv')]

    # Base URL for raw file content
    raw_base_url = f'https://raw.githubusercontent.com/{owner}/{repo}/main/{directory}/'

    # List to hold DataFrames
    df_list = []

    # Download and read each CSV file
    for file in csv_files:
        csv_url = raw_base_url + file['name']
        csv_response = requests.get(csv_url)
        df = pd.read_csv(StringIO(csv_response.text))
        df_list.append(df)

    # Concatenate all DataFrames
    combined_df = pd.concat(df_list, ignore_index=True)

    # Convert INDEX_DATE to a proper date format (assuming YYYYMM format)
    combined_df['INDEX_DATE'] = pd.to_datetime(combined_df['INDEX_DATE'], format='%Y%m')

    # Sort by ITEM_ID and INDEX_DATE
    combined_df = combined_df.sort_values(by=['ITEM_ID', 'INDEX_DATE'])

    # Create 'Tax' column and set to 1 if ITEM_ID == 520206 and INDEX_DATE >= Jan 2021
    combined_df['Tax'] = 0
    combined_df.loc[(combined_df['ITEM_ID'] == 520206) & (combined_df['INDEX_DATE'] >= '2021-01-01'), 'Tax'] = 1

    # Select only specific columns by index
    selected_columns = [0, 1, 2, 5, -1]  # Column indices to keep
    combined_df = combined_df.iloc[:, selected_columns]

    # Reset index
    combined_df = combined_df.reset_index(drop=True)

    # Filter only ITEM_IDs that have exactly 53 observations
    counts = combined_df['ITEM_ID'].value_counts()
    balanced_item_ids = counts[counts == 53].index  # Get ITEM_IDs with 53 observations
    combined_df = combined_df[combined_df['ITEM_ID'].isin(balanced_item_ids)]

    combined_df['ITEM_ID'] = 'Unit ' + combined_df['ITEM_ID'].astype(str)

    combined_df = combined_df.reset_index(drop=True)

    return combined_df


owner = 'DanNeidle'
repo = 'tampontax'
directory = 'ONS_data'
df = fetch_and_combine_github_csvs(owner, repo, directory)

treat = "Tax"
outcome = "ITEM_INDEX"
unitid = "ITEM_ID"
time = "INDEX_DATE"

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "method": "l2", "tau": .25
}

model = PDA(config)

arco = model.fit()
```

I won't go over the details of it here, as I've done that [elsewhere](https://jgreathouse9.github.io/docs/scdense.html), but the returned ATT we get with a tau equal to 0.25 is -2.027 with a 95% CI of [-3.01,-1.05]. For context, the authors find a price drop of -1.5.

My analysis has the advantage of using formal causal methods which explicitly takes advantage of a control group (more than 700 controls) instead of the dozen or so the original folks do, and addresses variance estimation using heteoskedasticity and autocorrelation adjusted CIs.

The point of this post is that we can't just shove a dataset into ```scipy``` and (metaphorically) pray for valid results. We can't say "well, parallel trends for DID doesn't hold, let's go with the old school t-test and report that" (by the way, parallel trends DOES in fact hold when we use the Forward DID method from ```mlsynth```). In such cases,  we need a little more care to be put into these questions if we mean to even try to answer them.

My main conclusion reminds me more or less of [a post](https://statmodeling.stat.columbia.edu/2023/10/12/debate-over-effect-of-reduced-prosecutions-on-urban-homicides-also-larger-questions-about-synthetic-control-methods-in-causal-inference/) by Andrew Gelman, who writes pretty much what I think about the report above. It isn't that the question under study is silly- the question matters, but

> The problem is that the data are observational, the data are sparse and highly variable; that is, the problem is hard. And it doesn’t help when researchers are under the impression that these real difficulties can be easily resolved using canned statistical identification techniques.

Said another way, the answer you get to the question is less important than the mathematical reasoning process you use to get to the answer. All of the tests we run and models we estimate have assumptions. These assumptions fulfill identification criteria, and are themselves are usually underpinned by some theoretically motivated DGP. For objective causal inference, researchers should respect this idea, even if it means we have to employ more advanced estimators.
