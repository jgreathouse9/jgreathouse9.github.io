---
title: 'Forward Synthetic Control Estimation'
date: 2025-03-31
categories: [Econometrics, Causal Inference, Data Science]
---
# Intro
Interpolation bias is known to be a problem with synthetic control methods. The donors we choose should be as similar to the target unit as possible in the pre-treatment period for valid counterfactual prediction. This means that selecting a donor pool is critical, but this isn't always easy in settings where many controls exist. This post introduces the [Forward Selected Synthetic Control Method](https://doi.org/10.1016/j.econlet.2024.111976). This method uses the Forward Selection method to choose the donor pool for a synthetic control model. Then, it uses that selected donor pool to estimate the out of sample predictions. 

Let $\mathbb{R}$ denote the set of real numbers. Let a caligraphic letter, say $\mathcal S$, denote a descrete set whose cardinality is $S=|\mathcal{S}|$. Let $t \in \mathbb{N}$ and $i \in \mathbb{N}$, represent the indices for $T$ time periods and  $N$ units. The pre-treatment period consists of consecutive time periods $\mathcal{T}_1 = \{1, 2, \ldots, T_0\}$ (cardinality $T_1$), while the post-treatment period is given by $\mathcal{T}_2 = \{T_0 + 1, \ldots, T\}$ (cardinality $T_2$). The treated unit is indexed by $i = 1$, while the remaining set of units, $\mathcal{N}_0 \coloneqq \{2, \ldots, N_0 + 1\}$ (cardinality $N_0$), forms the control group. Each outcome for all units is denoted by $y_{it}$. Denote the outcome vector for the treated unit as $\mathbf{y}_1 \coloneqq \begin{bmatrix} y_{11} & y_{12} & \cdots & y_{1T} \end{bmatrix}^\top \in \mathbb{R}^T$, where each entry corresponds to the outcome of the treated unit at time $t$. The donor pool matrix, similarly, is defined as $\mathbf{Y}_0$, or the concatenation of the control unit vectors, with $N_0$ columns. The corresponding space of synthetic control weights is the $(N_0)$-dimensional probability simplex:

$$
\Delta^{N_0} \coloneqq \left\{ \mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0} : \|\mathbf{w}\|_1 = 1 \right\}.
$$

Now, consider a restricted donor pool chosen by forward selection. Consisting of a strict subset of control units, $\mathcal{S} \subseteq \mathcal{N}_0$, with cardinality $|\mathcal{S}| = k$ where $k < N_0$, this subset induces a subsimplex of the original simplex of donor units, defined as:

$$
\Delta^{k - 1}(\mathcal{S}) \coloneqq \left\{ \mathbf{w}' \in \mathbb{R}_{\geq 0}^{k} : \|\mathbf{w}'\|_1 = 1 \right\}.
$$

Since any feasible weight vector $\mathbf{w}' \in \Delta^{k-1}(\mathcal{S})$ can be embedded in $\Delta^{N_0-1}$ by setting the weights of all units outside $\mathcal{S}$ to zero, we have the natural inclusion:

$$
\Delta^{k - 1}(\mathcal{S}) \subseteq \Delta^{N_0 - 1}.
$$

Now we can discuss how we choose our donor pool. At each iteration we identify the optimal subset of control units that minimize the mean squared error (MSE) in the pre-treatment period. First, each donor $j \in \mathcal{N}_0$ is evaluated independently by solving the program:

$$
\mathbf{w}^{(j)} = \arg\min_{\mathbf{w} \in \Delta^0(\{j\})} \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w} \|_2^2.
$$

We compute 1 MSE per donor unit (effectively this is searching for the nearest neighbor and uses it as the optimal donor). The donor with the lowest MSE, denoted as $j^\ast$, is selected as the initial candidate control unit:

$$
 j^\ast = \arg\min_{j \in \mathcal{N}_0} \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w}^{(j)} \|_2^2.
$$

Next, we iteratively expand the selected donor pool by adding each of the control units in the control group, or $j \in \mathcal{N}_0 \setminus \mathcal{S}$, to a new synthetic control model. As before, our criteria for donor selection is the two units which together minimizes the MSE over the new subsimplex:

$$
 j^\ast = \arg\min_{j \in \mathcal{N}_0 \setminus \mathcal{S}} \min_{\mathbf{w} \in \Delta^{k}(\mathcal{S} \cup \{j\})} \| \mathbf{y}_{1,\mathcal{T}_1} - \mathbf{Y}_{0,\mathcal{T}_1} \mathbf{w} \|_2^2.
$$

This equation represents the greedy step where the next donor $j^\ast$ is selected based on the optimization over the subsimplex formed by the current set $\mathcal{S}$ and the candidate donor $j$. To generalize, we can define a tuple of sets of donor selections,  $\mathcal{S}$, and their corresponding MSE values. Let $\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_K$ represent the different sets of donors chosen during the iterative process, and let their corresponding MSE values be denoted as $MSE(\mathcal{S}_1), MSE(\mathcal{S}_2), \dots, MSE(\mathcal{S}_K)$. Then, the final optimal set $\mathcal{S}^\ast$ is the set that minimizes the MSE, which can be expressed as:

$$
\mathcal{S}^\ast = \arg\min_{\mathcal{S}_i \in \{\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_K\}} MSE(\mathcal{S}_i).
$$

## Estimation in ```mlsynth```

As ususal, we begin by installing ```mlsynth```.

```bash
pip install -U git+https://github.com/jgreathouse9/mlsynth.git
```

And then we load the Prop 99 data and fit the model in the ususal ```mlsynth``` fashion.

```{python}
#| fig-align: center

import pandas as pd
from IPython.display import display, Markdown
from mlsynth.mlsynth import FSCM
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"
data = pd.read_csv(url)
config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": "red"}

arco = FSCM(config).fit()
```

After estimating, we can get the weights like

```{python}
weights_dict = arco['Weights'][0]
df = pd.DataFrame(list(weights_dict.items()), columns=['State', 'Weight'])
display(Markdown(df.to_markdown(index=False)))
```
These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the synthetic control, with only 6 being assigned positive weight.
