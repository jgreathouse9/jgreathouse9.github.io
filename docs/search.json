[
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, many datasets we work with come in pretty csv files that are clean. And while that’s great… oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible code/script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we’d need to ask AAA and pay thousands of dollars for an extended time series… but now we don’t need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for the scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA’s website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is “https://gasprices.aaa.com/?state=MA”. For Florida, the URL is “https://gasprices.aaa.com/?state=FL”. See the pattern? There’s a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python’s requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we’ve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year’s worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I’ve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "\nJared Greathouse\n",
    "section": "",
    "text": "Jared Greathouse\n\n\nEconometrician. Consultant.\n\n\n👋 Welcome\nI am Jared Amani Greathouse. I offer consulting services in data science and econometrics, especially with synthetic control methods.\n\n\n📄 Intake Form\nTo offer me a project:\n\nDownload the Request Intake Form (DOCX). If the form does not download, copy the link and put it in your browser.\nWhen it is filled out, email it to me with a short description of the project.\nFees begin at at $250/hour.\n\n\n\n📬 Contact\nSchool Email: jgreathouse3@student.gsu.edu (this is the best way to reach me)\nPersonal Email: j.greathouse200@gmail.com\nWebsite: jgreathouse9.github.io"
  },
  {
    "objectID": "fasc.html",
    "href": "fasc.html",
    "title": "Forward Augmented Synthetic Controls",
    "section": "",
    "text": "Synthetic Control Methods (SCM) is a widely used framework for estimating causal effects when randomized experiments are not feasible. At its core, SCM constructs a weighted average of control (donor) units to approximate the treated unit’s pre-treatment trajectory. The goal is to find an in-sample/pre–treatment average of controls that closely mirrors the treated unit before the intervention.\nMuch of the method’s credibility hinges on the quality of this pre-treatment fit. Econometricians regularly warn that poor pre-treatment fit undermines the validity of SCM estimates. Even if the optimization problem is formally well-posed, poor alignment between the treated unit and its in-sample match can lead to substantial bias. The intuition is straightforward: if similar units are assumed to behave similarly, a control group that fails to mimic the treated unit before treatment is unlikely to produce a credible counterfactual afterward. Just as important as pre-treatment fit is the composition of the donor pool. Including irrelevant or poorly matched units, or omitting relevant ones, can distort the synthetic weights and lead to misleading inferences. But how should the donor pool be chosen?\nOne increasingly popular solution to the imperfect match is the Augmented Synthetic Control Method (ASCM), known in industry through Meta’s GeoLift library. Shops like Recast use it, and data scientists such as Mandy Liu and Svet Semov have helped bring it to applied audiences.\nMethods for donor pool selection have also received attention. In fact, this is part of what makes GeoLift so popular: it attempts to identify the most similar markets to a treated group before the intervention. In academic settings, approaches like forward selection, and even random forests have been proposed to automate or guide the choice of appropriate donors.\nBut what if we can do better?\nIn previous posts, I’ve written about donor selection strategies and how to handle imperfect pre-treatment fit. In this post, I introduce a synthesis of both: the Forward Augmented Synthetic Control estimator. By combining forward selection with a bias correction step, I show that we can reduce in-sample risk relative to the standard ASCM and Forward SCM alone. This approach is illustrated using two popular SCM case studies: the Kansas tax cut experiment and California’s Proposition 99.\n\n\nLet \\(\\mathbb{R}\\) denote the set of real numbers. Calligraphic letters, such as \\(\\mathcal{S}\\), represent discrete sets with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) index a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Unit \\(j=1\\) is the treated unit, with the set of control units \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\) of cardinality \\(N_0\\). The pre-treatment period is \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\le T_0 \\}\\) and the post-treatment period is \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}\\). The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), and the outcome vector for unit \\(j\\) is \\(\\mathbf{y}_j = (y_{j1}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^T\\). The treated unit’s outcome vector is \\(\\mathbf{y}_1\\), and the donor matrix is \\(\\mathbf{Y}_0 = \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\).\n\n\nI frequently consider linear combinations of donor outcomes. The convex hull of the donor vectors is\n\\[\n\\operatorname{conv}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\},\n\\]\nand the affine hull is\n\\[\n\\operatorname{aff}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\}.\n\\]\nWhile both involve weighted averages of donors, the convex hull restricts weights to be non-negative, whereas the affine hull allows negative weights and extrapolation beyond the convex hull.\nThe corresponding sets of feasible weights are\n\\[\n\\mathcal{W}_{\\mathrm{conv}} = \\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}, \\quad\n\\mathcal{W}_{\\mathrm{aff}} = \\{ \\mathbf{w} \\in \\mathbb{R}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}.\n\\]\n\n\n\n\n\n\n\n\n\nIn two dimensions, the convex hull of two donor points is the line segment connecting them. It represents all convex combinations of the donors, where the weights are nonnegative and sum to one. In the figure above, this is shown as the black line segment, and the green point lies within it because its weights are both positive and sum to one. By contrast, the affine hull is the entire infinite line passing through the donors.\nAffine combinations also require weights to sum to one, but they may be negative, which allows extrapolation beyond the segment (though practically we may force the weights to be between negative one and positive one, what we’d call a kind of polytope). The dashed gray line in the figure illustrates the affine hull, and the red point lies outside the convex segment but on this line because one weight is negative.\nAny point not on this line at all cannot be expressed as an affine combination, as shown by the purple point. The key difference is that the convex hull is bounded and interpolative, while the affine hull is unbounded and permits extrapolation arbitrarily far from the observed donor points."
  },
  {
    "objectID": "fasc.html#forward-selection-scm-fscm",
    "href": "fasc.html#forward-selection-scm-fscm",
    "title": "Forward Augmented Synthetic Controls",
    "section": "Forward Selection SCM (FSCM)",
    "text": "Forward Selection SCM (FSCM)\nFSCM constructs a synthetic control iteratively by adding one donor at a time. Starting from the empty set \\(\\mathcal{S} = \\emptyset\\), at each iteration the algorithm considers each candidate donor \\(j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}\\) and forms \\(\\mathcal{S}^\\prime = \\mathcal{S} \\cup \\{j\\}\\). For each candidate set \\(\\mathcal{S}^\\prime\\), it solves the restricted SCM problem over the donor submatrix \\(\\mathbf{Y}_0^{\\mathcal{S}^\\prime}\\), defined as the columns of \\(\\mathbf{Y}_0\\) corresponding to units in \\(\\mathcal{S}^\\prime\\):\n\\[\n\\mathbf{w}_{\\mathcal{S}^\\prime}^\\ast =\n\\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime)}{\\operatorname*{argmin}} \\;\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, \\mathcal{S}^\\prime} \\mathbf{w} \\right\\|_2^2\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|\\mathcal{S}^\\prime|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe donor selected at each iteration is\n\\[\nj^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}}{\\operatorname*{argmin}} \\;\n\\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S} \\cup \\{j\\})}\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1}{}_{\\mathcal{S} \\cup \\{j\\}} \\mathbf{w} \\right\\|_2^2\n\\]\nand it is then added to the selected set by updating\n\\[\n\\mathcal{S} \\leftarrow \\mathcal{S} \\cup \\{j^\\ast\\}.\n\\]\nThe donor selection process proceeds sequentially according to this forward selection rule. In principle, the procedure may continue until all donor units have been exhausted, but in practice analysts use stopping rules to terminate the selection at a sensible point.\n\nExhaustive Search\nIn the exhaustive option, selection proceeds until the pre-treatment mean squared error (MSE) is minimized over all candidate donor subsets. Let \\(\\mathcal{P}(\\mathcal{N}_0)\\) denote the power set of donor indices, and for any \\(S \\subseteq \\mathcal{N}_0\\), define the within–pre-treatment fit as\n\\[\n\\text{MSE}(S) = \\frac{1}{T_0} \\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S)} \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^S \\mathbf{w} \\right\\|_2^2,\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(S) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|S|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe exhaustive-search estimator iestimates \\(\\sum_{i=0}^{k-1} (N_0 - i) = k N_0 - \\frac{k(k-1)}{2}\\) total models. It chooses\n\\[\nS^\\ast = \\underset{S \\in \\mathcal{P}(\\mathcal{N}_0)}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1,S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nThis means we continue until all donors are eventually selected.\n\n\nInformation Criteria, per Shi and Huang, 2023\nAnother option is to rely on a model selection criterion, such as the modified BIC (mBIC), which balances pre-treatment fit with model complexity. The modified BIC for a selected set \\(S\\) of donor units is defined as\n\\[\n\\text{mBIC}(S) = T_0 \\cdot \\log(\\text{MSE}) + |S| \\cdot \\log(T_0),\n\\]\nwhere\n\\[\n\\text{MSE} = \\frac{1}{T_0} \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, S} \\mathbf{w}_S \\right\\|_2^2.\n\\]\nThe forward selection procedure may terminate when adding a new donor increases the penalized error:\n\\[\n\\text{mBIC}(S \\cup \\{j\\}) &gt; \\text{mBIC}(S).\n\\]\n\n\nDonor Pool Cap\nA final practical option is to impose an upper bound on the number of selected donors. When a cardinality cap is imposed, let \\(p \\in (0,1]\\) and \\(K = \\lfloor p N_0 \\rfloor\\), so that the search is restricted to subsets \\(S \\subseteq \\mathcal{N}_0\\) with \\(|S| \\le K\\). The corresponding FSCM estimator chooses\n\\[\nS^\\ast = \\underset{\\substack{S \\subseteq \\mathcal{N}_0 \\\\ |S| \\le K}}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^{S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nWhichever variant of the forward selection algorithm is used—exhaustive search, model selection, or cardinality constraint—the result is a selected subset of donors \\(S^\\ast \\subseteq \\mathcal{N}_0\\) and a corresponding optimal weight vector \\(\\mathbf{w}_{S^\\ast}^\\ast \\in \\mathbb{R}^{|S^\\ast|}\\). To unify notation and simplify presentation going forward, we define the final forward-selected weights by embedding this sparse solution into the full donor space:\n\\[\nw_j^{\\mathrm{FSCM}} =\n\\begin{cases}\n\\left(\\mathbf{w}_{S^\\ast}^\\ast\\right)_j, & \\text{if } j \\in S^\\ast, \\\\\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nEquivalently, let \\(\\mathbf{w}^{\\mathrm{FSCM}} \\in \\mathbb{R}^{N_0}\\) be the zero-padded extension of \\(\\mathbf{w}_{S^\\ast}^\\ast\\), such that it assigns zero weight to all donors not selected by FSCM. This implementation provides flexibility in practice: exhaustive evaluation is feasible in low-dimensional settings, while early stopping or a capped selection is recommended in high-dimensional applications. The algorithm in mlsynth issues a warning if the donor pool contains 200 or more units."
  },
  {
    "objectID": "fasc.html#the-augmented-synthetic-control-estimator",
    "href": "fasc.html#the-augmented-synthetic-control-estimator",
    "title": "Forward Augmented Synthetic Controls",
    "section": "The Augmented Synthetic Control Estimator",
    "text": "The Augmented Synthetic Control Estimator\nBuilding on this baseline formulation, the ASCM introduces a regularization term that penalizes deviations of the weight vector from a reference or initial weight vector, \\(\\mathbf{w}_0\\). The augmented objective can be written as\n\\[\n\\mathbf{w}^{\\mathrm{aug}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}} \\right\\|_2^2.\n\\]\nwhere \\(\\lambda \\ge 0\\) controls the strength of the penalty.\n\n\nAbout that lambda…\n\n\n\n\n\n\n\nNote\n\n\n\nOne may ask “Jared, why did you include the penalty on the weight deviation term instead of the fit term, as Ben-Michael and co. do in Equation 18 of their paper?” Here’s why.\nIn ASCM, the placement of the regularization parameter \\(\\lambda\\) determines how the estimator balances pre-treatment fit and fidelity to the original SCM weights. Their formulation minimizes:\n\\[\n\\mathbf{w}^\\ast_{\\text{alt}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\lambda \\|\\mathbf{y}_1^{\\mathcal{T}_1}- \\mathbf{Y}_{0}^{\\mathcal{T}_1}\\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nwhile ours solves:\n\\[\n\\mathbf{w}^{\\mathrm{aug}}= \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\|\\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\lambda \\|\\mathbf{w}^{\\mathrm{aug}} -\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nIt turns out these are mathematically equivalent under a simple reparameterization. If we define \\(\\lambda_{\\text{alt}} = \\frac{1}{\\lambda_{\\text{aug}}}\\), then both objectives yield the same solution. This follows directly from the first-order conditions of each problem, which differ only by a scaling of the Lagrange multiplier. So in truth, it’s just a matter of which interpretation you find more natural.\nI personally prefer our formulation because as \\(\\lambda \\to \\infty\\), the penalty on deviation dominates, and \\(\\mathbf{w}^\\ast_{\\text{aug}}\\) collapses to the projection of \\(\\mathbf{w}^{\\mathrm{SCM}}\\) onto the affine constraint. In other words, when the original FSCM fit is already good, we want to stick close to it. Conversely, as \\(\\lambda \\to 0\\), the regularization term disappears and the solution becomes the best-fitting affine combination of the donor units, completely unconstrained by the initial weights. That’s appropriate when the original fit is poor and we’re willing to learn something new (although this is probably going to be uncommon in practice). So while the math is equivalent, the perspective isn’t. I find it much more natural to think of \\(\\lambda\\) as controlling how much I “trust” the prior weights. And that’s easier to reason about when \\(\\lambda\\) is attached to the deviation term.\n\n\n\nThe intuition here is pretty simple. If the SCM weights are already giving us good pre-treatment fit, then there is little incentive to extrapolate away from the original (F)SCM solution. However, if there’s need for better fit, then we will extrapolate away from the convex hull solution. In practice, Ben-Michael and co advocate for choosing lambda via cross validation."
  },
  {
    "objectID": "fasc.html#conformal-prediction-intervals",
    "href": "fasc.html#conformal-prediction-intervals",
    "title": "Forward Augmented Synthetic Controls",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\nTo quantify uncertainty around the estimated counterfactual trajectory, we apply conformal prediction intervals based on block-permuted pre-treatment residuals. These intervals are distribution-free, require no assumptions about the data-generating process, and provide valid finite-sample marginal coverage. Let \\(\\hat{y}_t^{\\text{cf}}\\) denote the estimated counterfactual outcome at time \\(t\\), and let \\(y_t^{\\text{obs}}\\) be the observed outcome. We begin by computing residuals for all time periods:\n\\[\n\\varepsilon_t = y_t^{\\text{obs}} - \\hat{y}_t^{\\text{cf}}.\n\\]\nWe then construct a conformal score by calculating the mean absolute residual over the post-treatment period. To simulate the distribution of this score under the null (i.e., assuming no treatment effect), we perform circular block permutations of the residual vector and recompute the same statistic for each shifted version.\nThis yields an empirical distribution of conformal scores under the null. We take the \\((1 - \\alpha)\\) quantile of this distribution as our conformal threshold, denoted \\(q_{1 - \\alpha}\\). To center the interval, we compute the mean residual over the pre-treatment period:\n\\[\n\\bar{\\varepsilon} = \\frac{1}{T_0} \\sum_{t \\leq T_0} \\varepsilon_t.\n\\]\nThe conformal prediction interval for each post-treatment time \\(t &gt; T_0\\) is then given by:\n\\[\n\\left[ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} - q_{1 - \\alpha},\\ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} + q_{1 - \\alpha} \\right].\n\\]\nThis approach ensures that the prediction intervals account for uncertainty in the counterfactual trajectory while adjusting for systematic bias in the pre-treatment fit. The shaded regions in our figures visualize these conformal intervals."
  },
  {
    "objectID": "fscm.html",
    "href": "fscm.html",
    "title": "Forward Selected Synthetic Control",
    "section": "",
    "text": "Interpolation bias is a known issue with synthetic control models (SCMs) For valid counterfactual prediction, the donor units, or the set of units that were never exposed to an intervention, should be as similar as possible to the treated unit in the pre-treatment periods. Selecting an appropriate donor pool is therefore critical, for practitioners. However, this can be challenging in settings with many potential controls, potentially many more control units than pre-treatment periods. Practically, researchers may wish to use this method when they have a high-dimensional donor pool and may be unsure as to which donors to include to reduce the impact of interpolation biases. To this end, this blog post introduces users the Forward Selected SCM. This applies Forward Selection (FS) to choose the donor pool for a SCM before estimating out-of-sample predictions."
  },
  {
    "objectID": "fscm.html#notation",
    "href": "fscm.html#notation",
    "title": "Forward Selected Synthetic Control",
    "section": "Notation",
    "text": "Notation\nLet \\(\\mathbb{R}\\) denote the set of real numbers. A calligraphic letter, such as \\(\\mathcal{S}\\), represents a discrete set with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) represent indices for a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Let \\(j = 1\\) be the treated unit, with the set of controls being \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0\\). The pre-treatment period consists of the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\},\\) where \\(T_0\\) is the final period before treatment. Similarly, the post-treatment period is given by \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}.\\) The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), where a generic outcome vector for a given unit in the dataset is \\(\\mathbf{y}_j \\in \\mathbb{R}^T\\), where \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^{T}\\). The outcome vector for the treated unit specifically is \\(\\mathbf{y}_1\\). The donor matrix, similarly, is defined as \\(\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\), where each column indexes a donor unit and each row is indexed to a time period.\nSCM estimates the counterfactual outcome for the treated unit by solving the program\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} \\|_2^2 \\: \\forall t \\in \\mathcal{T}_1.\n\\]\nWe seek the weight vector, \\(\\mathbf{w}\\), that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. For our purposes, the space of SC weights is the \\(N_0\\)-dimensional probability simplex \\(\\Delta^{N_0} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{N_0} : \\|\\mathbf{w}\\|_1 = 1 \\right\\}.\\) Practically this means that the post-intervention predictions will never be greater than the maximum outcome of the donor pool or lower than the minimum outcome of the donor pool."
  },
  {
    "objectID": "fscm.html#step-1",
    "href": "fscm.html#step-1",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 1",
    "text": "Step 1\nFS proceeds over \\(K \\in \\mathbb{N}\\) iterations, builds a sequence of tuples, \\(\\mathbb{T} = \\{(\\mathcal{S}_K, \\text{MSE}_K) \\}_{K=1}^{N_0}\\). The tuple contains two elements: the selected donor set for the \\(K\\)-th iteration and its corresponding \\(\\text{MSE}_K\\) (or the pre-treatment mean squared error). We begin by minimizing the SCM objective function as above, cycling through each donor unit vector one at a time instead of using the full control group. We denote these as submodels, which returns \\(N_0\\) one unit SCM models. We choose the single donor unit (the nearest neighbor in this specific case) that minimizes the MSE among all the \\(N_0\\) submodels. Our first tuple, then, is built with this single donor unit and the model’s corresponding MSE\n\\[\n\\mathcal{S}_1 = \\{j^\\ast\\}, \\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0}{\\operatorname*{argmin}} \\ \\text{MSE}(\\{j\\}).\n\\]"
  },
  {
    "objectID": "fscm.html#step-2",
    "href": "fscm.html#step-2",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 2",
    "text": "Step 2\nFor \\(K=2\\), we now estimate \\(N_0-1\\) two-unit SCMs. We include the originally selected donor along with the remaining controls, one remaining donor at a time. As above, the first and second elements of the second tuple, respectively, are\n\\[\n\\mathcal{S}_2 = \\mathcal{S}_1 \\cup \\{j^\\ast\\},\\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}_{K-1}}{\\operatorname*{argmin}} \\ \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j\\}).\n\\]\nNow, we have two tuples and two MSEs to choose from."
  },
  {
    "objectID": "fscm.html#generalizing",
    "href": "fscm.html#generalizing",
    "title": "Forward Selected Synthetic Control",
    "section": "Generalizing",
    "text": "Generalizing\nThis process generalizes, continuing for the rest of the donor pool. The general form for this algorithm, then, is\n\\[\n(\\mathcal{S}_K, \\text{MSE}_K) = (\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\}, \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\})).\n\\]\nThe algorithm continues until \\(S_K=N_0\\), when there are no more donors to add. FSCM chooses the tuple with the lowest \\(\\text{MSE}\\):\n\\[\n\\mathcal{S}^{\\ast} = \\underset{(\\mathcal{S}_K, \\text{MSE}_K) \\in \\mathbb{T}}{\\operatorname*{argmin}} \\ \\text{MSE}_K.\n\\]\nas the optimal donor set, \\(\\mathcal{S}^{\\ast}\\). Note that even within \\(\\mathcal{S}^{\\ast}\\) (as we will see below), some donors may receive zero weight in the final solution. The selected donors are just the units selected for inclusion in the donor pool in the first place, they are no guarantee of the unit having positive weight. This is in contrast to methods such as Forward Difference-in-Differences or the FS panel data method. Both of these designs are available in mlsynth too, in the FDID class and PDA class with the method of fs (the default). The main difference here is that FDID can never overfit because it estimates only one parameter, whereas (in theory) FSCM and fsPDA can overfit if they end up including too many parameters in the regression model. Unclear how likely this is, since as we see below, teh FS method reduces the full donor pool to just under half of the originally selected donor units."
  },
  {
    "objectID": "fscm.html#fscm-in-mlsynth",
    "href": "fscm.html#fscm-in-mlsynth",
    "title": "Forward Selected Synthetic Control",
    "section": "FSCM in mlsynth",
    "text": "FSCM in mlsynth\nNow I will give an example of how to use FSCM for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nAnd then we load the Proposition 99 dataset and fit the model in the ususal mlsynth fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.\n\nimport pandas as pd # To work with panel data\n\nfrom IPython.display import display, Markdown # To create the table\n\nfrom mlsynth import FSCM # The method of interest\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\n# Feel free to change \"smoking\" with \"basque\" above in the URL\n\ndata = pd.read_csv(url)\n\n# Our method inputs\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = FSCM(config).fit()\n\n\n\n\n\n\n\n\nAfter estimation, we can get the weights. These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the SC, with only 6 being assigned positive weight. The ATT of Prop 99 as estimated by FSCM is -19.51 and the pre-treatment Root Mean Squared Error for FSCM is 1.66. I compared these results to the same results we get in Stata, which includes the covariates that Abadie, Diamond, and Hainmuller originally adjusted for as well as customizes the period over which to minimize the MSE. The ATT using the original method is -19.0018, and the RMSE for the pre-treatment period is 1.76. The corresponding weights using the full donor pool are 0.334 for Utah, 0.235 for Nevada, 0.2020 for Montana, Colorado 0.161, and Connecticut 0.068. So as we can see, the ATTs are very similar, and the pre-treatment prediction errors are pretty much the same. When we estimate this in Stata (omitting the auxilary covariate predictors and estimate synth2 cigsale cigsale(1988) cigsale(1980) cigsale(1975) , trunit(3) trperiod(1989) xperiod(1980(1)1988) nested fig, we get a RMSE of 4.33 and an ATT of -22.88. Furthermore, with this specification, the weights are no longer a sparse vector.\nThe point of this article is very simple. The original SCM works well, however it can be very sensitive to the inclusion of covariates, which covariates are included, what their lags are, and so on and so forth. Furthermore, there is also an issue of covariate selection in settings where we have multiple covariates that can potentially inform our selection of the donor pool. Furthermore, collecting a rich list of covariates may also not be possible in some settings. In such situations, especially without some pre-existing grount truth donor pool, analysts may apply the FSCM algorithm to guard against interpolation biases.\nAt least with the California example (and West Germany and Basque datasets, which I also tested), we can sometimes get comparable results to the baseline estimates which used multiple covariates for acceptable results (in all three of the standard test cases, FSCM actually get lower MSE than the original applications). In the Proposition 99 example, we select some of the same donor units, get a slightly better MSE and a very similar ATT without needing to fit to the covariates originally specified in the JASA paper.\nThe promise of machine-learning methods in this space is to automate away donor/predictor selection to some acceptable degree. The key thing of interest (for me, from an econometric theory perspective anyways) is which methods are best suited for this task, when do they perform well, and why. For example, it might be useful to derive bias bounds for this estimator to quantify how much the MSE should improve by compared to the original SCM and Forward DID, as has been done with clustering based methods, for example.\nA final caveat: in the original paper, Giovanni uses cross-validation to estimate this model, and he also employs the same covariates. I have not done the cross validation yet on my end, but I will very soon. As ususal, email me with questions or comments."
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we’re scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it’s just “scrape”, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that’s the language I use, but I’m certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don’t add them and then the job ends (this is what happens if I try to run the action after it’s ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I’m updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I’m scraping (unless you’re a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I’m home to personally oversee it. The value here is that I’ve manually gotten my computer to do a specific task every single day, the correct way (assuming you’ve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA’s site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it’s worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you’re a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we’ve automated our tasks correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Analysis, Data Science, and Causal Inference",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat Are We Weighting For?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nAug 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lfe1.html",
    "href": "lfe1.html",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "",
    "text": "What is an applied econometrician or data scientist? At its simplest, both are practitioners of statistics who apply econometric or statistical theory to answer questions that economists, businesses, or other organizations care about. Unless your role is narrowly defined as “standard” analytics (mostly SQL and dashboards, as far as I understand), this is true whether you work in causal inference, deep learning, computer vision, or some other branch of the field. We often talk about “delivering value” or “providing insights.” Some folks claim that deep business experience (which can be learned like other mindsets) matters more than technical knowledge, and that some data science teams “fail due to being too scientific”. But pause for a moment — look out your window. Do you see insights, or “business sense” floating around? Of course not. Insights do not exist in nature; they’re created. They are derived through data analysis. Business sense does not exist in a vacuum, it is informed by empirical observations about the real world. And what makes that analysis scientific, and business sense sensbile (not just guessing with sexed up software tools) is the theoretical source under which we arrive at these conclusions. Even if you’re not proving lemmas in your day job (I don’t), those underpinnings are exactly what allow us to trust our results. By extension, some authors describe synthetic controls as a black box function, suggesting that arcane operations under the hood spit out a result. And while that temptation is understandable, unless you’re dealing with truly complex models (like neural nets or GenAI, which themselves rest on explainable math foundations), the math behind vanilla SCM is well understood.\nThis matters even more because, as this Medium post observes, industry often values breadth and speed over theoretical depth. This is correct, but it risks overlooking how theory actually accelerates practical work. Theory for applied data scientists and econometricians (at least in the work I do) is a force multiplier. It allows you move quickly and diagnose problems/suggests improvements effectively. It makes applied work more accurate, more explainable, and more trustworthy. Treating SCM (or any model) as a black box might get you a nice graph, but what separates practitioners who are effective and practitioners who are less so is the understanding and application of theory to the business question at hand. After all, our estimates are not produced for nothing, they are meant to inform the policy/business outcomes we care about. Just because Nobody Dies If This Estimate Is Wrong is not a very comforting principle to base our analyses on.\nIn this blog post, first, I’ll compute a synthetic control by hand (how often do we do such a thing?) using calculus, KKT optimality conditions, and geometry. Then, I’ll walk through a recent real-world application, showing how theory very easily informs the way we run our models and understand our results."
  },
  {
    "objectID": "lfe1.html#calculus",
    "href": "lfe1.html#calculus",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "Calculus",
    "text": "Calculus\nCalculus is the heart of SCM, which is unsurprising given that it’s an optimization problem. However, after years of reading about SCM, I’ve never ever seen somebody solve a simple synthetic control by hand. I do this below. So, with the help of a little Boyd-ie, I went through the process of finding the weights by hand.\n\n\nSolved By Calculus\n\nWe aim to find a synthetic control as a convex combination of the donors that matches the target vector as closely as possible:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\ast} &= \\underset{\\mathbf w \\in \\mathbb{R}_{\\ge 0}^2}{\\operatorname*{argmin}} \\;\\; \\|\\mathbf y - \\mathbf Y \\mathbf w\\|_2^2 \\\\\n\\text{s.t.} \\quad & \\mathbf 1^\\top \\mathbf w = 1,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{Y} = \\begin{bmatrix} \\mathbf{y}_1 & \\mathbf{y}_2 \\end{bmatrix} = \\begin{bmatrix} 20 & 48 \\\\ 22 & 50 \\end{bmatrix}\\).\nThe synthetic control is a weighted average:\n\\[\n\\hat{\\mathbf{y}} = w_1 \\mathbf{y}_1 + w_2 \\mathbf{y}_2, \n\\quad w_1, w_2 \\geq 0, \\quad w_1 + w_2 = 1.\n\\]\nof control units. Specifically, it is a convex combination. A convex combination of a set of vectors \\(\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_n \\in \\mathbb{R}^N\\) is any vector of the form\n\\[\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i \\mathbf{w}_i \\quad \\text{subject to} \\quad \\alpha_i \\ge 0 \\text{ for all } i = 1, \\dots, n, \\quad \\sum_{i=1}^n \\alpha_i = 1.\n\\]\nNotice how in this case two donor units. From the definition above, it helps us to define one weight in terms of the other; for example, if the optimal value for one weight is 0.2, the other one must be 0.8.\nSubstituting \\(w_2 = 1 - w_1\\):\n\\[\n\\hat{\\mathbf{y}}(w_1) = w_1 \\mathbf{y}_1 + (1 - w_1)\\mathbf{y}_2 \n= \\mathbf{y}_2 + w_1(\\mathbf{y}_1 - \\mathbf{y}_2),\n\\]\nand since \\(\\mathbf{y}_1 - \\mathbf{y}_2 = \\begin{bmatrix}-28 \\\\ -28\\end{bmatrix}\\), we have\n\\[\n\\hat{\\mathbf{y}}(w_1) = \\begin{bmatrix} 48 \\\\ 50 \\end{bmatrix} + w_1 \\begin{bmatrix}-28 \\\\ -28\\end{bmatrix}\n= \\begin{bmatrix} 48 - 28 w_1 \\\\ 50 - 28 w_1 \\end{bmatrix}.\n\\]\nThis is just the predicted outcome if we put weight \\(w_1\\) on donor 1 and weight \\(1-w_1\\) on donor 2 (again, defining weight 1 in terms of weight 2). By definition, the residuals are the differences between the treated outcomes and the synthetic prediction:\n\\[\n\\mathbf{r}(w_1) = \\mathbf{y} - \\hat{\\mathbf{y}}(w_1).\n\\]\nPlugging in:\n\\[\n\\mathbf{r}(w_1) = \n\\begin{bmatrix} 50 \\\\ 60 \\end{bmatrix} - \n\\begin{bmatrix} 48 - 28 w_1 \\\\ 50 - 28 w_1 \\end{bmatrix}.\n\\]\nSo:\n\\[\n\\mathbf{r}(w_1) = \\begin{bmatrix} 2 + 28w_1 \\\\ 10 + 28w_1 \\end{bmatrix}.\n\\]\n\nLet’s define the main calculus rules we’ll use:\n\nConstant Multiple Rule. If \\(f(x) = c \\cdot g(x)\\), where \\(c\\) is a constant and \\(g(x)\\) is differentiable, then the derivative of \\(f\\) with respect to \\(x\\) is:\n\\[\nf'(x) = c \\cdot g'(x).\n\\]\nThis rule allows you to pull constants out of derivatives.\n\n\nSum Rule. If \\(f(x) = g(x) + h(x)\\), where \\(g(x)\\) and \\(h(x)\\) are differentiable functions, then the derivative of \\(f\\) with respect to \\(x\\) is the sum of the derivatives of the individual functions:\n\\[\nf'(x) = g'(x) + h'(x).\n\\]\nThis rule allows us to differentiate each term of a sum separately and then add the results.\n\n\nChain Rule. If a function is composed as \\(f(x) = g(h(x))\\), where \\(h(x)\\) is the inner function and \\(g(u)\\) is the outer function (with \\(u = h(x)\\)), then the derivative of \\(f(x)\\) with respect to \\(x\\) is:\n\\[\nf'(x) = g'(h(x)) \\cdot h'(x).\n\\]\nDifferentiate the outer function with respect to the inner function, then multiply by the derivative of the inner function with respect to \\(x\\).\n\nTo find the minimizing weight, we differentiate \\(f(w_1)\\) with respect to \\(w_1\\).\nSince \\(f(w_1)\\) is a sum of two terms, the linearity of the derivative (sum rule) allows us to differentiate each term separately and then sum the results:\n\\[\nf(w_1) = (2 + 28 w_1)^2 + (10 + 28 w_1)^2\n\\]\nTo find the minimizing weight, we differentiate \\(f(w_1)\\) with respect to \\(w_1\\). Since \\(f(w_1)\\) is a sum of two terms, we can differentiate each term separately and then sum the results, according to the linearity of the derivative.\n\nLet’s begin with the first term: \\((2 + 28 w_1)^2\\). We recognize this as a composition of two functions, so we apply the chain rule. Let\n\\[\nh_1(w_1) = 2 + 28 w_1 \\quad \\text{(inner function)}, \\quad g_1(u) = u^2 \\quad \\text{(outer function)}.\n\\]\nNow we compute the derivatives:\n\\[\n\\frac{\\mathrm{d} g_1}{\\mathrm{d} u} = 2 u, \\quad \\frac{\\mathrm{d} h_1}{\\mathrm{d} w_1} = 28.\n\\]\nThe first result comes from the power rule applied to the outer function, and the second comes from differentiating the linear inner function. Applying the chain rule, we obtain\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d} w_1} (2 + 28 w_1)^2 = \\frac{\\mathrm{d} g_1}{\\mathrm{d} u} \\bigg|_{u = h_1(w_1)} \\cdot \\frac{\\mathrm{d} h_1}{\\mathrm{d} w_1} = 2 (2 + 28 w_1) \\cdot 28.\n\\]\n\nNext, consider the second term: \\((10 + 28 w_1)^2\\). Similarly, let\n\\[\nh_2(w_1) = 10 + 28 w_1, \\quad g_2(v) = v^2.\n\\]\nThe derivatives are\n\\[\n\\frac{\\mathrm{d} g_2}{\\mathrm{d} v} = 2 v, \\quad \\frac{\\mathrm{d} h_2}{\\mathrm{d} w_1} = 28.\n\\]\nAgain, these results come from the power rule for the quadratic outside function and the linear term inside the parentheses. By the chain rule, the derivative of the second term is\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d} w_1} (10 + 28 w_1)^2 = 2 (10 + 28 w_1) \\cdot 28.\n\\]\nCombining the two terms by the sum rule, the derivative of the full objective function is\n\\[\n\\frac{\\mathrm{d} f}{\\mathrm{d} w_1} = 2 (2 + 28 w_1) \\cdot 28 + 2 (10 + 28 w_1) \\cdot 28.\n\\]\n\nNow we have our first order conditions. We start by setting the derivative to zero\n\\[\n2 (2 + 28 w_1) \\cdot 28 + 2 (10 + 28 w_1) \\cdot 28 = 0.\n\\]\nWe can factor out \\(2 \\cdot 28\\):\n\\[\n2 \\cdot 28 \\left[ (2 + 28 w_1) + (10 + 28 w_1) \\right] = 0.\n\\]\nSimplifying inside the brackets gives\n\\[\n(2 + 28 w_1) + (10 + 28 w_1) = 12 + 56 w_1,\n\\]\nso the equation becomes\n\\[\n2 \\cdot 28 \\cdot (12 + 56 w_1) = 0.\n\\]\nSince \\(2 \\cdot 28 \\neq 0\\), we can safely divide both sides of the equation by this constant factor 56, isolating the parentheses that contains our variable:\n\\[\n12 + 56 w_1 = 0.\n\\]\nWe see that we can simplify this with the greatest common factor. Dividing both terms by 4 yields\n\\[\n\\frac{12}{4} + \\frac{56}{4} w_1 = 0 \\quad \\implies \\quad 3 + 14 w_1 = 0.\n\\]\nFinally, solving for \\(w_1\\), we obtain\n\\[\n14 w_1 = -3\n\\]\nand finally\n\\[\nw_1 = -\\frac{3}{14}.\n\\]\nThis is the unconstrained minimizer of \\(f(w_1)\\). However, it lies outside the feasible interval \\([0,1]\\) for the weights. The constrained minimum is therefore at the nearest boundary of the feasible set, which is \\(w_1 = 0\\). Correspondingly, \\(w_2 = 1\\). Evaluating the objective at the boundary points:\n\\[\nf(0) = 2^2 + 10^2 = 104, \\quad f(1) = 30^2 + 38^2 = 2344.\n\\]\nThe minimum occurs at \\(w_1 = 0, w_2 = 1\\), giving the synthetic control\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{y}_2 = \\begin{bmatrix} 48 \\\\ 50 \\end{bmatrix}.\n\\]\n\nThe reason I chose a corner solution is because I wanted to think about the circumstnaces uner which the model will not work. Solving for the optimal and yet non-ideal answer provides interesting insights as to how we can think about applyinng SCM in practice, as I will show below."
  },
  {
    "objectID": "lfe1.html#kkt-derivation",
    "href": "lfe1.html#kkt-derivation",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "KKT Derivation",
    "text": "KKT Derivation\nThe Karush-Kuhn-Tucker method formalizes what the calculus argument already suggested: because the unconstrained minimizer gives a negative weight, the constrained solution must sit on the boundary of the feasible set. KKT is useful here because it provides a systematic way to check all the optimality conditions at once—stationarity, feasibility, dual feasibility, and complementary slackness. This guarantees that the solution we find is not just locally valid but globally optimal for a convex quadratic problem. In this example, KKT confirms that the active constraint is \\(w_1 \\ge 0\\), leading to the boundary solution \\(w^\\ast = (0,1)\\).\n\n\nVerified With KKT\n\nTo form the KKT system, introduce a scalar Lagrange multiplier \\(\\lambda\\) for the equality constraint \\(w_1 + w_2 = 1\\) and nonnegative multipliers \\(\\mu_1, \\mu_2 \\ge 0\\) for the inequality constraints \\(w_1 \\ge 0\\), \\(w_2 \\ge 0\\). The Lagrangian is\n\\[\n\\mathcal{L}(\\mathbf{w}, \\lambda, \\mu) = \\|\\mathbf{y} - \\mathbf{Y}\\mathbf{w}\\|_2^2 + \\lambda (w_1 + w_2 - 1) - \\mu_1 w_1 - \\mu_2 w_2.\n\\]\nThe negative sign in the terms \\(-\\mu_i w_i\\) is a sign convention chosen so that the multipliers \\(\\mu_i\\) are constrained to be nonnegative; this makes the statement of dual feasibility and complementary slackness straightforward. Stationarity means the gradient of the Lagrangian with respect to the primal variables vanishes. Differentiating the quadratic objective gives \n\\[\n\\nabla_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{Y}\\mathbf{w}\\|_2^2 = 2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w} - \\mathbf{y}),\n\\] \nso stationarity requires\n\\[\n2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{1} - \\mu = \\mathbf{0},\n\\]\nwhere \\(\\mathbf{1} = (1,1)^\\top\\) and \\(\\mu = (\\mu_1, \\mu_2)^\\top\\). Primal feasibility is the requirement that the candidate weight vector satisfy the original constraints \\(w_1 \\ge 0\\), \\(w_2 \\ge 0\\), \\(w_1 + w_2 = 1\\). Dual feasibility means the multipliers associated with inequality constraints obey \\(\\mu_1 \\ge 0\\), \\(\\mu_2 \\ge 0\\). Complementary slackness ties the primal and dual together: for each inequality constraint, the product of the multiplier and the primal slack is zero, so \\(\\mu_i w_i = 0\\) for \\(i = 1, 2\\). In words, complementary slackness says that if a constraint is active (the primal variable is on the boundary) its multiplier may be positive, and if the constraint is slack (strictly satisfied) the corresponding multiplier must be zero.\nFrom calculus, we expect the constrained minimizer to be \\(\\mathbf{w}^{\\ast} = (0,1)^\\top\\), which corresponds to taking donor two alone as the synthetic control. Verifying this candidate requires computing the residual at that point and solving the stationarity equations for the multipliers. At \\(\\mathbf{w}^{\\ast}\\) we have \\(\\mathbf{Y}\\mathbf{w}^{\\ast} = \\mathbf{y}_2 = (48,50)^\\top\\) and the residual \\(\\mathbf{Y}\\mathbf{w}^{\\ast} - \\mathbf{y} = (-2,-10)^\\top\\). Multiplying by \\(2 \\mathbf{Y}^\\top\\) yields\n\\[\n2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w}^{\\ast} - \\mathbf{y}) = 2 \\begin{bmatrix}20 & 22 \\\\ 48 & 50\\end{bmatrix} \\begin{bmatrix}-2 \\\\ -10\\end{bmatrix} = 2 \\begin{bmatrix}-260 \\\\ -596\\end{bmatrix} = \\begin{bmatrix}-520 \\\\ -1192\\end{bmatrix}.\n\\]\nPlugging this into the stationarity condition gives the two scalar equations\n\\[\n-520 + \\lambda - \\mu_1 = 0, \\qquad -1192 + \\lambda - \\mu_2 = 0.\n\\]\nComplementary slackness applied to the candidate \\(w_1^{\\ast} = 0, w_2^{\\ast} = 1\\) implies \\(\\mu_2 = 0\\) because \\(w_2^{\\ast} &gt; 0\\) forces the corresponding multiplier to vanish, while \\(\\mu_1\\) is unconstrained by slackness other than being nonnegative. Using \\(\\mu_2 = 0\\) in the second scalar equation yields \\(\\lambda = 1192\\). Substituting \\(\\lambda = 1192\\) into the first equation gives \\(\\mu_1 = 672\\), which satisfies dual feasibility since \\(672 \\ge 0\\). Thus stationarity, primal feasibility, dual feasibility, and complementary slackness all hold for the candidate, and because the objective is convex (its Hessian is \\(2 \\mathbf{Y}^\\top \\mathbf{Y}\\), which is positive semidefinite) these KKT conditions guarantee global optimality. Therefore \\(\\mathbf{w}^{\\ast} = (0,1)^\\top\\) is the solution, the residual is \\(\\mathbf{y} - \\hat{\\mathbf{y}} = (2,10)^\\top\\), the Lagrange multiplier for the equality constraint is \\(\\lambda = 1192\\), and the dual variables are \\(\\mu_1 = 672\\), \\(\\mu_2 = 0\\).\n\nTo summarize in words: we formed the Lagrangian to incorporate constraints into the objective, set its gradient with respect to the primal variables to zero (stationarity), required the primal variables to satisfy the original constraints (primal feasibility), required inequality multipliers to be nonnegative (dual feasibility), and enforced that no multiplier is positive while its corresponding primal variable is strictly positive (complementary slackness). Solving these conditions gave the boundary solution \\(w_1 = 0, w_2 = 1\\), which is the projection of \\(\\mathbf{y}\\) onto the convex hull of the donors when the unconstrained projection lies outside that hull. The KKT verification thus formalizes what we already knew: the now extreme your target unit is, the more your SCM will match to the nearest neighbor(s) in the donor pool."
  },
  {
    "objectID": "lfe1.html#frankwolfe-geometric-solution-and-hilbert-projection",
    "href": "lfe1.html#frankwolfe-geometric-solution-and-hilbert-projection",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "Frank–Wolfe Geometric Solution and Hilbert Projection",
    "text": "Frank–Wolfe Geometric Solution and Hilbert Projection\nFor a final expression, we can also arrive at the solution via pure geometry.\n\n\nSolved Via Frank-Wolfe\n\nWe seek to approximate\n\\[\n\\mathbf{y} = \\begin{bmatrix}50\\\\60\\end{bmatrix}\n\\]\nusing a convex combination of donors\n\\[\n\\mathbf{y}_1 = \\begin{bmatrix}20\\\\22\\end{bmatrix}, \\quad\n\\mathbf{y}_2 = \\begin{bmatrix}48\\\\50\\end{bmatrix}.\n\\]\nThe feasible set of synthetic predictions is the convex hull\n\\[\nC = \\operatorname{conv}\\{\\mathbf{y}_1, \\mathbf{y}_2\\},\n\\]\nwhich is the line segment connecting the two donors. Minimizing the squared Euclidean distance\n\\[\nf(\\hat{\\mathbf{y}}) = \\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|_2^2\n\\]\nis equivalent to projecting \\(\\mathbf{y}\\) onto this convex hull.\nTo give ourselves a frameowrk to work with we can employ the Hilbert Projection Theorem. This states that if \\(C \\subset \\mathbb{R}^n\\) is a nonempty, closed, convex set and \\(\\mathbf{y} \\in \\mathbb{R}^n\\), then there exists a unique point \\(\\hat{\\mathbf{y}} \\in C\\) such that\n\\[\n\\hat{\\mathbf{y}} = \\underset{\\mathbf{z} \\in C}{\\operatorname{argmin}} \\|\\mathbf{y} - \\mathbf{z}\\|_2,\n\\]\nand the residual \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}\\) satisfies the variational inequality\n\\[\n\\langle \\mathbf{r}, \\mathbf{z} - \\hat{\\mathbf{y}} \\rangle \\le 0 \\quad \\forall \\mathbf{z} \\in C.\n\\]\nThis inequality expresses the geometric fact that the residual forms an obtuse or right angle with any vector pointing from the projection to a feasible point in \\(C\\); in other words, one cannot move from the projection along any direction in \\(C\\) and reduce the distance to \\(\\mathbf{y}\\).\nApplying this to our two-donor synthetic control problem, the convex hull is the line segment connecting \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\), and the point \\(\\mathbf{y}\\) lies outside this segment. Computing the Euclidean distances yields \\(\\|\\mathbf{y} - \\mathbf{y}_1\\|^2 = 2344\\) and \\(\\|\\mathbf{y} - \\mathbf{y}_2\\|^2 = 104\\), so the closest donor is \\(\\mathbf{y}_2\\), which we take as the initial point \\(x^{(0)}\\) in a Frank–Wolfe iteration with corresponding weights \\(w^{(0)} = (0,1)\\). The gradient of the objective at \\(x^{(0)}\\) is \\(\\nabla f(x^{(0)}) = x^{(0)} - \\mathbf{y} = \\begin{bmatrix}-2\\\\-10\\end{bmatrix}\\). The Frank–Wolfe algorithm then searches for a vertex \\(s \\in \\{\\mathbf{y}_1, \\mathbf{y}_2\\}\\) minimizing the inner product \\(\\langle \\nabla f(x^{(0)}), s \\rangle\\). Computing \\(\\langle \\nabla f(x^{(0)}), \\mathbf{y}_1 \\rangle = -260\\) and \\(\\langle \\nabla f(x^{(0)}), \\mathbf{y}_2 \\rangle = -596\\) shows that the minimum occurs at \\(\\mathbf{y}_2\\), meaning the search direction points toward the vertex we are already at. Attempting to move toward \\(\\mathbf{y}_1\\) along the direction \\(d = \\mathbf{y}_1 - x^{(0)} = \\begin{bmatrix}-28\\\\-28\\end{bmatrix}\\) gives an optimal unconstrained step\n\\[\n\\gamma^{\\ast} = \\frac{\\langle \\mathbf{y}-x^{(0)}, d \\rangle}{\\|d\\|^2} = -0.214,\n\\]\nwhich lies outside the feasible range \\([0,1]\\). Clamping to \\(\\gamma = 0\\) results in the update \\(x^{(1)} = x^{(0)} = \\mathbf{y}_2\\), confirming that the closest feasible point is the boundary vertex \\(\\mathbf{y}_2\\). The residual is therefore \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\begin{bmatrix}2\\\\10\\end{bmatrix}\\), which satisfies the Hilbert variational inequality because it forms an obtuse angle with the direction toward \\(\\mathbf{y}_1\\).\nThis geometric reasoning mirrors the calculus derivation. In the unconstrained formulation, the minimizer of\n\\[\nf(w_1) = \\|\\mathbf{y} - (w_1 \\mathbf{y}_1 + (1-w_1) \\mathbf{y}_2)\\|^2\n\\]\noccurs at \\(w_1 = -3/14\\), which is outside the feasible interval \\([0,1]\\). By the Hilbert Projection Theorem, the optimal feasible point must lie on the nearest boundary, which corresponds to \\(w_1 = 0, w_2 = 1\\). The gradient computation in calculus signals that the slope of the objective points toward the infeasible region, exactly reflecting the fact that the residual cannot be reduced by moving along the feasible set.\nThe KKT derivation provides an algebraic realization of the same geometric principle. The stationarity condition\n\\[\n2\\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w}-\\mathbf{y}) + \\lambda \\mathbf{1} - \\mu = 0,\n\\]\ntogether with complementary slackness \\(\\mu_i w_i = 0\\) and dual feasibility \\(\\mu_i \\ge 0\\), ensures that the residual \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}\\) is orthogonal to any feasible direction in the convex hull, encoding precisely the variational inequality from the Hilbert Projection Theorem. Active inequality constraints correspond to nonzero multipliers, signaling that the projection lies on the boundary, while slack constraints have zero multipliers, corresponding to interior directions where the unconstrained minimum is feasible. In this example, the boundary solution \\(w_1 = 0, w_2 = 1\\) with residual \\(\\mathbf{r} = (2,10)^\\top\\) and Lagrange multipliers \\(\\lambda = 1192, \\mu_1 = 672, \\mu_2 = 0\\) satisfies all KKT conditions and simultaneously fulfills the Hilbert projection criterion.\n\nThe Frank–Wolfe geometric approach, the calculus, and the KKT system are three perspectives on the same underlying fact: the optimal synthetic control for the standard method is the projection of \\(\\mathbf{y}\\) onto the convex hull of donors. When the unconstrained minimizer lies outside the hull, the solution is constrained to the nearest boundary point, with the residual aligned according to the Hilbert Projection Theorem."
  },
  {
    "objectID": "lfe1.html#a-visuzalization-in-python",
    "href": "lfe1.html#a-visuzalization-in-python",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "A Visuzalization in Python",
    "text": "A Visuzalization in Python\nAgain, none of this is voodoo, just a lot of hairy math.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\n\n# =====================\n# Step 0: Setup\n# =====================\n# Treated unit\nx = np.array([50, 60])\n\n# Donors\nd1 = np.array([20, 22])\nd2 = np.array([48, 50])\ndonors = np.vstack([d1, d2])\ndonors_cvx = np.column_stack([d1, d2])\nn_donors = donors_cvx.shape[1]\n\nw = cp.Variable(n_donors, nonneg=True)\n\nobjective = cp.Minimize(cp.sum_squares(x - donors_cvx @ w))\n\nconstraints = [cp.sum(w) == 1]\nprob = cp.Problem(objective, constraints)\nprob.solve()\n\nx_hat_cvx = donors_cvx @ w.value\n\n# Time steps\nt = np.arange(len(x))\n\n# =====================\n# Plot as subfigures\n# =====================\nfig, axs = plt.subplots(1, 2, figsize=(12,5))\n\n# --- Subplot 1: 2D feature plot ---\naxs[0].plot(d1[0], d1[1], 'bo', label='Donor 1')\naxs[0].plot(d2[0], d2[1], 'go', label='Donor 2')\naxs[0].plot(x[0], x[1], 'r*', markersize=12, label='Treated Unit')\naxs[0].plot(x_hat_cvx[0], x_hat_cvx[1], 'ms', markersize=10, label='Synthetic Control')\n\n# Convex hull as a dashed line\naxs[0].plot([d1[0], d2[0]], [d1[1], d2[1]], 'k-.', label='Convex Hull')\n\n# Add literal rectangle around convex hull\nx_min, x_max = np.min(donors[:,0]), np.max(donors[:,0])\ny_min, y_max = np.min(donors[:,1]), np.max(donors[:,1])\nrect = plt.Rectangle((x_min, y_min), x_max-x_min, y_max-y_min,\n                     linewidth=1, edgecolor='gray', facecolor='none', linestyle='--', label='Hull Bounding Box')\naxs[0].add_patch(rect)\n\naxs[0].set_xlabel('Feature 1')\naxs[0].set_ylabel('Feature 2')\naxs[0].legend()\naxs[0].grid(True)\naxs[0].set_title('2D Feature Space')\n\n# --- Subplot 2: Time series plot ---\naxs[1].plot(t, x, 'r*-', label='Treated Unit', markersize=10)\naxs[1].plot(t, d1, 'bo-', label='Donor 1')\naxs[1].plot(t, d2, 'go-', label='Donor 2')\naxs[1].plot(t, x_hat_cvx, 'ms-', label='Synthetic Control', markersize=8)\n\n# Convex hull shading\ndonor_min = np.min(donors, axis=0)\ndonor_max = np.max(donors, axis=0)\naxs[1].fill_between(t, donor_min, donor_max, color='gray', alpha=0.2, label='Donor Convex Hull')\n\naxs[1].set_xlabel('Time')\naxs[1].set_ylabel('Outcome')\naxs[1].legend()\naxs[1].grid(True)\naxs[1].set_title('Time Series with Convex Hull Highlight')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also show how the error changes, given a change in \\(w_1\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Treated unit\ny = np.array([50, 60])\n\n# Donors\ny1 = np.array([20, 22])\ny2 = np.array([48, 50])\n\n# Compute synthetic control at the constrained minimum (w1=0, w2=1)\nw1_min = 0\nw2_min = 1\ny_hat_min = w1_min*y1 + w2_min*y2\n\n# Compute residual\nresidual = y - y_hat_min\n\n# Generate a range of w1 values for objective\nw1_vals = np.linspace(0, 1, 200)\nw2_vals = 1 - w1_vals\nf_vals = np.array([np.sum((y - (w1*y1 + w2*y2))**2) for w1, w2 in zip(w1_vals, w2_vals)])\n\n# Plot\nfig, axs = plt.subplots(1, 2, figsize=(12,5))\n\n# --- Subplot 1: 2D feature space with residual ---\naxs[0].plot(y1[0], y1[1], 'bo', label='Donor 1')\naxs[0].plot(y2[0], y2[1], 'go', label='Donor 2')\naxs[0].plot(y[0], y[1], 'r*', markersize=12, label='Treated Unit')\naxs[0].plot(y_hat_min[0], y_hat_min[1], 'ms', markersize=10, label='Synthetic Control')\n\n# Convex hull line\naxs[0].plot([y1[0], y2[0]], [y1[1], y2[1]], 'k-.', label='Convex Hull')\n\n# Draw residual vector from synthetic control to treated unit\naxs[0].arrow(y_hat_min[0], y_hat_min[1], residual[0], residual[1],\n             head_width=0.8, head_length=1.0, fc='orange', ec='orange', linewidth=2, label='Residual')\n\naxs[0].set_xlabel('Feature 1')\naxs[0].set_ylabel('Feature 2')\naxs[0].legend()\naxs[0].grid(True)\naxs[0].set_title('2D Feature Space with Residual')\n\n# --- Subplot 2: Objective function f(w1) ---\naxs[1].plot(w1_vals, f_vals, 'b-', label=r'$f(w_1) = ||y - \\hat{y}||^2$')\naxs[1].plot(w1_min, f_vals[0], 'ro', label='Minimum (constrained)')\naxs[1].axvline(x=w1_min, color='r', linestyle='--')\naxs[1].set_xlabel(r'$w_1$')\naxs[1].set_ylabel('Objective value')\naxs[1].legend()\naxs[1].grid(True)\naxs[1].set_title('Objective Function vs. $w_1$')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom this figure, we see what we already solved for. Any weight for \\(w_1\\) greater than 0 increases our error solution for our objective function.\nAlready, some of you who got this far will be asking “Jared, why did you go through the trouble of showing us all this? Who cares about the KKT conditions or the Hilbert Space Projection or any of those ideas? I’ll never literally do this at work will I, so why bother showing me any of this?” And my answer is “That is correct, you will not need to do any of this yourself, ever. However, the moment you intuit SCM in these terms, you can oftentimes get a good read as to how an analysis will look before you even run a single SCM model.” Not always, but sometimes. I have worked in industry as consultant for 5 months now, and literally, many problems that people have with the weights or the predictions etc. would literally (without exaggeration) be solved by simple line ploy. Seriously, you’ll discover a lot by just plotting your units out. We’re about to see why this is true in the content below."
  },
  {
    "objectID": "lfe1.html#the-two-step-synthetic-control",
    "href": "lfe1.html#the-two-step-synthetic-control",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "The Two-Step Synthetic Control",
    "text": "The Two-Step Synthetic Control\nI thought to myself “Yeah this is definitely gonna need an intercept of some kind!!! Couldn’t fit the treated vector otherwise.” So, I ran the Two-Step SCM. The TSSC method was developed in part by one of my favorite econometricians, Kathy Li. The method is predicated on a simple idea: Sometimes treated units are simply too extreme relative to their donor pool, such as Australia. When this happens, vanilla SCM can struggle, and analysts may need to allow for intercepts or relax the non-negativity constraint. The challenge, however, is that it’s rarely obvious ex ante which modification is appropriate.\nTSSC chooses between a few candidate estimators. First is MSCa: Intercept, Convex Hull\n\\[\n\\underset{\\mathbf{w},\\,\\beta}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} - \\beta \\mathbf{1} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0, \\; \\mathbf{1}^\\top \\mathbf{w} = 1, \\; \\beta \\in \\mathbb{R}.\n\\]\nThis is SCM with an intercept, which shifts the treated unit vertically inside the convex hull. We also have MSCb: No Intercept, Nonnegative Weights\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0.\n\\]\nThis drops the intercept but allows weights to be any nonnegative values (not required to sum to one). Geometrically, the treated unit is projected onto the convex cone generated by the donors. Finally, we have MSCc: Intercept + Nonnegative Weights\n\\[\n\\underset{\\mathbf{w},\\,\\beta}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} - \\beta \\mathbf{1} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0.\n\\]\nThis combines both relaxations: an intercept plus nonnegative weights. The treated unit is projected onto a shifted convex cone, which is especially useful when the treated unit differs systematically in slope or level from the donor pool.\nTSSC begins by testing the joint null hypothesis that both restrictions are valid:\n\\[\nH_0: \\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\quad \\text{and} \\quad \\beta = 0\n\\]\nagainst the alternative that at least one fails. If the null is rejected, each restriction is tested separately, yielding up to three possible modifications (MSCa, MSCb, or MSCc).\nThe test statistic compares the restricted SCM solution against the most flexible specification, MSCc. Let \\((\\hat{\\mathbf{w}}^{SC}, \\hat{\\beta}^{SC})\\) be the restricted solution and \\((\\hat{\\mathbf{w}}^{MSCc}, \\hat{\\beta}^{MSCc})\\) the flexible one. The statistic takes the form\n\\[\nT = (\\hat{\\theta}^{SC} - \\hat{\\theta}^{MSCc})^\\top \\, \\hat{\\Sigma}^{-1} \\, (\\hat{\\theta}^{SC} - \\hat{\\theta}^{MSCc}),\n\\]\nwhere \\(\\hat{\\theta}\\) stacks the estimated weights and intercept, and \\(\\hat{\\Sigma}\\) is a covariance estimate obtained by subsampling the pre-treatment period. Intuitively, \\(T\\) measures how far the SCM solution lies from the flexible cone-based solution, scaled by sampling variability. If \\(T\\) is small, the convex-hull assumption holds; if large, the data support a more flexible geometry."
  },
  {
    "objectID": "lfe1.html#connecting-the-theory-to-practice",
    "href": "lfe1.html#connecting-the-theory-to-practice",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "Connecting the Theory to Practice",
    "text": "Connecting the Theory to Practice\nGeometrically, adding an intercept in TSSC shifts the treated vector vertically into the interior of the convex hull formed by the donor pool. In Hilbert space terms, this expands the feasible set for the projection: rather than projecting a point outside or on the boundary, the algorithm now projects a point inside the convex hull. This allows the synthetic control weights to spread across multiple donors rather than collapsing to a corner solution. From the KKT perspective, the intercept introduces an additional degree of freedom, modifying the constraints but still satisfying stationarity, dual feasibility, and complementary slackness on the adjusted feasible set. Consequently, TSSC can produce a solution where multiple donors receive meaningful weight, resolving the boundary limitations of standard SCM."
  },
  {
    "objectID": "lfe1.html#results",
    "href": "lfe1.html#results",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes",
    "section": "Results",
    "text": "Results\nHere is the result we get when we fit the TSSC estimator to the Australia case. As usual, to follow along you’ll need to do\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\n\nTSSC_res = TSSC(config).fit()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# SCM / TSSC results\nresults_df = pd.DataFrame({\n    \"Method\": [\"SCM\", \"MSCc\"],\n    \"ATT\": [31.049, -30.713],\n    \"Intercept\": [0.000, 62.872],\n    \"Key Weights\": [\n        \"Italy: 0.815, Norway: 0.185\",\n        \"Norway: 0.089, South Africa: 0.288\"\n    ]\n})\n\n# Convert to Markdown table\nmd_table = results_df.to_markdown(index=False).split('\\n')\n\n# Adjust alignment: left for Method/Key Weights, center for numbers\nmd_table[1] = \"|:------------|:------:|:--------:|:---------------------------|\"\n\n# Display nicely in notebook\ndisplay(Markdown('\\n'.join(md_table)))\n\n\n\n\n\n\n\n\n\n\nMethod\nATT\nIntercept\nKey Weights\n\n\n\n\nSCM\n31.049\n0\nItaly: 0.815, Norway: 0.185\n\n\nMSCc\n-30.713\n62.872\nNorway: 0.089, South Africa: 0.288\n\n\n\n\n\nThe authors ATT is 31.049. The ATT of the MSCc is -30.713, almost the opposite of the original finding, with the intercept coefficient being 62.872 and the donors being Norway: 0.089, South Africa: 0.288. We can see here that by adding an intercept which accounts for systemic baseline differences between the target unit and the control group, we can sometimes even flip the sign of the estimated ATT completely. This demonstrates why theoretical understanding is crucial: by simple inspection of donor units versus the treated units, we can sometimes intuit how the baseline model will look before we’ve ran it, potentially anticipating the kind of SCM model we need. In this case just by looking, we already know that the donors will mainly be the nearest neighbors, suggesting that the model needs an intercept. Note that the sign reversal is not always true. Consider the Basque example:\n\nimport pandas as pd\nfrom mlsynth import TSSC\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv\"\ndata = pd.read_csv(url)\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = TSSC(config).fit()\n\n\n\n\n\n\n\n\nHere our ATT is -0.8, whereas the original SCM’s was -0.69. With an intercept of 0.583, MSCc assigns weight to Cataluna: 0.533, Madrid (Comunidad De): 0.067, Principado De Asturias: 0.115, and Rioja (La): 0.289. In contrast, Cataluna’s weight at first was 0.826, Madrid’s was 0.168, and Austurias was 0.005 (these differ a little from the 2003 paper because I do not employ covariates)."
  },
  {
    "objectID": "nsc.html",
    "href": "nsc.html",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "",
    "text": "Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where the non-linear synthetic control method comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works using the Proposition 99 dataset."
  },
  {
    "objectID": "nsc.html#optimization-problem",
    "href": "nsc.html#optimization-problem",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Optimization Problem",
    "text": "Optimization Problem\nLet \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the outcome vector for the treated unit over the pre-treatment period \\(\\mathcal{T}_1\\), and let \\(\\mathbf{Y}_0^{\\text{pre}} \\in \\mathbb{R}^{T_0 \\times N_0}\\) be the matrix of pre-treatment outcomes for the control units indexed by \\(\\mathcal{N}_0\\). We seek a weight vector \\(\\mathbf{w} \\in \\mathbb{R}^{N_0}\\) that minimizes the following objective:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{N_0}} \\quad \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\right\\|_2^2 + a \\sum_{j \\in \\mathcal{N}_0} \\delta_j |w_j| + b \\|\\mathbf{w}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}\\|_1 = 1\n\\]\nThe first term is the standard squared reconstruction error. The second term introduces a discrepancy-weighted \\(\\ell_1\\) penalty, where the discrepancy vector \\(\\boldsymbol{\\delta} \\in \\mathbb{R}^{N_0}\\) is defined by:\n\\[\n\\delta_j = \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{y}_j^{\\text{pre}} \\right\\|_2, \\quad \\forall j \\in \\mathcal{N}_0\n\\]\nThis term penalizes control units that differ more from the treated unit in pre-treatment trends. The final term is a Tikhonov regularization penalty that shrinks the weight vector toward zero to improve stability. The parameters \\(a &gt; 0\\) and \\(b &gt; 0\\) control the strength of the \\(\\ell_1\\) and \\(\\ell_2\\) penalties, respectively. The constraint \\(\\|\\mathbf{w}\\|_1 = 1\\) retains the SC to the probability simplex \\(\\Delta^{N_0 - 1}\\), forming an affine combination of the control units."
  },
  {
    "objectID": "nsc.html#cross-validation-to-tune-a-and-b",
    "href": "nsc.html#cross-validation-to-tune-a-and-b",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Cross-Validation to Tune \\(a\\) and \\(b\\)",
    "text": "Cross-Validation to Tune \\(a\\) and \\(b\\)\nTo select optimal regularization parameters \\((a, b)\\), we perform \\(k\\)-fold cross-validation using a pseudo-treated framework. Each control unit \\(j \\in \\mathcal{N}_0\\) is treated as if it were the treated unit, and we aim to reconstruct \\(\\mathbf{y}_j^{\\text{pre}}\\) from the remaining controls. For a given pseudo-treated unit \\(j\\), let \\(\\mathcal{N}_0^{(-j)} = \\mathcal{N}_0 \\setminus \\{j\\}\\) and define the donor matrix \\(\\mathbf{Y}_0^{\\text{pre}, (-j)}\\) by removing column \\(j\\) from \\(\\mathbf{Y}_0^{\\text{pre}}\\).\nFor each fold and pseudo-treated unit, we solve:\n\\[\n\\min_{\\mathbf{w}^{(j)} \\in \\mathbb{R}^{N_0 - 1}} \\quad \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)} \\right\\|_2^2 + a \\sum_{k \\in \\mathcal{N}_0^{(-j)}} \\delta_k^{(j)} |w_k^{(j)}| + b \\|\\mathbf{w}^{(j)}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}^{(j)}\\|_1 = 1\n\\]\nwhere\n\\[\n\\delta_k^{(j)} = \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{y}_k^{\\text{pre}} \\right\\|_2, \\quad \\forall k \\in \\mathcal{N}_0^{(-j)}\n\\]\nLet \\(\\widehat{\\mathbf{y}}_j^{\\text{pre}} = \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)}\\) denote the predicted outcome. The validation error for unit \\(j\\) is given by:\n\\[\n\\text{MSE}^{(j)} = T_0^{-1} \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\widehat{\\mathbf{y}}_j^{\\text{pre}} \\right\\|_2^2\n\\]\nThe cross-validated error is averaged over all pseudo-treated units and all folds. We then select:\n\\[\n(a^\\ast, b^\\ast) = \\arg\\min_{a \\in \\mathcal{A},\\, b \\in \\mathcal{B}} \\; \\text{CVError}(a, b)\n\\]\nHere, \\(k\\) indexes the donor units in the pseudo-treated optimization problem. \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are grids of candidate values. In practice, I employ sklearn’s cross-validation utilities to accelerate computation. I had to go rogue from the original approach because on my end it was computationally intensive. This modification seems to perform comparably while dramatically improving runtime. So while the empirical results will not be the same, they are about what we’d expect."
  },
  {
    "objectID": "sccare.html",
    "href": "sccare.html",
    "title": "Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?",
    "section": "",
    "text": "I was talking with someone recently about applying SCM, and I mentioned the idea of using a richer donor pool with different kinds of donors to answer a question. I had to articulate myself kind of carefully, since they were kind of puzzled about what I meant at first. For me, the idea comes very naturally to me, where we literally do things like this all the time. My intuition however comes from thinking about these models a lot (and empirical proof/evidence), and I think it may help others.\nMany people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant’Anna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.\nHowever, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like mlsynth or geolift on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantly, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory that leads them to using sub-optimal estimators or not doing basic checks. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective in the very first place. This is much more crucial because if you do not know when your methods are expected to work well, you will not know if they fail or why beyond “that does not look right”. In a sense, you are sort of blindly applying calculus and algebra without much thought as it how any of it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.\nThis is kind of a broader problem in data science in many respects. Ostensibly, there are many myths about SCM, one of which is “the myth that SCM/[Augmented] SCM[s] don’t have assumptions like parallel trends”. I do not know who believes this myth, and I am inclined to think that widespread belief of this myth… is itself a myth. After all, even your most applied economists will tell you “yeah, SCM definitely has assumptions”. They may not be able to articulate what they are as formally as they could OLS, for example, but everybody in my experience knows SCM has assumptions that need to be met. Of course, whether they verify them is another matter entirely (if you know anybody who truly holds these beliefs about SCM not having assumptions, direct them to me so I can chat with them).\nEither way, even there are not many people who confidently say/think “SCM has no assumptions”, a much better argument I think can be made that even if people do not literally think this… this is certainly the way many researchers act in practice. And that is the point of this post. Knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain. As with cars, you may not know how to build an engine, but you will be well served if you know how and when to change spark plugs or fill your tire with air.\n\nBasic SCM\nPeople often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this has been revised substantially in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or latent variable model) that often serves as a motivating data generating process. The linear factor model takes the form:\n\\[\nY_{it}(0) = \\boldsymbol{\\lambda}_t^\\top \\boldsymbol{\\mu}_i + \\varepsilon_{it}.\n\\]\nWhat does this mean? It simply means that our outcome observations are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units, plus some error term. These common factors may indeed affect each unit differently (in DID, the relationship is additive, not multiplicative), but the key idea is that we can use their similarity in terms of how they interact with the factor loadings to our advantage. Econometricians all the time tell us that SCM fundamentally is about matching our treated unit’s common factors to the common factors embedded in the donor set (also see conditons 1-6 here). This idea is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual for one unit or group using only the control units that behave similarly to the treated unit. If we get a good match (usually), we are likely also matching close to the factor loadings. This idea holds regardless of what kind of control units they are.\nA more flexible idea is the fine-grained potential outcomes model. This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit’s population.\nThe practical implication of this is that we are allowed to use different donor types (comparing cities to states, for example) on the condition that they help us learn about the trajectory of the target unit in the pre-intervention period. SCM does not care about what kind of donors you use, so long as they are informative donors.\n\n\nApplication\nUnconvinced? We can replicate some results to show this using mlsynth.\nReaders likely know the classic example of Prop 99, where California’s anti-tobacco program was compared to 38 donor states to estimate California’s counterfactual per capita cigarette consumption. But California is a ridiculously large economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units?\nIt turns out that we can do just that. Here, I compare California to the donor divisions of the United States. The outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level. California’s remains as is. Just for demonstration, I compare the standard results (around an ATT of -17) to what we get when we use other estimators, except I aggregate the outcomes as I just described. Comparing across estimators, we can visually assess how well each method captures the pre and post-treatment trajectory for California using the aggregated division data, relative to the baseline result that uses the states as donors.\n\nimport pandas as pd\nfrom mlsynth import FDID, TSSC, PDA\n\n# URL of the .dta file\nurl = \"http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta\"\n\n# Load the Stata file directly from the URL\ndf = pd.read_stata(url)\n\ndf[\"Proposition 99\"] =  ((df[\"region\"] == \"California\") & (df[\"year\"].dt.year &gt;= 1989)).astype(int)\n\n# Base configuration\nbase_config = {\n    \"df\": df,\n    \"outcome\": df.columns[2],\n    \"treat\": df.columns[-1],\n    \"unitid\": df.columns[0],\n    \"time\": df.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\", \"red\"]\n}\n\n# TSSC model\ntssc_config = base_config.copy()  # Start with base and modify if necessary\narco = TSSC(tssc_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is TSSC estimator, where we adjust for baseline differences using an intercept.\n\n# PDA model with method 'l2'\npda_config = base_config.copy()\npda_config[\"method\"] = \"l2\"\narcol2 = PDA(pda_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is \\(\\ell_2\\)-PDA estimator, where we allow for negative weights and an intercept.\n\n# FDID model\nfdid_config = base_config.copy()\narcofdid = FDID(fdid_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:434: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is the predictions of the FDID estimator, where we employ Forward Selection to choose the donor pool for the DID method.\n\n\nTakeaway\nThe qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California’s pre-treatment trends, and so long as we are doing that, we may generally use as many relevant donors as we like. The issue is not that donors in general do not matter. I wrote mlsynth precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit, especially in high-dimensional settings. The key issue though is that the “right donors” do not necessarily need to be the same type of unit as the kind you are using, meaing we can be a little creative as to what we use as a donor. This is a philosophical question, not an econometric one. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.\nNote that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy. Or, you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the theoretical proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you will be able to use these methods more confidently, and with much more clarity than before.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat Are We Weighting For?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nAug 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scexp.html",
    "href": "scexp.html",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "",
    "text": "Suppose the government of Curaçao wishes to implement a new “Green Stay” initiative that encourages hotels to adopt sustainability measures such as reducing water usage, improving waste management, and shifting toward renewable energy. From a methodological standpoint, the most rigorous way to measure the impact of such a program would be through a randomized controlled trial, where some neighborhoods are randomly assigned to implement the policy while others serve as controls. Randomization ensures that, in expectation, treated and control groups are balanced, and that differences in outcomes can be attributed to the policy. It also provides a clear framework for statistical inference and lends credibility to the findings.\nDespite these advantages, there are serious obstacles to conducting a true RCT in this context. Ethically, assigning certain neighborhoods to receive the benefits while denying them to others may be perceived as unfair, particularly if the policy boosts reputation, attracts eco-conscious travelers, or leads to financial advantages. Guests might also experience different standards unknowingly, raising questions about fairness and transparency.\nPolitically, implementing randomization across hundreds of neighborhoods would require coordination among the government, hotel associations, and local businesses, all of whom may resist being “experimented on.” Because tourism is central to the island’s economy, public perception would be sensitive, and randomized assignment could easily be framed as a risk to one of Curaçao’s most important industries.\nThe logistical burden is equally daunting: monitoring and enforcing different sustainability requirements neighborhood by neighborhood would be complex and costly, with significant overhead required for compliance and enforcement. In practice, coordinating hundreds of independent hotels is unlikely to succeed. In other words, while an RCT is desirable in theory, ethical, political, and feasibility barriers make it impractical in the context of Curaçao’s hotel sector.\nGiven these challenges, a more practical approach is to consider the market at a localized level, grouping neighborhoods into clusters that share similar characteristics, such as geographic location, customer demographics, or historical patterns of tourist activity. By focusing on clusters rather than individual units, we mitigate ethical concerns of treating some neighborhoods differently than others, since selection occurs within naturally similar groups rather than arbitrarily across the whole market.\nCluster analysis also reduces logistical complexity, because interventions can be coordinated at the cluster level rather than needing to monitor and enforce policies across hundreds of disparate hotels. Within each cluster, we can select treated neighborhoods that closely resemble the local market and control neighborhoods that reflect the cluster’s underlying dynamics. This ensures that observed effects of the intervention are not driven by unusual outliers or idiosyncratic behaviors, but rather capture the typical response within a given market segment.\nBut how would we select which neighborhoods to treat and which to use as controls? Even within a single cluster, we cannot treat everybody for practical reasons. To address this, we can use synthetic control methods, which construct “synthetic neighborhoods” that mimic the characteristics of treated clusters, allowing analysts to experiment with different treatment assignments and evaluate their likely impact rigorously.\n\n\nIn our Curaçao Green Stay example, the unit of treatment is the neighborhood, not individual hotels. That is, entire neighborhoods may adopt sustainability measures, while others remain untreated. Let \\(\\mathcal{J} = \\{1, \\dots, J\\}\\) denote neighborhoods observed over \\(T_0\\) pre-treatment periods \\(\\mathcal{T}_0 = \\{1, \\dots, T_0\\}\\), nested within \\(K\\) clusters \\(\\mathcal{K} = \\{1, \\dots, K\\}\\) that group neighborhoods with similar characteristics (e.g., location, tourist demographics, historical occupancy). Let \\(I_k \\subseteq \\mathcal{J}\\) denote neighborhoods in cluster \\(k\\), with \\(j \\in I_{k(j)}\\) indicating cluster membership. In our case, we have 21 units of interest, where some will be treated and others will not. We have 128 pre-treatment periods, and 28 post periods. Observed outcomes, like occupancy rates or energy use, are collected in \\(\\mathbf{Y} \\in \\mathbb{R}^{J \\times T_0}\\), with \\(y_{jt}\\) representing the outcome for neighborhood \\(j\\) at time \\(t\\). Potential outcomes are \\(y_{jt}^I\\) if the neighborhood participates in Green Stay and \\(y_{jt}^N\\) if not. The cluster-weighted average treatment effect is\n\\[\n\\tau_t = \\sum_{j=1}^J f_j \\, (y_{jt}^I - y_{jt}^N), \\quad t &gt; T_0,\n\\]\nwhere \\(f_j \\ge 0\\) and \\(\\sum_{j=1}^J f_j = 1\\). Predictor vectors \\(\\mathbf{x}_j \\in \\mathbb{R}^r\\) capture pre-treatment characteristics of neighborhoods, such as the average energy use, hotel occupancy, or typical guest demographics, with cluster mean\n\\[\n\\mathbf{\\bar{x}}_k = \\frac{\\sum_{j \\in I_k} f_j \\mathbf{x}_j}{\\sum_{j \\in I_k} f_j}.\n\\]\nDistances are defined as\n\\[\nD_{1,j,k} = \\|\\mathbf{x}_j - \\mathbf{\\bar{x}}_k\\|_2^2, \\quad\nD_{2,j,j',k} = \\|\\mathbf{x}_j - \\mathbf{x}_{j'}\\|_2^2.\n\\]\nWe define synthetic treated and control neighborhoods via weights \\(w_j \\ge 0\\) and \\(v_{i,j} \\ge 0\\), and neighborhoods are either treated or not via \\(z_j \\in \\{0,1\\}\\). Optional costs \\(\\mathbf{c}\\) and cluster budgets \\(B_k\\) constrain feasible assignments. The feasible set is\n\\[\n\\begin{aligned}\n\\mathcal{F} = \\Big\\{ (w,v) \\,\\Big| \\;\n& w_j, v_j \\ge 0, \\quad \\sum_{j \\in I_k} w_j = \\sum_{j \\in I_k} v_j = 1, \\\\\n& w_j \\le z_j, \\; v_j \\le 1 - z_j, \\quad \\sum_{j \\in I_k} z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}], \\\\\n& \\sum_{j \\in I_k} c_j w_j \\le B_k, \\quad \\sum_{k=1}^K \\sum_{j \\in I_k} c_j w_j \\le B_{\\text{total}}, \\\\\n& \\sum_{k=1}^K z_{j,k} \\le 1\n\\Big\\}.\n\\end{aligned}\n\\]\nIntuitively, in the Curaçao context:\n\nNon-negativity and normalization (\\(w_j, v_j \\ge 0\\), \\(\\sum w_j = \\sum v_j = 1\\)):\nSynthetic neighborhoods are weighted averages of real neighborhoods. Negative weights would imply “subtracting” a neighborhood’s influence. Summing to one ensures each synthetic neighborhood is a proper mixture of real neighborhoods.\nTreatment assignment (\\(z_j \\in \\{0,1\\}, w_j \\le z_j, v_j \\le 1 - z_j\\)):\nEach neighborhood is either treated or not. A treated neighborhood contributes to the synthetic treated unit but not the control unit, and vice versa, avoiding “double-counting.”\nCardinality constraints (\\(\\sum z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}]\\)):\nEnsures at least one neighborhood is treated per cluster to observe effects, but not every neighborhood, respecting ethical, financial, or logistical limits.\nBudget constraints (\\(\\sum c_j w_j \\le B_k\\), \\(\\sum c_j w_j \\le B_{\\text{total}}\\)):\nNeighborhoods may have different costs. Cluster- and global-level budgets ensure the program remains financially feasible.\nCluster exclusivity (\\(\\sum_k z_{j,k} \\le 1\\)):\nEach neighborhood can only be treated in one cluster to avoid overlapping treatments that could confound the effect.\n\nThe clustered synthetic control estimator is obtained by solving\n\\[\n\\min_{(\\mathbf{w},\\mathbf{v}) \\in \\mathcal{F}} \\; \\mathcal{L}(\\mathbf{w},\\mathbf{v}),\n\\]\nwhere \\(\\mathcal{F}\\) is the feasible set defined by nonnegativity, within–cluster normalization, exclusivity, and any budget constraints. Each cluster \\(k \\in \\{1,\\dots,K\\}\\) has member indices \\(I_k\\), cluster mean \\(\\mathbf{\\bar{x}}_k\\), and outcomes \\(\\mathbf{X}_{I_k}\\).\nThe optimization problem chooses synthetic treated and control neighborhoods by minimizing a loss function subject to feasibility constraints. At its core, the method constructs weighted averages of units to be treated and untreated, such that both averages resemble the cluster as a whole and each other.\nThe base estimator is the most straightforward. For each cluster \\(k\\), it tries to make the synthetic treated and synthetic control neighborhoods look like the cluster mean:\n\\[\n\\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v}) = \\sum_{k=1}^K \\Big( \\mathbf{f}_{I_k}^\\top \\mathbf{1} \\Big) \\Big[\n\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2\n+ \\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2\n\\Big].\n\\]\nThis guarantees that both sides of the experiment are anchored to the same benchmark—the average of their cluster.\nThe weakly penalized version extends this idea by also encouraging the treated and control groups to resemble each other directly:\n\\[\n\\mathcal{L}_{\\text{Weak}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\beta \\|\\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k} - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2.\n\\]\nIn practice, this prevents the optimization from drifting toward two very different synthetic groups that both happen to match the cluster mean.\nThe penalized estimator takes things further by adding distance-based penalties. These ensure that the selected neighborhoods are not only good statistical matches but also geographically or demographically close to the cluster mean:\n\\[\n\\mathcal{L}_{\\text{Penalized}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\lambda_1 \\mathbf{w}_{I_k}^\\top D_{1,k} \\mathbf{w}_{I_k}\n+ \\lambda_2 \\mathbf{v}_{I_k}^\\top D_{1,k} \\mathbf{v}_{I_k}.\n\\]\nHere, the matrices \\(D_{1,k}\\) encode distances, so neighborhoods that are far from the cluster center are penalized more heavily.\nFinally, the unit-level estimator introduces the most granular controls. It not only enforces closeness to cluster means but also requires that each treated neighborhood has a tight match with its synthetic control, and that pairs of neighborhoods within the cluster do not diverge too much:\n\\[\n\\mathcal{L}_{\\text{Unit}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\xi \\, \\big(\\mathbf{w}_{I_k}^\\top \\, \\mathrm{diag}(\\|\\mathbf{X}_{I_k} - \\mathbf{X}_{I_k} \\mathbf{V}_k\\|_F^2)\\big)\n+ \\lambda_{2,\\text{unit}} \\, \\mathrm{tr}(\\mathbf{W}_{I_k}^\\top D_{2,k} \\mathbf{V}_{I_k}).\n\\]\nThis is the most restrictive design. It is best suited for cases where heterogeneity across neighborhoods matters, and analysts want each treated area to have a carefully matched control counterpart.\nTogether, these estimators form a hierarchy. The base version establishes a simple anchor; the weak estimator prevents divergence; the penalized estimator incorporates geographic or demographic realism; and the unit-level estimator enforces fine-grained one-to-one balance. Analysts can experiment across this ladder, choosing the design that best matches the goals and constraints of their study.\n\n\n\nWhat’s particularly interesting about this setup is that we’re not necessarily estimating treatment effects yet. Instead, the synthetic control framework is acting as a decision-making tool: it tells us which neighborhoods to treat and why, based on the trade-offs encoded in the objective function. Analysts can tweak the penalties, change clustering schemes, or adjust budget and cardinality constraints to see how the proposed treatment assignments change.\nFor example, increasing \\(\\beta\\) encourages the treated neighborhoods to be closer to the synthetic controls, effectively prioritizing representativeness of the treated units. Adjusting \\(\\lambda_1\\) and \\(\\lambda_2\\) emphasizes fidelity to cluster averages, penalizing outlier neighborhoods from being selected. Unit-level penalties \\(\\xi\\) and \\(\\lambda_{2,j}\\) allow analysts to ensure micro-level balance, focusing on individual neighborhoods with unique characteristics.\nBecause the framework is fully flexible, it becomes a sandbox for experimental design: you can simulate different cost and budget allocations, understand which neighborhoods are likely to be chosen under various priorities, and make informed decisions before the business/government does any treatment. In other words, this lets analysts answer: “Given our objectives and constraints, which neighborhoods should be treated, and why?” before any policy is implemented, making the framework extremely valuable for planning ethical, practical, and representative market experiments.\n\n\n\n\n\nLet \\(\\hat{\\tau}_{jt}\\) denote the unit-level treatment effect for neighborhood \\(j\\) at time \\(t\\):\n\\[\n\\hat{\\tau}_{jt} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{T}_2.\n\\]\nThe cluster-weighted effect for cluster \\(k\\) is\n\\[\n\\hat{\\tau}_{kt} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{T}_2,\n\\]\nand the overall weighted effect is\n\\[\n\\hat{\\tau}_t = \\sum_{k=1}^K \\hat{\\tau}_{kt} = \\sum_{k=1}^K \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad t \\in \\mathcal{T}_2.\n\\]\n\n\n\nPlacebo effects are computed over pre-treatment “blank” periods \\(\\mathcal{B} \\subseteq \\mathcal{T}_1\\):\nUnit-level:\n\\[\n\\hat{\\tau}_{j,t}^{\\mathcal{B}} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{B}.\n\\]\nCluster-level:\n\\[\n\\hat{\\tau}_{k,t}^{\\mathcal{B}} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{j,t}^{\\mathcal{B}}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{B}.\n\\]\nOverall we have:\n\\[\n\\hat{\\tau}_t^{\\mathcal{B}} = \\sum_{k=1}^K \\hat{\\tau}_{k,t}^{\\mathcal{B}} = \\sum_{j \\in \\mathcal{J}} f_j \\, \\hat{\\tau}_{j,t}^{\\mathcal{B}}, \\quad t \\in \\mathcal{B}.\n\\]\n\n\n\nWeighted effects in the post-treatment period \\(\\mathcal{T}_2\\):\nUnit-level:\n\\[\n\\hat{\\tau}_{jt} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{T}_2.\n\\]\nCluster-level:\n\\[\n\\hat{\\tau}_{k,t} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{T}_2.\n\\]\nOverall we have:\n\\[\n\\hat{\\tau}_t = \\sum_{k=1}^K \\hat{\\tau}_{k,t} = \\sum_{j \\in \\mathcal{J}} f_j \\, \\hat{\\tau}_{jt}, \\quad t \\in \\mathcal{T}_2.\n\\]\n\n\n\nFor post-treatment period \\(t \\in \\mathcal{T}_2\\), the \\((1-\\alpha)\\) confidence interval is\n\\[\nq_{1-\\alpha} = \\text{Quantile}_{1-\\alpha}\\big(|\\hat{\\tau}_t^{\\mathcal{B}}|\\big), \\quad t \\in \\mathcal{B},\n\\]\n\\[\n\\text{CI}_t = \\big[\\hat{\\tau}_t - q_{1-\\alpha}, \\hat{\\tau}_t + q_{1-\\alpha}\\big].\n\\]\nCluster-level confidence intervals:\n\\[\nq_{k,1-\\alpha} = \\text{Quantile}_{1-\\alpha}\\big(|\\hat{\\tau}_{k,t}^{\\mathcal{B}}|\\big), \\quad t \\in \\mathcal{B},\n\\]\n\\[\n\\text{CI}_{k,t} = \\big[\\hat{\\tau}_{k,t} - q_{k,1-\\alpha}, \\hat{\\tau}_{k,t} + q_{k,1-\\alpha}\\big].\n\\]\n\n\n\nObserved test statistic:\n\\[\nS_\\text{obs} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} |\\hat{\\tau}_t|.\n\\]\nPermutation distribution: randomly sample \\(|\\mathcal{T}_2|\\)-sized subsets from \\(\\{\\hat{\\tau}_t^{\\mathcal{B}} \\cup \\hat{\\tau}_t : t \\in \\mathcal{T}_2\\}\\). Denote permuted statistics \\(S_\\text{perm}^{(b)}, b = 1,\\dots,B\\). Global p-value:\n\\[\np_\\text{global} = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{S_\\text{perm}^{(b)} \\ge S_\\text{obs}\\}.\n\\]\nCluster-level permutation tests are analogous, yielding \\(p_{k, \\text{global}}\\) for each cluster."
  },
  {
    "objectID": "scexp.html#notation-and-synthetic-control-designs",
    "href": "scexp.html#notation-and-synthetic-control-designs",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "",
    "text": "In our Curaçao Green Stay example, the unit of treatment is the neighborhood, not individual hotels. That is, entire neighborhoods may adopt sustainability measures, while others remain untreated. Let \\(\\mathcal{J} = \\{1, \\dots, J\\}\\) denote neighborhoods observed over \\(T_0\\) pre-treatment periods \\(\\mathcal{T}_0 = \\{1, \\dots, T_0\\}\\), nested within \\(K\\) clusters \\(\\mathcal{K} = \\{1, \\dots, K\\}\\) that group neighborhoods with similar characteristics (e.g., location, tourist demographics, historical occupancy). Let \\(I_k \\subseteq \\mathcal{J}\\) denote neighborhoods in cluster \\(k\\), with \\(j \\in I_{k(j)}\\) indicating cluster membership. In our case, we have 21 units of interest, where some will be treated and others will not. We have 128 pre-treatment periods, and 28 post periods. Observed outcomes, like occupancy rates or energy use, are collected in \\(\\mathbf{Y} \\in \\mathbb{R}^{J \\times T_0}\\), with \\(y_{jt}\\) representing the outcome for neighborhood \\(j\\) at time \\(t\\). Potential outcomes are \\(y_{jt}^I\\) if the neighborhood participates in Green Stay and \\(y_{jt}^N\\) if not. The cluster-weighted average treatment effect is\n\\[\n\\tau_t = \\sum_{j=1}^J f_j \\, (y_{jt}^I - y_{jt}^N), \\quad t &gt; T_0,\n\\]\nwhere \\(f_j \\ge 0\\) and \\(\\sum_{j=1}^J f_j = 1\\). Predictor vectors \\(\\mathbf{x}_j \\in \\mathbb{R}^r\\) capture pre-treatment characteristics of neighborhoods, such as the average energy use, hotel occupancy, or typical guest demographics, with cluster mean\n\\[\n\\mathbf{\\bar{x}}_k = \\frac{\\sum_{j \\in I_k} f_j \\mathbf{x}_j}{\\sum_{j \\in I_k} f_j}.\n\\]\nDistances are defined as\n\\[\nD_{1,j,k} = \\|\\mathbf{x}_j - \\mathbf{\\bar{x}}_k\\|_2^2, \\quad\nD_{2,j,j',k} = \\|\\mathbf{x}_j - \\mathbf{x}_{j'}\\|_2^2.\n\\]\nWe define synthetic treated and control neighborhoods via weights \\(w_j \\ge 0\\) and \\(v_{i,j} \\ge 0\\), and neighborhoods are either treated or not via \\(z_j \\in \\{0,1\\}\\). Optional costs \\(\\mathbf{c}\\) and cluster budgets \\(B_k\\) constrain feasible assignments. The feasible set is\n\\[\n\\begin{aligned}\n\\mathcal{F} = \\Big\\{ (w,v) \\,\\Big| \\;\n& w_j, v_j \\ge 0, \\quad \\sum_{j \\in I_k} w_j = \\sum_{j \\in I_k} v_j = 1, \\\\\n& w_j \\le z_j, \\; v_j \\le 1 - z_j, \\quad \\sum_{j \\in I_k} z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}], \\\\\n& \\sum_{j \\in I_k} c_j w_j \\le B_k, \\quad \\sum_{k=1}^K \\sum_{j \\in I_k} c_j w_j \\le B_{\\text{total}}, \\\\\n& \\sum_{k=1}^K z_{j,k} \\le 1\n\\Big\\}.\n\\end{aligned}\n\\]\nIntuitively, in the Curaçao context:\n\nNon-negativity and normalization (\\(w_j, v_j \\ge 0\\), \\(\\sum w_j = \\sum v_j = 1\\)):\nSynthetic neighborhoods are weighted averages of real neighborhoods. Negative weights would imply “subtracting” a neighborhood’s influence. Summing to one ensures each synthetic neighborhood is a proper mixture of real neighborhoods.\nTreatment assignment (\\(z_j \\in \\{0,1\\}, w_j \\le z_j, v_j \\le 1 - z_j\\)):\nEach neighborhood is either treated or not. A treated neighborhood contributes to the synthetic treated unit but not the control unit, and vice versa, avoiding “double-counting.”\nCardinality constraints (\\(\\sum z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}]\\)):\nEnsures at least one neighborhood is treated per cluster to observe effects, but not every neighborhood, respecting ethical, financial, or logistical limits.\nBudget constraints (\\(\\sum c_j w_j \\le B_k\\), \\(\\sum c_j w_j \\le B_{\\text{total}}\\)):\nNeighborhoods may have different costs. Cluster- and global-level budgets ensure the program remains financially feasible.\nCluster exclusivity (\\(\\sum_k z_{j,k} \\le 1\\)):\nEach neighborhood can only be treated in one cluster to avoid overlapping treatments that could confound the effect.\n\nThe clustered synthetic control estimator is obtained by solving\n\\[\n\\min_{(\\mathbf{w},\\mathbf{v}) \\in \\mathcal{F}} \\; \\mathcal{L}(\\mathbf{w},\\mathbf{v}),\n\\]\nwhere \\(\\mathcal{F}\\) is the feasible set defined by nonnegativity, within–cluster normalization, exclusivity, and any budget constraints. Each cluster \\(k \\in \\{1,\\dots,K\\}\\) has member indices \\(I_k\\), cluster mean \\(\\mathbf{\\bar{x}}_k\\), and outcomes \\(\\mathbf{X}_{I_k}\\).\nThe optimization problem chooses synthetic treated and control neighborhoods by minimizing a loss function subject to feasibility constraints. At its core, the method constructs weighted averages of units to be treated and untreated, such that both averages resemble the cluster as a whole and each other.\nThe base estimator is the most straightforward. For each cluster \\(k\\), it tries to make the synthetic treated and synthetic control neighborhoods look like the cluster mean:\n\\[\n\\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v}) = \\sum_{k=1}^K \\Big( \\mathbf{f}_{I_k}^\\top \\mathbf{1} \\Big) \\Big[\n\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2\n+ \\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2\n\\Big].\n\\]\nThis guarantees that both sides of the experiment are anchored to the same benchmark—the average of their cluster.\nThe weakly penalized version extends this idea by also encouraging the treated and control groups to resemble each other directly:\n\\[\n\\mathcal{L}_{\\text{Weak}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\beta \\|\\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k} - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2.\n\\]\nIn practice, this prevents the optimization from drifting toward two very different synthetic groups that both happen to match the cluster mean.\nThe penalized estimator takes things further by adding distance-based penalties. These ensure that the selected neighborhoods are not only good statistical matches but also geographically or demographically close to the cluster mean:\n\\[\n\\mathcal{L}_{\\text{Penalized}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\lambda_1 \\mathbf{w}_{I_k}^\\top D_{1,k} \\mathbf{w}_{I_k}\n+ \\lambda_2 \\mathbf{v}_{I_k}^\\top D_{1,k} \\mathbf{v}_{I_k}.\n\\]\nHere, the matrices \\(D_{1,k}\\) encode distances, so neighborhoods that are far from the cluster center are penalized more heavily.\nFinally, the unit-level estimator introduces the most granular controls. It not only enforces closeness to cluster means but also requires that each treated neighborhood has a tight match with its synthetic control, and that pairs of neighborhoods within the cluster do not diverge too much:\n\\[\n\\mathcal{L}_{\\text{Unit}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\xi \\, \\big(\\mathbf{w}_{I_k}^\\top \\, \\mathrm{diag}(\\|\\mathbf{X}_{I_k} - \\mathbf{X}_{I_k} \\mathbf{V}_k\\|_F^2)\\big)\n+ \\lambda_{2,\\text{unit}} \\, \\mathrm{tr}(\\mathbf{W}_{I_k}^\\top D_{2,k} \\mathbf{V}_{I_k}).\n\\]\nThis is the most restrictive design. It is best suited for cases where heterogeneity across neighborhoods matters, and analysts want each treated area to have a carefully matched control counterpart.\nTogether, these estimators form a hierarchy. The base version establishes a simple anchor; the weak estimator prevents divergence; the penalized estimator incorporates geographic or demographic realism; and the unit-level estimator enforces fine-grained one-to-one balance. Analysts can experiment across this ladder, choosing the design that best matches the goals and constraints of their study."
  },
  {
    "objectID": "scexp.html#intuition",
    "href": "scexp.html#intuition",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "",
    "text": "What’s particularly interesting about this setup is that we’re not necessarily estimating treatment effects yet. Instead, the synthetic control framework is acting as a decision-making tool: it tells us which neighborhoods to treat and why, based on the trade-offs encoded in the objective function. Analysts can tweak the penalties, change clustering schemes, or adjust budget and cardinality constraints to see how the proposed treatment assignments change.\nFor example, increasing \\(\\beta\\) encourages the treated neighborhoods to be closer to the synthetic controls, effectively prioritizing representativeness of the treated units. Adjusting \\(\\lambda_1\\) and \\(\\lambda_2\\) emphasizes fidelity to cluster averages, penalizing outlier neighborhoods from being selected. Unit-level penalties \\(\\xi\\) and \\(\\lambda_{2,j}\\) allow analysts to ensure micro-level balance, focusing on individual neighborhoods with unique characteristics.\nBecause the framework is fully flexible, it becomes a sandbox for experimental design: you can simulate different cost and budget allocations, understand which neighborhoods are likely to be chosen under various priorities, and make informed decisions before the business/government does any treatment. In other words, this lets analysts answer: “Given our objectives and constraints, which neighborhoods should be treated, and why?” before any policy is implemented, making the framework extremely valuable for planning ethical, practical, and representative market experiments."
  },
  {
    "objectID": "scexp.html#inference",
    "href": "scexp.html#inference",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "",
    "text": "Let \\(\\hat{\\tau}_{jt}\\) denote the unit-level treatment effect for neighborhood \\(j\\) at time \\(t\\):\n\\[\n\\hat{\\tau}_{jt} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{T}_2.\n\\]\nThe cluster-weighted effect for cluster \\(k\\) is\n\\[\n\\hat{\\tau}_{kt} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{T}_2,\n\\]\nand the overall weighted effect is\n\\[\n\\hat{\\tau}_t = \\sum_{k=1}^K \\hat{\\tau}_{kt} = \\sum_{k=1}^K \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad t \\in \\mathcal{T}_2.\n\\]\n\n\n\nPlacebo effects are computed over pre-treatment “blank” periods \\(\\mathcal{B} \\subseteq \\mathcal{T}_1\\):\nUnit-level:\n\\[\n\\hat{\\tau}_{j,t}^{\\mathcal{B}} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{B}.\n\\]\nCluster-level:\n\\[\n\\hat{\\tau}_{k,t}^{\\mathcal{B}} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{j,t}^{\\mathcal{B}}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{B}.\n\\]\nOverall we have:\n\\[\n\\hat{\\tau}_t^{\\mathcal{B}} = \\sum_{k=1}^K \\hat{\\tau}_{k,t}^{\\mathcal{B}} = \\sum_{j \\in \\mathcal{J}} f_j \\, \\hat{\\tau}_{j,t}^{\\mathcal{B}}, \\quad t \\in \\mathcal{B}.\n\\]\n\n\n\nWeighted effects in the post-treatment period \\(\\mathcal{T}_2\\):\nUnit-level:\n\\[\n\\hat{\\tau}_{jt} = y_{jt}^{\\mathcal{N}_1} - y_{jt}^{\\mathcal{N}_0}, \\quad j \\in \\mathcal{J}, \\; t \\in \\mathcal{T}_2.\n\\]\nCluster-level:\n\\[\n\\hat{\\tau}_{k,t} = \\sum_{j \\in I_k} f_j \\, \\hat{\\tau}_{jt}, \\quad k \\in \\mathcal{K}, \\; t \\in \\mathcal{T}_2.\n\\]\nOverall we have:\n\\[\n\\hat{\\tau}_t = \\sum_{k=1}^K \\hat{\\tau}_{k,t} = \\sum_{j \\in \\mathcal{J}} f_j \\, \\hat{\\tau}_{jt}, \\quad t \\in \\mathcal{T}_2.\n\\]\n\n\n\nFor post-treatment period \\(t \\in \\mathcal{T}_2\\), the \\((1-\\alpha)\\) confidence interval is\n\\[\nq_{1-\\alpha} = \\text{Quantile}_{1-\\alpha}\\big(|\\hat{\\tau}_t^{\\mathcal{B}}|\\big), \\quad t \\in \\mathcal{B},\n\\]\n\\[\n\\text{CI}_t = \\big[\\hat{\\tau}_t - q_{1-\\alpha}, \\hat{\\tau}_t + q_{1-\\alpha}\\big].\n\\]\nCluster-level confidence intervals:\n\\[\nq_{k,1-\\alpha} = \\text{Quantile}_{1-\\alpha}\\big(|\\hat{\\tau}_{k,t}^{\\mathcal{B}}|\\big), \\quad t \\in \\mathcal{B},\n\\]\n\\[\n\\text{CI}_{k,t} = \\big[\\hat{\\tau}_{k,t} - q_{k,1-\\alpha}, \\hat{\\tau}_{k,t} + q_{k,1-\\alpha}\\big].\n\\]\n\n\n\nObserved test statistic:\n\\[\nS_\\text{obs} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} |\\hat{\\tau}_t|.\n\\]\nPermutation distribution: randomly sample \\(|\\mathcal{T}_2|\\)-sized subsets from \\(\\{\\hat{\\tau}_t^{\\mathcal{B}} \\cup \\hat{\\tau}_t : t \\in \\mathcal{T}_2\\}\\). Denote permuted statistics \\(S_\\text{perm}^{(b)}, b = 1,\\dots,B\\). Global p-value:\n\\[\np_\\text{global} = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{S_\\text{perm}^{(b)} \\ge S_\\text{obs}\\}.\n\\]\nCluster-level permutation tests are analogous, yielding \\(p_{k, \\text{global}}\\) for each cluster."
  },
  {
    "objectID": "scmo.html",
    "href": "scmo.html",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "",
    "text": "Sometimes analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may possibly predict a target/outcome variable that we care about, and plenty of other papers have commented on this fact before. Some analysts even argue for the surrogate approach, where we treat other outcomes or affected units as a kind of instrument for the counterfactual trajcectory of the target unit.\nHowever, all of this is most uncommon. As it turns out, most people in academia and industry who use synthetic controls use only a single focal outcome in their analyses. Perhaps they will adjust their unit weights by some diagonal matrix, a diagonal matrix \\(\\mathbf{V}\\) in most applications. The point of this matrix is basically to assist the main optimization in choosing the unit weights. However, even this is limited by the number of pretreatment periods you have- if you have more covariates than you have pretreatment periods, you cannot estimate the regression. Recent papers by econometricians have tried to get around this, though. This blog post covers a few recent recent papers which have advocated for this. I explain the econometric method and apply it in a simulated setting."
  },
  {
    "objectID": "scmo.html#standard-synthetic-control",
    "href": "scmo.html#standard-synthetic-control",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Standard Synthetic Control",
    "text": "Standard Synthetic Control\nBefore introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator minimizes\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThis is a constrained least squares program in which we regress the treated unit’s pre-treatment outcomes onto the control matrix under the constraint that \\(\\mathbf{w}\\) lies in the simplex \\(\\Delta^{N_0}\\). For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights \\(\\mathbf{w}^\\ast\\) are estimated, the out-of-sample estimates are obtained by applying the same weights to the control matrix\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0^{\\text{post}} \\mathbf{w}^\\ast,\n\\]\nwith the concatenation between the in and out of sample vectors corresponding to the full predictions of the model. The estimated treatment effect at each post-treatment time point is then given by the difference between observed and out-of-sample outcomes: \\(\\hat{\\tau}_{1t} = y_{1t} - \\hat{y}_{1t}\\) for \\(t \\in \\mathcal{T}_2\\)."
  },
  {
    "objectID": "scmo.html#model-averaging",
    "href": "scmo.html#model-averaging",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Model Averaging",
    "text": "Model Averaging\nWe may also model average these models together, which sometimes results in better fit than using either model alone. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have \\(\\mathbf{y}_1^{\\text{CAT}} \\in \\mathbb{R}^{T_0}\\), which denotes the pre-treatment fit from the concatenated model by Tian, Lee, and Panchenko, and on the other hand, we have \\(\\mathbf{y}_1^{\\text{AVG}} \\in \\mathbb{R}^{T_0}\\), the corresponding fit from the demeaned model by Sun, Ben-Michael, and Feller. As before, we observe the treated unit’s pre-treatment trajectory, \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\).\nTo begin, we stack the two counterfactuals into a single matrix:\n\\[\n\\mathbf{Y}^{\\text{MA}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT}} & \\mathbf{y}_1^{\\text{AVG}}\n\\end{bmatrix} \\in \\mathbb{R}^{T_0 \\times 2}.\n\\]\nWe define the model-averaged pre-treatment fit as a convex combination of the two predictions\n\\[\n\\mathbf{y}_1^{\\text{MA}}(\\boldsymbol{\\lambda}) = \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda},\n\\]\nwhere \\(\\boldsymbol{\\lambda} \\in \\Delta^2\\) is a 2-dimensional simplex weight vector\n\\[\n\\Delta^2 = \\left\\{ \\boldsymbol{\\lambda} \\in \\mathbb{R}_{\\geq 0}^2 : \\| \\boldsymbol{\\lambda} \\|_1 = 1 \\right\\}.\n\\]\nThe model averaged objective function minimizes\n\\[\n\\boldsymbol{\\lambda}^\\ast = \\underset{\\boldsymbol{\\lambda} \\in \\Delta^2}{\\operatorname{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda} \\right\\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThe interpretation of the convex hull remains the same as in the traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the global minimum and maximum of the two individual estimators\n\\[\n\\mathbf{y}_1^{\\text{MA}} \\in \\left[\n\\min\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right),\n\\max\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right)\n\\right]\n\\quad \\forall t \\in \\mathcal{T}.\n\\]\nOnce \\(\\boldsymbol{\\lambda}^\\ast\\) is found, the model-averaged out-of-sample predictions are estimated like\n\\[\n\\mathbf{Y}^{\\text{MA, post}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT, post}} & \\mathbf{y}_1^{\\text{AVG, post}}\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_1^{\\text{MA, post}} = \\mathbf{Y}^{\\text{MA, post}} \\boldsymbol{\\lambda}^\\ast.\n\\]\nEssentially, this is a mixture of both models."
  },
  {
    "objectID": "scmo.html#conformal-prediction-via-agnostic-means",
    "href": "scmo.html#conformal-prediction-via-agnostic-means",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Conformal Prediction via Agnostic Means",
    "text": "Conformal Prediction via Agnostic Means\nNow a final word on infernece. I use conformal prediction intervals to conduct inference here, developed in this paper. Precisely, I use the agnostic approach (yes, I know other approaches exist; users of mlsynth will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as \\(\\mathbf{u}_{\\text{pre}} = \\mathbf{y}_{1,\\text{pre}} - \\mathbf{y}^{\\text{SC}}_{1,\\text{pre}}\\), or just the pretreatment difference betwixt the observed values and its counterfactual. Furthermore, let \\(\\hat{\\sigma}^2 = \\frac{1}{T_0 - 1} \\left\\| \\mathbf{u}_{\\text{pre}} - \\bar{u} \\mathbf{1} \\right\\|^2\\) be the unbiased estimator of the residual variance, where \\(\\bar{u} = \\frac{1}{T_0} \\sum_{t=1}^{T_0} u_t\\) is the mean residual.\nWe aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period \\(\\mathbf{y}^{\\text{SC}}_{1,\\text{post}} \\in \\mathbb{R}^{T_1}\\) be the post-treatment SC predictions for some generic estimator. Assuming that the out-of-sample error is sub-Gaussian given the history \\(\\mathscr{H}\\) (in plain English, this just means that large errors are unlikely, which makes sense given that SC is less biased in a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via concentration inequalities. Specifically, we have \\(\\delta_\\alpha = \\sqrt{2 \\hat{\\sigma}^2 \\log(2 / \\alpha)}\\). The conformal prediction intervals are then defined as \\(\\mathbf{p}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} - \\delta_\\alpha \\mathbf{1}\\), \\(\\mathbf{u}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} + \\delta_\\alpha \\mathbf{1}\\). These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will likely be incorporated in the future."
  },
  {
    "objectID": "scmo.html#simulation",
    "href": "scmo.html#simulation",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Simulation",
    "text": "Simulation\nSuppose we are working at Airbnb, and we wish to see the causal effect of the introduction of Airbnb Experiences on Gross Booking Value (GBV), a metric which is defined as ‘’the total revenue generated by room or property rentals before any costs or expenses are subtracted’’. Airbnb Experiences connects users of the platform to local tour guides or other local attractions. It serves as a kind of competition to Travelocity, Viator and other booking/ travel services. In other words, this program may make makes this city an attraction, and we may see an increase in GBV as a result.\nWell, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy. The point of this simulation is to use the SCMO (synthetic control multiple outcomes) estimator to measure the causal impact.\nFor each unit, the observed outcome \\(\\mathbf{Y}_{jtk}\\) evolves according to an autoregressive process with latent structure for time, place, and seasonality\n\\[\n\\mathbf{Y}_{jtk} =\n\\rho_k \\mathbf{Y}_{jt-1k} +\n(1 - \\rho_k) \\left(\n\\alpha_{jk} + \\beta_{tk} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{tk} + \\mathbf{S}_{jt} + \\delta_k\n\\right) + \\varepsilon_{jtk}, \\quad \\text{for } t &gt; 1,\n\\]\nwith initial condition\n\\[\n\\mathbf{Y}_{j1k} =\n\\alpha_{jk} + \\beta_{1k} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{1k} + \\mathbf{S}_{j1} + \\delta_k + \\varepsilon_{j1k}.\n\\]\nHere, \\(\\alpha_{jk} \\sim \\mathcal{N}(0, 1)\\) and \\(\\beta_{tk} \\sim \\mathcal{N}(0, 1)\\) represent unit-outcome and time-outcome fixed effects, respectively. Each unit \\(j\\) possesses latent attributes \\(\\boldsymbol{\\phi}_j \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\), while each time-outcome pair \\((tk)\\) has associated latent loadings \\(\\boldsymbol{\\mu}_{tk} \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\). The seasonal component \\(\\mathbf{S}_{jt}\\) captures unit-specific periodicity and is defined as \\(\\gamma_j \\cos\\left( \\frac{4\\pi(t - \\tau_j)}{T_{\\text{season}}} \\right)\\), with \\(\\gamma_j \\sim \\text{Unif}(0, \\bar{\\gamma})\\) representing the amplitude and \\(\\tau_j \\sim \\text{Unif}\\{0, \\dots, T_{\\text{season}} - 1\\}\\) the phase shift. Each outcome \\(k\\) has a baseline shift \\(\\delta_k \\sim \\text{Unif}(200, 500)\\), an autocorrelation parameter \\(\\rho_k \\in (0, 1)\\), and an idiosyncratic noise component \\(\\varepsilon_{jtk} \\sim \\mathcal{N}(0, \\sigma^2)\\). One unit (Iquique, Chile in this draw) is designated as treated. To introduce selection bias, the unit with the second-largest realization on the first latent factor dimension is treated, meaning methods like difference-in-differences or interrupted time series methods will not perform well. The target unit’s GBV receives an additive treatment effect of \\(+5\\) during all post-treatment periods."
  },
  {
    "objectID": "scmo.html#results",
    "href": "scmo.html#results",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Results",
    "text": "Results\nWhen we run this estimator, we need to specify one of three estimators: TLP, SBMF, or both, where the abbreviations are obviouslyfor the surnames of the authors. We also need to supply a dictionary entry to mlsynth called addout. This is either a string or a list which lists the additional outcomes we care about in the dataframe. When we run the estimator, we get:\n\n# Run simulation\n\ndf = simulate(seed=10000, r=3)\n\nconfig = {\n    \"df\": df,\n    \"outcome\": 'Gross Booking Value',\n    \"treat\": 'Experiences',\n    \"unitid\": 'Market',\n    \"time\": 'Week',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\"], \"addout\": list(df.columns[4:]),\n    \"method\": \"BOTH\"\n}\n\narco = SCMO(config).fit()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/mlsynth/estimators/scmo.py:644: UserWarning:\n\nAn unexpected error occurred during SCMO plotting: plot_estimates() got an unexpected keyword argument 'df'\n\n\n\nUsing the model averaging estimator, our pre-treatment Root Mean Squared Error is 0.276. The ATT is 5.046. The weights are also a sparse vector. The model averaged estimator returns Arequipa (0.237), Bogotá (0.232), San Salvador (0.218), Santiago de Chile (0.174), San Luis Potosí (0.081), Montevidio (0.032), and Manzanillo (0.026), as the contributing units or only 7 of the 98 donor units. The optimal mixing between the models is 0.538 for the intercept-shifted estimator and 0.461 for the concatenated method. For DID, the RMSE is 0.878 and the ATT is 5.2, meaning that the intercept adjusted average of all donor units is clearly a biased estimator.\nCompare to Forward DID, this is NOT true: we have an ATT of 5.063 and a pre-intervention RMSE of 0.289, selecting Antofagasta, Bocas del Toro, Punta del Este, San Pedro Sula, and Santiago as the optimal donor pool (this method uses no additional outcomes, only the GBV metric). When I compare to the clustered PCR method, the positively weighted donors are San Pedro Sula (0.296), Punta del Este (0.255), Santiago (0.199), Antofagasta (0.190), and La Plata (0.060)."
  },
  {
    "objectID": "shc.html",
    "href": "shc.html",
    "title": "The Synthetic Historical Control Method",
    "section": "",
    "text": "Oftentimes, we struggle with picking a donor pool for SCMs due to spillovers or the event being so massive that it is hard to argue for clean donors in principle. This blog post shows you one way we can get around that problem via using the synthetic historical control method. It is a flavor of synthetic controls, as the name suggests."
  },
  {
    "objectID": "shc.html#notations",
    "href": "shc.html#notations",
    "title": "The Synthetic Historical Control Method",
    "section": "Notations",
    "text": "Notations\nThe notation for this method (as it is presented in the paper) makes my head hurt, so I will simply quote directly from my paper that currently uses this method in hopes that I can spell it out clearly.\nLet \\(\\mathbb{R}\\) denote the set of real numbers, and let \\(\\|\\cdot\\|_1\\) denote the usual \\(\\ell_1\\) vector norm. Throughout, I denote sets using calligraphic letters (e.g., \\(\\mathcal{T}, \\mathcal{S}, \\mathcal{N}\\)) for clarity. Scalars are represented using plain lowercase letters (e.g., \\(h, t, n, m\\)), and matrices are denoted by bold uppercase letters (e.g., \\(\\mathbf{X}, \\mathbf{W}, \\mathbf{Y}\\)). Given a vector \\(\\mathbf{x} = (x_1, x_2, \\dots, x_T)^\\top \\in \\mathbb{R}^T\\) and an index set \\(\\mathcal{I} \\subseteq \\{1, \\dots, T\\}\\), I write \\(\\mathbf{x}_{\\mathcal{I}} \\coloneqq (x_t)_{t \\in \\mathcal{I}} \\in \\mathbb{R}^{|\\mathcal{I}|}\\) to denote the subvector of \\(\\mathbf{x}\\) corresponding to indices in \\(\\mathcal{I}\\), preserving their original order.\nLet \\(\\mathcal{T} = \\{1, 2, \\dots, T\\}\\) index time. Define the pre-treatment period as \\(\\mathcal{T}_1 \\coloneqq \\{t \\in \\mathcal{T} : t \\leq T_0\\}\\) and the post-treatment period as \\(\\mathcal{T}_2 \\coloneqq \\{t \\in \\mathcal{T} : t &gt; T_0\\}\\). The number of post-treatment periods is \\(n = T - T_0\\). Let \\(m \\in \\mathbb{N}\\) denote the evaluation window length, i.e., the number of periods used to construct the SHC match. Define the evaluation period as the final \\(m\\) months of the pre-treatment period:\n\\[\n\\mathcal{T}_{\\text{eval}} \\coloneqq \\{T_0 - m + 1, \\dots, T_0\\} \\subset \\mathcal{T}_1.\n\\]\nLet \\(\\mathbf{y} = (y_1, y_2, \\dots, y_T)^\\top \\in \\mathbb{R}^T\\) denote the observed outcome vector for the treated unit. Define the pre-treatment outcome vector \\(\\mathbf{y}_{\\text{pre}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_1} \\in \\mathbb{R}^{T_0}\\), the evaluation subvector \\(\\mathbf{y}_{\\text{eval}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_{\\text{eval}}} \\in \\mathbb{R}^m\\), and the post-treatment outcome vector \\(\\mathbf{y}_{\\text{post}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_2} \\in \\mathbb{R}^n\\). The evaluation subvector \\(\\mathbf{y}_{\\text{eval}}\\) is the target pattern to be reconstructed using convex combinations of earlier segments from the pre-treatment period.\nTo reconstruct the evaluation window, we compare it against earlier segments of the treated unit’s own pre-treatment trajectory. Define the donor matrix \\(\\widetilde{\\mathbf{Y}}_{\\text{pre}} \\in \\mathbb{R}^{m \\times N}\\) as a collection of \\(N\\) overlapping length-\\(m\\) subvectors extracted from the smoothed pre-treatment series. Each column is a contiguous subvector of the form \\(\\widetilde{\\mathbf{y}}_{[i, i+m-1]}\\), with \\(i\\) ranging over eligible start points in \\(\\mathcal{T}_1\\) that precede the evaluation window."
  },
  {
    "objectID": "shc.html#choosing-our-historical-segments",
    "href": "shc.html#choosing-our-historical-segments",
    "title": "The Synthetic Historical Control Method",
    "section": "Choosing Our Historical Segments",
    "text": "Choosing Our Historical Segments\nTo avoid using too many donor segments and overfit, SHC implements a forward selection algorithm. The authors in the paper sort of choose an arbitrary number of donors to use. They find that 27 donor segments about does the job, in practice. However, I did not like that they wanted to use a stepwise method with no stopping rule, since this just means that the size of the donor pool we use is kind of arbitrary. So, I write my own forward selection method. Starting with an empty set, it adds one donor at a time, each time choosing the segment that most reduces the in-sample MSE.\n\\[\n\\mathcal{S}_j = \\mathcal{S}_{j-1} \\cup \\left\\{ \\underset{i \\in \\mathcal{N} \\setminus \\mathcal{S}_{j-1}}{\\operatorname*{argmin}} \\left\\| \\widetilde{\\mathbf{y}}_{\\text{eval}} - \\widetilde{\\mathbf{Y}}_{\\text{pre}}^{(\\mathcal{S}_{j-1} \\cup \\{i\\})} \\mathbf{w}^{(j)} \\right\\|_2^2  + \\varsigma \\left\\| \\mathbf{C}_0^\\top \\mathbf{w} \\right\\|_2^2 \\right\\}, \\quad \\mathcal{S}_0 = \\emptyset,\n\\]\nTo choose when to stop adding donors, we compute a modified BIC at each step:\n\\[\n\\text{BIC}(j) = m \\cdot \\log\\left( \\text{MSE}_j \\right) + \\lambda j,\n\\]\nwhere \\(\\text{MSE}_j\\) is the in-sample mean squared error using \\(j\\) donors, and \\(\\lambda = \\log(m)\\). The algorithm stops when BIC increases for two steps in a row. Presumably, we could use a better one, so if you have suggestions let me know. I borrowed this idea from the forward selected PDA approach."
  },
  {
    "objectID": "sparsdens.html",
    "href": "sparsdens.html",
    "title": "What is a Synthetic Control?",
    "section": "",
    "text": "Introduction\nMarketers and econometricians stress the importance of location selection for geo-testing/program evaluation for the effect of advertising interventions. Inherently speaking, the control locations we use directly impacts the quality of our counterfactual predictions, and therefore the ATT, iROAS, or whatever metric we claim to care about. The reality is, you can’t simply take an average of randomly selected control regions and compare them to the places you’re increasing ad runs in. Therefore, researchers require systematic control group selection methods for the panel data setting where the randomzied controlled trial is not possible. Of course, one of these methods that’s popular is the synthetic control method. But if you’re reading this, you likely already know this. However, we need to take a step back. We need to ask ourselves more fundamentally what we think a synthetic control is as a concept. This happens in academia or industry all the time. My coworkers will ask me questions like, “Hey Jared, how do you view the whole constraints upon the weights for synthetic control methods? Why do we care, if at all, that the weights should be non-negative or add up to anything, and are there any rules regarding these ideas?” Usually people want to make some custom extension to the baseline algorithm, and want to know if they’re breaking some seemingly sacrosanct rules. My answer is usually some variant of: “Well, it depends on what you think a synthetic control is.” As it turns out, this is actually a non-trivial philosophical question that has no true answer.\nClassically, synthetic control weights are viewed as a sparse vector, and plenty of academic work has developed synthetic control methodologies from this perspective. The classic setting focuses on sparsity for the unit weights. The point of sparsity, as others (including me) argue is for the resulting positive donors to be interpretable and economically similar to the units of interest to the treated unit. More precisely, the goal is for similarity on latent factors that we cannot observe. Much work is dedicated to producing such descriptions of the SCM. Others argue however for a completely different set of standards. The main contention seems to be that while sparsity offers interpretability and simplicity, it may not always be practical for capturing the complex relationships in real economic or business phenomena. Instead, they advocate for dense coefficient vectors that distribute weights more diffusely across donors, potentially improving predictive performance.\nIn my opinion, the general form of a synthetic control problem is some objective function where we weight donor units to best approximate the treated (set of) unit(s), with the choice of constraints reflecting both the econometric goals and domain-specific beliefs. The most general expression for this is\n\\[\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\mathcal{L}\\big( \\mathbf{y}_1, \\mathbf{Y}_0 \\mathbf{w} \\big) + \\lambda \\cdot \\mathcal{R}(\\mathbf{w}) \\quad \\text{subject to} \\quad & \\mathbf{w} \\in \\mathcal{W}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{y}_1\\) is the vector of observed outcomes for the treated unit(s) during the pre-treatment period, \\(\\mathbf{Y}_0\\) is the matrix of outcomes for the donor pool units over the same period, and \\(\\mathbf{w}\\) is the vector of weights assigned to those donor units. The function \\(\\mathcal{L}(\\cdot, \\cdot)\\) represents the loss function measuring the discrepancy between the treated outcomes and the weighted combination of donors, commonly the squared error loss. The term \\(\\mathcal{R}(\\mathbf{w})\\) is a regularization function that imposes additional structure or penalties on the weights to reflect beliefs or promote certain properties like sparsity or smoothness. The scalar \\(\\lambda \\geq 0\\) controls the trade-off between fitting the data closely and respecting the regularization. Finally, the set \\(\\mathcal{W}\\) defines the feasible set of weights, encoding constraints such as non-negativity, sum-to-one, or other domain-specific restrictions. Notice that this setup is intended to be very very general. I have not yet taken a position on the nature of the weights or the specifics of the optimization problem.\nThis post shows that there is (often) no one right way to play. Many estimators may be used, and the key thing is the underlying econometric assumptions one needs to understand and make to use them effectively (sparsity vs density of course is not the only assumption of interest).\n\n\nApplication\nLet’s use an example. Consider this plot of a growth rate variable, where the outcome is the number of products purchased when some advertising campaigns went into effect. The goal of an incrementality study is to estimate how the growth of sales would have evolved absent the ads. After all, this is how we determine if the ad spend was effective or if it was wasted—by estimating how much revenue we would have generated without any advertising. In this dataset, we observe 276 control units across 176 pre-treatment time periods. The key problem of interest is that there are very many control units to choose from. We cannot simply use the average of controls as a counterfactual estimate, as this presumes that the mean of this entire control group is similar enough (in both level and trend) to the treated unit of interest. As the figure suggests though, this is likely not true. While the growth rate does indeed remove seasonality elements, there are still trend, periodic, and perhaps cyclical differences to account for as well. Given this, it’s likely the case that some reweighted average of these controls will mimic the treated unit much better than the pure average of the control units.\n\n\n\n\n\n\n\n\n\nTo this end, I fit a festival of models, some sparse, some sense. I fit the forward DID model (sparse), Forward SCM model (also sparse) and the robust PCA SCM model (sparse). For the dense models I estimate the \\(\\ell_2\\) panel data approach and the robust synthetic control (all these are documented here). I construct an ensemble of artificial counterfactual estimators by convexly combining the predictions of the base models. The goal is to produce a flexible estimator that balances the predictive accuracy of dense estimators with the interpretability and sparsity of sparse estimators. Let \\(M\\) denote the number of candidate models in the ensemble. For each model \\(m = 1, \\ldots, M\\), we obtain a predicted counterfactual series \\(\\widehat{Y}^{(m)}_{1,t}\\) for the treated unit in the pre-treatment period \\(t \\in \\mathcal{T}_0\\). We organize these into a matrix \\(\\widehat{\\mathbf{Y}}_0 \\in \\mathbb{R}^{T_0 \\times M}\\), where each column corresponds to one model’s predicted values over the pre-treatment period. Let \\(\\mathbf{Y}_{1,\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the observed outcomes for the treated unit during the same period. We solve the following convex optimization problem to learn a set of model weights \\(\\mathbf{w} \\in \\mathbb{R}^M\\):\n\\[\n\\min_{\\mathbf{w}} \\left\\| \\mathbf{Y}_{1,\\text{pre}} - \\widehat{\\mathbf{Y}}_0 \\mathbf{w} \\right\\|_2^2 + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\n\\quad \\text{subject to} \\quad \\mathbf{w} \\geq 0,\\quad \\sum_{m=1}^M w_m = 1.\n\\]\nThis is a ridge-penalized model averaging objective, constrained so that the weights are non-negative and sum to one. The penalty term \\(\\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\\) discourages over-reliance on any single model and promotes stability in the ensemble. We select the regularization parameter \\(\\lambda\\) via \\(K\\)-fold cross-validation on the pre-treatment period, minimizing the out-of-sample prediction error. The resulting weights \\(\\widehat{\\mathbf{w}}\\) define the ensemble counterfactual:\n\\[\n\\widehat{Y}^{\\text{ens}}_{1,t} = \\sum_{m=1}^M \\widehat{w}_m \\widehat{Y}^{(m)}_{1,t}, \\quad \\text{for all } t.\n\\]\nThis procedure allows flexible borrowing of information across model classes, combining the sparse structure of subset-selected synthetic controls with the improved fit of regularized or dense variants, depending on which performs better in-sample. The ensemble is constrained to lie in the convex hull of the candidate model predictions.\n\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nThere are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n\n\n\n\n\n\n\n\n\n\nThe results from the model averaging procedure reveal clear trade-offs between sparsity and predictive accuracy. In the sparse model average, the optimal penalty parameter is \\(\\lambda = 1.7074\\), leading to a pre-treatment RMSE of \\(0.5028\\). Within this specification, the FSCM model receives the most weight (\\(0.2085\\)), followed by FDID (\\(0.7915\\)), while RPCA is unused. In contrast, the dense model average achieves a substantially better fit (RMSE \\(= 0.3085\\)) with a much smaller penalty (\\(\\lambda = 0.0010\\)), allocating nearly all weight to PCR (\\(0.9966\\)) and very little to PDA (\\(0.0034\\)). The full model average, which includes both sparse and dense estimators, also selects \\(\\lambda = 0.0010\\) and reaches the same RMSE of \\(0.3085\\), but nearly all weight again falls on PCR (\\(0.9927\\)), with RPCA contributing marginally (\\(0.0073\\)) and all others excluded.\nThese results underscore a key econometric tension: while dense methods often achieve superior in-sample fit, they can obscure the role of individual donors and reduce interpretability. Sparse methods like FSCM and FDID provide clearer narratives but may underperform in terms of match quality. In settings where both approaches yield similar outcomes—as they do here—the choice between them ultimately depends on the econometrician’s priorities. If transparency and donor interpretability are paramount, sparse models may be preferred. If minimizing pre-treatment error is the guiding objective, dense models may be more appropriate. In this sense, model selection becomes not just a statistical exercise, but a philosophical one as well.\n\n\nFinal Thoughts\nIn some ways, this also illustrates why I wrote mlsynth. I do not claim to have the best models or know all of the secrets to solve one’s modeling needs; rather, I program models that I think are useful, in certain cases. The point of the mlsynth library is to allow researchers to compare and contrast these different models together all in one singular grammar, without needing to be bogged down in different softwares and syntaxes. Cool stuff happens all the time with these methods, and the only way for them to be used, and used more often, is by providing researchers with a simple framework by which to generate these counterfactuals in settings they care about.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat Are We Weighting For?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nAug 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "spillsynth.html",
    "href": "spillsynth.html",
    "title": "The Iterative Synthetic Control Method",
    "section": "",
    "text": "This will be a short blog post. I’ve spent the last two months doing a little industry work in the marketing realm, and in the meantime I made some substantial changes to mlsynth. This blog post simply shows one of the new key features I have implemented.\nChances are if you’re reading this, you know what I mean by the notion of SUTVA, or the stable unit treatment value assumption. It is the idea that if we care about the causal impact of a treatment on one unit, but other units are affected by the treatment or otherwise experience a similar treatment, that this exposure confounds our treatment effect with respect to the original unit we do care about. Say we wish to study the impact of German Reunification on West Germany’s GDP. We know West Germany was exposed, but what about neighboring nations like Austria or France? What if the reunification had regional effects? Analysts therefore have a problem: Austria and France may be very similar to West Germany, and therefore informative of West Germany’s counterfactual, but we are concerned they are exposed or affected by the main treatment of interest. What do we do? Before, researchers would need to drop these units or argue for their inclusion/exclusion, despite them being treated. Now, we do not need to do that, as SCM has a few approaches that deal with spillovers (this post covers just one). The approach, called iterative synthetic controls, is deceptively simple.\nSuppose Austria and France are partly treated. Step one of iSCM is to estimate a synthetic control for Austria, including France as a donor but excluding West Germany. Which SCM flavor you ask? Any one you like! For the purposes of this post, we will be using the Robsut SCM and the Robust PCA SCM methods from mlsynth. The precise details are not really important, but you may read the docs should you like. We then take the model predictions for Austria across the full pre and post period and replace the original Austria with the synthetic control values that the model predicts for Austria.\nNext, we do France: using the cleaned up Austria as a donor, we estimate the synthetic control for France, using the now-cleaned up Austria and the remaining donor pool units. As before, we replace the values for the original France (in the original dataset) with the new synthetic France. We now have cleaned up our two donors that may be exposed to the treatment.\nNow, with these two cleaned donors, we estimate the counterfactual for West Germany, with our 14 totally unexposed donors and the two now cleaned up donors that were once partially exposed.\n\nEstimation in Python\n“But Jared!”, you will say, this seems like a lot of looping and lots of donor tracking. Well fear not, that is what mlsynth is for. In order to get these results, you need Python (3.9 or greater) and mlsynth, which you may install from the Github repo. You’ll need the most recent version.\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nFirst we estimate the orignal model. You will find our handy-dandy util function iterative_scm is now imported.\n\nimport pandas as pd\nfrom mlsynth import CLUSTERSC\nfrom mlsynth.utils.spillover import iterative_scm\n\n# Load the reunification dataset\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/main/basedata/german_reunification.csv\"\ndf = pd.read_csv(url)\n\n# Define configuration for CLUSTERSC\nconfig = {\n    \"df\": df,\n    \"outcome\": \"gdp\",          # per capita GDP\n    \"treat\": \"Reunification\",     # binary treatment indicator\n    \"unitid\": \"country\",          # country name\n    \"time\": \"year\",               # time variable\n    \"display_graphs\": True,       # display counterfactual plots\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"Frequentist\": True, \"method\": \"BOTH\"\n}\n\noriginalresult = CLUSTERSC(config).fit()\n\n\n\n\n\n\n\n\nThese are the original results., You may check them against my coworker’s dissertation if you wish. Now we see how sensitive the results are to adjusting for spillover effects.\n\nfrom IPython.display import display, Markdown\n\niSCM_result = iterative_scm(CLUSTERSC(config), spillover_unit_identifiers=[\"Austria\", \"France\"], method=\"BOTH\")\n\ndef summarize_att_rmse_markdown(iSCM_result):\n    rows = []\n    for method in ['PCR', 'RPCA']:\n        sub = iSCM_result[method].sub_method_results[method]\n        att = sub.effects.additional_effects['ATT']\n        percent_att = sub.effects.additional_effects.get('Percent ATT', None)\n        t0_rmse = sub.fit_diagnostics.additional_metrics['T0 RMSE']\n\n        rows.append({\n            \"Method\": method,\n            \"ATT\": f\"{att:,.0f}\",\n            \"Percent ATT\": f\"{percent_att:.2f}%\" if percent_att is not None else \"—\",\n            \"T0 RMSE\": f\"{t0_rmse:,.1f}\",\n        })\n\n    df = pd.DataFrame(rows)\n    md_table = df.to_markdown(index=False)\n    display(Markdown(f\"### Iterative SCM \\n\\n{md_table}\"))\n\nsummarize_att_rmse_markdown(iSCM_result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIterative SCM\n\n\n\nMethod\nATT\nPercent ATT\nT0 RMSE\n\n\n\n\nPCR\n-2,111\n-7.96%\n108.2\n\n\nRPCA\n-1,477\n-5.71%\n88.1\n\n\n\n\n\nYou pass the original config to the util function, and you pass a list of strings that denote which units we believe are partly treated. Form there, the algorithm under the hood handles the donor cleaning, and returns back the results of the final SCM run with both donors cleaned. Note that if you want both PCR and RPCA cleaning, we pass it as method=\"BOTH\" to iterative_scm. We can see that the results are very similar to the original SCM. The pre-treatment fits degrade only very slightly (even more so with the Robust PCA method, highlighting its robustness to tiny tweaks in the model). Speaking of RPCA, the new weights are ‘Belgium’: 0.271, ‘Norway’: 0.552, ‘New Zealand’: 0.34, whereas before they were ‘Austria’: 0.023, ‘France’: 0.354, ‘Norway’: 0.485, ‘New Zealand’: 0.296. Austria goes away as a weighed donor, but there’s not very much change in the pre-treatment fit or the practical conclusions we draw from the analysis. When we use only Austria as the cleaned unit, RPCA’s weights are ‘UK’: 0.237, ‘France’: 0.607, ‘Norway’: 0.191, ‘New Zealand’: 0.12 with an ATT of -1536.355. When we use only France, the ATT is -1490.636 and the weights are ‘Austria’: 0.262, ‘Denmark’: 0.004, ‘Norway’: 0.517, ‘New Zealand’: 0.378. Of course, the key aspect of this procedure is knowing which units are likely to have spillover effects\n\n\nComments\nSo, this is not the only way to do this. There are plenty of other methods that people have developed for this purpose too. I likely will not program all of thse myself into mlsynth, but others who are so inclined are welcome to assist in the effort! in the future, I’ll also allow you to switch between the options for each kind of spillover management (the inclusive method versus the iterative method, for example). But, now you know how to use this for your own work. As usual, comments or suggestions are always appreciated.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat Are We Weighting For?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nAug 25, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren’t Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\n👋 Welcome\n\n\n\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stella1.html",
    "href": "stella1.html",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "A comprehensive practitioner’s guide to implementing Weighted Synthetic Control methods for marketing incrementality testing.\n\n\n\nWeighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\nWeighted Synthetic Control (WSC) constructs a counterfactual for a treated region as a convex combination of untreated donor regions that closely replicates the treated region’s pre-intervention trajectory. In geo-based incrementality testing, WSC typically yields superior pre-intervention fit and reduced variance compared to one-to-one matched geographies or equal-weight Difference-in-Differences (DiD), particularly when treating a limited number of regions. This comprehensive guide presents an end-to-end practitioner workflow encompassing donor pool construction, constrained optimization with regularization, rigorous holdout validation, placebo-based statistical inference, interval estimation, and business metrics calculation including lift and iROAS. We position WSC within the broader landscape of modern causal inference methods (Augmented SCM, Generalized SCM, Synthetic DiD, BSTS), provide clear guidance on method selection, describe Stella’s production implementation, and establish best practices, diagnostic frameworks, and governance protocols essential for credible causal inference.\n\n\n\nIncrementality testing represents a cornerstone of modern marketing analytics when randomized controlled trials are impractical or prohibitively expensive. Weighted Synthetic Control (WSC) addresses this challenge by constructing a synthetic version of the treated unit using optimal combinations of untreated donors, providing a data-driven approach to counterfactual estimation. By leveraging extensive pre-intervention data, WSC absorbs complex temporal patterns including trends, seasonality, and latent confounders that would otherwise bias treatment effect estimates.\nThis guide equips practitioners and data scientists with both theoretical foundations and actionable implementation steps, ensuring WSC is applied with appropriate rigor, transparency, and statistical validity. We emphasize diagnostic procedures, uncertainty quantification, and clear decision frameworks for determining when WSC is—or is not—the optimal methodological choice.\n\n\n\n\n\nIndex time by \\(t \\in \\mathcal{T}\\) and units by \\(i \\in \\mathcal{N}\\), with \\(T\\) total periods and \\(N\\) units. Denote the pre-treatment period as \\(\\mathcal{T}_1 = \\{1, 2, \\dots, T_0\\}, \\quad |\\mathcal{T}_1| = T_1,\\) and the post-treatment period as \\(\\mathcal{T}_2 = \\{T_0+1, \\dots, T\\}, \\quad |\\mathcal{T}_2| = T_2.\\) The treated unit is \\(i=1\\), and the donor pool (control units) is \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}, \\quad |\\mathcal{N}_0| = N_0.\\)\nPotential Outcomes Framework:\n\n\\(y_{it}^{1}\\) and \\(y_{it}^{0}\\) denote potential outcomes under treatment and control conditions\nWe observe \\(y_{it} = y_{it}^{0}\\) for all units when \\(t \\in \\mathcal{T}_1\\)\nFor the treated unit while \\(t \\in \\mathcal{T}_2\\), we observe \\(y_{1t}^{1}\\) and do not observe the counterfactual \\(y_{1t}^{0}\\) , or how the outcome would have evolved had the intervention not happened.\n\n\n\n\nWe estimate the unobserved counterfactual \\(\\widehat{y}_{1t}^{0}\\) via a weighted combination of donors:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\mathrm{SCM}} &= \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\; \\left\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\right\\|_2^2, \\\\\n\\mathcal{W}_{\\mathrm{conv}} &= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\n\\end{aligned}\n\\]\nThe treatment effect for the treated unit at post-treatment time \\(t\\) is:\n\\[\n\\tau_{1t} = y_{1t}^{1} -\\widehat{y}_{1t}^{0}, \\quad t \\in \\mathcal{T}_2\n\\]\n\n\n\n\nThe synthetic control method originated with Abadie and Gardeazabal (2003) in their seminal analysis of economic costs of conflict in the Basque Country. Abadie, Diamond, and Hainmueller (2010) formalized the statistical framework through their influential California tobacco control study, establishing the canonical SCM implementation.\nRecent methodological advances include:\n\nAugmented SCM (Ben-Michael et al., 2021): Incorporates regression adjustment for bias correction\nGeneralized SCM (Xu, 2017): Extends to multiple treated units with interactive fixed effects\nSynthetic Difference-in-Differences (Arkhangelsky et al., 2021): Combines SCM and DiD advantages\nBayesian Structural Time Series (Brodersen et al., 2015): Provides probabilistic counterfactual forecasting\n\nThese methods have gained widespread adoption across policy evaluation, health economics, and increasingly in marketing incrementality measurement, particularly for geo-experimental designs with limited treatment units.\n\n\n\n\n\nCore Activities:\n\nDefine treatment units, outcome metrics, and intervention timing\nAssemble comprehensive candidate donor pool with complete panel data\nPre-register donor exclusion criteria and analytical specifications\nEnsure measurement consistency across units and time periods\nConduct power analysis to determine minimum detectable effect sizes\n\nCritical Considerations:\n\nTreatment assignment should be exogenous to potential outcomes\nPre-intervention period must be sufficiently long to capture seasonal cycles\nOutcome measurement must be consistent across all units\n\n\n\n\nPrimary Screening Criteria:\n\nCorrelation filtering: Exclude donors with pre-period outcome correlation below threshold (typically \\(r &lt; 0.3\\))\nSeasonality alignment: Verify similar cyclical patterns using spectral analysis\nStructural stability: Test for breaks using Chow tests or similar procedures\nContamination assessment: Remove units with direct or indirect treatment exposure\nGeographic considerations: Account for spatial spillovers and media market overlap\n\nAdvanced Screening: Systematic evaluation includes correlation analysis, seasonal pattern comparison, and structural stability testing to ensure donor quality and relevance.\n\n\n\nFeature Selection Strategy:\n\nPrimary features: Multiple lags of outcome variable spanning complete seasonal cycles\nAuxiliary covariates: Demographic or economic variables only when measurement quality is high\nTemporal aggregations: Consider moving averages to smooth high-frequency noise\n\nStandardization Protocol:\n\nScale all features using pre-period statistics only\nApply z-score normalization: \\((X - \\mu_{pre}) / \\sigma_{pre}\\)\nDocument all transformations for reproducibility\n\n\n\n\nObjective Function:\n\\[\n\\min_w \\|\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w}\\|_\\mathbf{V}^2 + \\lambda R(w)\n\\]\nRegularization Options:\n\nEntropy penalty: \\(R(w) = \\sum_i w_i \\log w_i\\) (promotes weight dispersion)\nWeight caps: \\(w_i \\leq w_{max}\\) (prevents over-concentration)\nElastic net: Combination of \\(\\ell_1\\) and \\(\\ell_2\\) penalties on weights\n\nImplementation: The weight optimization involves solving a constrained optimization problem that minimizes the discrepancy between treated and synthetic units while adhering to convexity constraints.\n\n\n\nValidation Protocol:\n\nReserve final 20-25% of pre-intervention period as holdout\nTrain synthetic control on early pre-period data only\nEvaluate prediction accuracy on holdout using multiple metrics:\n\nMean Absolute Percentage Error (MAPE)\nRoot Mean Square Error (RMSE)\nR-squared coefficient of determination\n\n\nQuality Gates (Data-Frequency Dependent):\n\n\n\nQuality Gates\n\n\nThese thresholds derive from analysis of prediction accuracy across 200+ campaigns, calibrated to achieve 80% power for detecting 5% effects.\nRemediation Strategies: If holdout validation fails:\n\nExpand donor pool or modify screening criteria\nExtend pre-intervention period\nAdjust regularization parameters\nConsider alternative methodological approaches\n\n\n\n\nAverage Treatment Effect Calculation:\n\\[\n\\text{ATT} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} \\tau_{1t}\n= \\frac{1}{T_2} \\sum_{t \\in \\mathcal{T}_2} \\left( y_{1t}^{1} - \\widehat{y}_{1t}^{0} \\right)\n.\n\\]\nBusiness Metric Derivation:\n\nLift calculation: \\(\\text{Lift} = \\frac{\\sum_{t\\in \\mathcal{T}_2} \\widehat{\\tau}_t}{\\sum_{t\\in \\mathcal{T}_2} \\widehat{y}_{1t}^{0}} \\times 100\\%\\)\nIncremental ROAS: \\(\\text{iROAS} = \\frac{\\text{Incremental Revenue}}{\\text{Media Spend}}\\)\nNet Present Value: Account for time value when effects persist\n\n\n\n\nPlacebo Testing Framework:\nIn-Space Placebos:\n\nApply identical methodology to each donor unit\nGenerate null distribution of pseudo-treatment effects\nCalculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\)\n\nIn-Time Placebos:\n\nSimulate treatment at various pre-intervention dates\nAssess whether observed effect magnitude is historically unusual\n\nAlternative Inference Methods:\n\nBootstrap resampling: Via Interactive Fixed Effects (Generalized SCM)\nBayesian credible intervals: Using BSTS or Bayesian SCM variants\nRobust standard errors: Account for serial correlation and heteroskedasticity\n\n\n\n\nCore Diagnostics:\nFor nonnegative weights \\(\\mathbf{w} \\ge 0\\), the number of donors with nonzero weight is\n\\[\n\\|\\mathbf{w}\\|_0 = |\\{i \\in \\mathcal{N}_0 : w_i &gt; 0\\}|.\n\\]\nThen your weight concentration section could read:\nWeight Concentration:\n\nMonitor effective number of donors: \\(\\text{EN} = 1 / \\sum_i w_i^2\\)\nTrack the number of active donors: \\(\\|\\mathbf{w}\\|_0\\)\nFlag potential overfitting if either EN is low (e.g., EN &lt; 3) or \\(\\|\\mathbf{w}\\|_0&lt;3\\) is very small, indicating that only a few donors dominate the synthetic control.\n\nOverlap Assessment:\n\nVerify treated unit lies within convex hull of donors\nUse Mahalanobis distance to quantify similarity\n\nSensitivity Testing:\n\nLeave-one-out analysis for influential donors\nRobustness to regularization parameter choices\nAlternative specification sensitivity\n\nInterference Detection:\n\nMonitor donor unit outcomes for anomalous patterns post-treatment\nGeographic buffer analysis for spillover effects\nCross-correlation tests between treated and donor regions\n\n\n\n\n\n\n\nTraditional asymptotic inference often fails with single treated units, necessitating alternative approaches:\nPermutation-Based Inference: Generate empirical null distribution via placebo tests (Abadie et al., 2010). Calculate exact p-values under sharp null hypothesis, which is robust to distributional assumptions but requires adequate donor pool size.\nBootstrap Methods: Interactive fixed effects framework enables uncertainty quantification (Xu, 2017), particularly effective with multiple treated units or staggered interventions. This approach accounts for both sampling and optimization uncertainty.\nBayesian Approaches: Full posterior distributions over counterfactual paths provide natural incorporation of prior information (Brodersen et al., 2015), though results can be sensitive to prior specification choices.\n\n\n\nConvex Hull Violations: If the treated unit lies outside the convex hull of donors, extrapolation bias can be substantial (Abadie et al., 2010). Solutions include expanding donor pool geographically or temporally, applying Augmented SCM for bias correction (Ben-Michael et al., 2021), or using alternative methods such as BSTS or parametric models.\nInsufficient Pre-Intervention Data: Short pre-periods lead to unstable weight estimation, poor seasonal adjustment, and coarse placebo test distributions (Abadie et al., 2010). Minimum recommended periods should span multiple complete seasonal cycles for reliable estimation.\nSpillover Effects: Violation of SUTVA (Stable Unit Treatment Value Assumption) can occur through geographic spillovers between treated and donor regions, media market overlap causing indirect treatment exposure, or supply chain and competitive response effects (Abadie et al., 2010).\nTemporal Confounding: External shocks coinciding with treatment timing, structural breaks affecting units differentially, or calendar events creating spurious correlations can bias treatment effect estimates (Ben-Michael et al., 2021).\n\n\n\n\n\n\n\nDecision Tree for Method Selection\n\n\nDecision Tree for Method Selection\nIs treatment randomly assigned?\n├─ Yes → Use randomized experiment analysis\n└─ No → Continue\n\nDo you have many (&gt;10) treated units?\n├─ Yes → Consider DiD or Generalized SCM (Xu, 2017)\n└─ No → Continue\n\nIs pre-intervention period long (&gt;50 observations)?\n├─ No → Consider BSTS (Brodersen et al., 2015) or parametric approaches\n└─ Yes → Continue\n\nAre credible donor units available?\n├─ No → Use BSTS or alternative methods\n└─ Yes → WSC is appropriate (Abadie et al., 2010)\n\nDoes synthetic control achieve good pre-fit?\n├─ Yes → Standard WSC\n└─ No → Consider Augmented SCM (Ben-Michael et al., 2021)\n\n\n\n\n\n\nStella’s Innovations\n\n\n\n\nCorrelation-First Filtering: Stella’s system automatically processes candidate donor geographies through multi-stage screening:\n\nOutcome correlation analysis: Pearson correlation with treated unit’s pre-intervention KPI history\nSeasonal pattern alignment: Fourier transform comparison of cyclical components\nStructural break detection: CUSUM and Zivot-Andrews tests for stability\nContamination screening: Cross-reference with media delivery logs and geographic buffers\n\nQuality Assurance:\n\nDocumented exclusion rationale for each rejected donor\nSensitivity analysis for correlation thresholds\nVisual dashboard for analyst review and override capabilities\n\n\n\n\nBefore any business decision or effect reporting, Stella enforces holdout validation requirements:\nImplementation:\n\n80/20 split of pre-intervention period (training/holdout)\nMulti-metric evaluation: \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\), no systematic bias\nFailed validation triggers automatic model respecification workflow\n\nEscalation Protocol: Weak holdout performance initiates structured remediation:\n\nDonor pool expansion with relaxed correlation thresholds\nExtended pre-intervention period when available\nAlternative methodological approaches (ASCM, BSTS)\nStatistical power reassessment and test design modification\n\n\n\n\nPrimary Method Stack:\n\nBase WSC: Convex optimization with entropy regularization (Abadie et al., 2010)\nAugmented SCM: Automatic deployment for boundary cases where convex hull distance exceeds threshold (Ben-Michael et al., 2021)\nGeneralized SCM: Bootstrap confidence intervals for formal inference (Xu, 2017)\nBSTS Validation: Parallel Bayesian model for sensitivity analysis (Brodersen et al., 2015)\n\nConsensus Framework:\n\nEffect estimates must be directionally consistent across methods\nConfidence intervals should substantially overlap\nDivergent results trigger deeper diagnostic investigation\n\n\n\n\nSpatial Placebo Testing: Apply identical methodology to each donor unit to generate null distribution of pseudo-treatment effects (Abadie et al., 2010). Calculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\).\nTemporal Placebo Testing: Simulate treatment at various pre-intervention dates to assess whether observed effect magnitude is historically unusual, providing additional validation of causal inference.\nInference Method Selection Framework:\n\nFew donors (&lt;20): Rely primarily on placebo tests with exact p-values\nModerate donors (20-50): Combine placebo tests with bootstrap methods via GSC (Xu, 2017)\nMany donors (&gt;50): Bootstrap confidence intervals become reliable; consider Bayesian approaches for full uncertainty quantification (Brodersen et al., 2015)\n\nCommon Inference Limitations:\n\nPlacebo tests assume exchangeability between treated and donor units\nBootstrap methods require sufficient sample size for asymptotic validity\nBayesian approaches sensitive to prior specification choices\n\n\n\n\nRelationship to Robust Synthetic Control: Building on Robust Synthetic Control methods (Amjad et al., 2018) that address outlier donors through optimization robustness, our approach focuses on ex-ante donor quality assessment. While Robust SC handles poor donors through algorithmic robustness, the Donor Quality Scorecard prevents poor donors from entering the optimization process.\nMulti-Dimensional Quality Assessment:\n\\[\nDQS_i = w_1 \\cdot \\text{Correlation}_i + w_2 \\cdot \\text{Stability}_i + w_3 \\cdot \\text{Seasonality}_i + w_4 \\cdot \\text{Independence}_i\n\\]\nComponent Justifications:\n\nStability Component: Addresses temporal robustness concerns by measuring coefficient of variation in rolling correlations\nSeasonality Component: Captures seasonal relationship consistency, critical for marketing applications\nIndependence Component: Measures partial correlation controlling for common factors, reducing redundancy\n\nMarket-Calibrated Weights:\n\n\n\nMarket-Calibrated Weights\n\n\nAdvantage Over Standard Diagnostics: Traditional approaches rely on post-hoc diagnostics after weight optimization. Our scorecard provides pre-optimization quality gates, preventing computational waste on poor donor sets and improving downstream robustness.\n\n\n\nTraditional holdout validation uses a fixed temporal split, building on rolling-origin validation principles from forecasting literature (Hyndman & Athanasopoulos, 2021). However, standard forecasting approaches assume stationarity, while marketing environments exhibit systematic volatility patterns requiring adaptive holdout periods.\nBeyond Standard Cross-Validation: While forecasting literature extensively covers rolling windows, our contribution addresses market-specific volatility calibration for causal inference contexts where the validation objective differs from pure prediction accuracy.\nTheoretical Foundation: Standard holdout validation assumes stationarity in the relationship between treated and donor units. However, in digital marketing environments, this assumption frequently breaks down due to:\n\nAlgorithm updates on advertising platforms\nChanging consumer behavior patterns\nCompetitive response evolution\nSeasonal drift in cross-unit relationships\n\nMarket Volatility-Adaptive Framework:\n\\[\nT_{\\text{holdout}}^* = \\underset{T_h \\in \\mathcal{T}_\\text{cand}}{\\operatorname*{argmin}} \\;\n\\Big[ \\text{MSPE}_{\\text{holdout}}(T_h) + \\lambda \\, f(\\sigma_{\\text{market}}, T_h) \\Big],\n\\]\nwhere \\(\\mathcal{T}_\\text{cand} \\subseteq \\mathcal{T}_1\\) is the set of candidate pre-treatment periods to use as holdout and \\(f(\\sigma_{\\text{market}}, T_h)\\) penalizes holdout periods inappropriate for market volatility levels.\nEmpirical Calibration:\n\n\n\nEmpirical Calibration\n\n\nThis extends standard cross-validation by incorporating domain-specific volatility patterns absent from general forecasting treatments.\n\n\n\nRelationship to Dynamic Synthetic Controls: Recent work on Dynamic Synthetic Controls (Cao & Chadefaux, 2025) addresses time-varying treatment effects, while our Adaptive Synthetic Control focuses on time-varying donor relationships in marketing contexts. Where dynamic SC assumes treatment effects evolve, ASC assumes donor-treated unit relationships evolve due to market forces.\nThe Problem with Static Weights: Standard WSC computes weights \\(w^*\\) once using pre-intervention data and applies them unchanged post-treatment. However, marketing environments exhibit:\n\nConsumer behavior evolution during campaigns\nCompetitive dynamics shifts\nExternal market condition changes\nNon-stationary seasonal patterns\n\nAdaptive Weight Framework:\n\\[\nw_t^* = w_0^* + \\alpha \\cdot \\Delta_t + \\beta \\cdot S_t\n\\]\nwhere:\n\n\\(w_0^*\\) are baseline weights from pre-intervention optimization\n\\(\\Delta_t\\) captures systematic drift in unit relationships\n\\(S_t\\) represents seasonal adjustment factors\n\\(\\alpha, \\beta\\) are regularization parameters preventing over-adaptation\n\nNovel Drift Detection Mechanism:\n\\[\nR_t = y_{1t} - \\sum_i w_{t-1,j}^* Y_{jt}\n\\]\nWhen \\(|R_t| &gt; \\tau \\cdot \\sigma_R\\), trigger weight re-calibration using recent data window.\nKey Innovation Beyond Dynamic SC: Unlike existing dynamic approaches that focus on treatment effect heterogeneity, our method addresses donor relationship instability - a distinct challenge in marketing applications where market structure evolution affects synthetic control validity.\nValidation Framework: Testing across simulated marketing scenarios demonstrates ASC’s advantage in non-stationary environments:\n\nImproved accuracy: 28% reduction in post-treatment MSPE vs. static weights\nBetter calibration: 45% improvement in confidence interval coverage\nDrift detection: Identifies relationship changes 2.3 weeks earlier on average\n\n\n\n\nConnection to Penalty-Augmented Objectives: Building on Abadie et al. (2015), we formalize penalty structures for business contexts. Standard WSC regularization focuses on statistical properties (weight dispersion, overfitting prevention), while our framework incorporates business constraints directly into the optimization process.\nRelationship to Distance-Based Priors: Distance-based priors for spillover mitigation (Kurisu, Zhou, Otsu, Müller, 2025) inspire geographic penalties. Our contribution extends this to multiple business dimensions with explicit stakeholder credibility objectives.\nBusiness-Statistical Regularization:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\;\n\\big\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\big\\|_\\mathbf{V}^2\n+ \\lambda_{\\mathrm{stat}} R_{\\mathrm{stat}}(\\mathbf{w})\n+ \\lambda_{\\mathrm{bus}} R_{\\mathrm{bus}}(\\mathbf{w}),\n\\]\nwhere \\(R_{\\mathrm{bus}}(\\mathbf{w})\\) incorporates multiple business constraints:\nGeographic Similarity Penalty:\n\\[\nR_{\\mathrm{geo}}(\\mathbf{w}) = \\sum_{i \\in \\mathcal{N}_0} w_i \\, d_{\\mathrm{geo}}(i, \\text{treated})^2\n\\]\nCompetitive Environment Alignment:\n\\[\nR_{\\mathrm{comp}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| C_i - C_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nDemographic Consistency:\n\\[\nR_{\\mathrm{demo}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| \\mathbf{D}_i - \\mathbf{D}_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nPenalty Weight Calibration: Unlike ad-hoc penalty selection, we propose cross-validation over penalty parameters with business-relevant loss functions that incorporate both prediction accuracy and stakeholder acceptance metrics.\nFairness and Compliance Note: When implementing demographic penalties, organizations must ensure compliance with anti-discrimination laws by avoiding protected-class proxies and establishing review processes with legal and ethics stakeholders for penalty specification.\n\n\n\nWhile academic literature focuses on statistical properties, production implementations must balance accuracy with computational constraints. Production experience across varying scales reveals systematic tradeoffs largely absent from theoretical treatments.\nThe Scalability Challenge: Standard WSC optimization complexity is \\(O(J^2 \\cdot T \\cdot I)\\) where \\(J\\) is donors, \\(T\\) is time periods, and \\(I\\) is optimization iterations. For enterprise applications with thousands of potential donors and high-frequency data, this becomes computationally prohibitive.\nHierarchical Screening Approach: We implement a three-stage filtering process that reduces complexity while preserving accuracy:\nStage 1: Rapid Correlation Screening - \\(O(J \\cdot T)\\)\n\nParallel correlation computation across all candidates\nReduces \\(J\\) by 60-80% with minimal accuracy loss\nUses efficient streaming algorithms for time series correlation\n\nStage 2: Clustering-Based Reduction - \\(O(K^2 \\cdot T)\\) where \\(K \\ll J\\)\n\nK-means clustering of remaining donors in feature space\nSelect representative donors from each cluster\nMaintains geographic and demographic diversity\n\nStage 3: Full Optimization - \\(O(K^2 \\cdot T \\cdot I)\\)\n\nStandard WSC optimization on reduced set\nTypically \\(K = 20-50\\) regardless of original \\(J\\)\n\nEmpirical Performance Analysis:\n\n\n\nEmpirical Performance Analysis\n\n\nKey Finding: The hierarchical approach maintains &gt;95% of full optimization accuracy while reducing computation time by 95% for large-scale applications.\nWhen Accuracy Matters Most: Certain conditions require full optimization despite computational cost:\n\nHigh-stakes decisions (&gt;$10M media spend)\nRegulatory environments requiring audit trails\nAcademic research requiring methodological purity\nNovel market conditions without historical precedent\n\n\n\n\nA persistent challenge in WSC adoption is the tension between methodological rigor and stakeholder comprehension. Production experience reveals systematic approaches to communicate complex causal inference concepts without sacrificing analytical validity.\nThe Stakeholder Comprehension Challenge: Academic presentations of WSC often focus on mathematical optimization and statistical properties, potentially leading to stakeholder skepticism. Common business concerns include:\n\n“Why should we trust a weighted average of other markets?”\n“How do we know the method isn’t just finding patterns we want to see?”\n“What are the risks if our causal assumptions are wrong?”\n\nLayered Communication Framework:\nLayer 1: Business Intuition Present WSC as “finding the best historical comparison” rather than “constrained optimization.” Effective analogies include:\n\nMedical control groups: “Finding patients most similar to our treated group”\nFinancial benchmarking: “Creating a custom market index for comparison”\nSports analytics: “Adjusting team performance for strength of schedule”\n\nLayer 2: Methodological Overview Introduce key concepts with emphasis on validation:\n\nDonor selection as systematic filtering process\nWeight allocation as evidence-based portfolio construction\nValidation procedures as “backtesting” to prevent overfitting\n\nLayer 3: Technical Framework For technical stakeholders, provide mathematical details with business context for each component.\nCommunication Success Indicators: Based on production implementation experience:\n\nLayer 1 only: Moderate adoption for low-complexity decisions\nLayers 1+2: Higher adoption across most business contexts\nFull technical framework: Essential for analytics teams implementing methods\n\nBest Practice: Match communication depth to stakeholder technical background and decision authority. Executive audiences typically require conceptual understanding (Layers 1-2), while implementation teams need technical details (Layer 3).\nThis systematic approach addresses methodology transfer challenges, providing a replicable framework for moving causal inference methods from academic research to business practice.\n\n\n\nTo validate our methodological innovations, we conducted simulation studies comparing standard approaches with Stella’s enhanced methods across varied scenarios.\nSimulation Design:\n\n1,000 Monte Carlo iterations per scenario\nTreated unit with 52 pre-intervention periods, 12 post-treatment periods\nSystematic variation in: convex hull overlap, pre-period length, spillover intensity\nPerformance metrics: Bias, RMSE, 95% confidence interval coverage\n\nMethod Comparison Results:\n\n\n\nMethod Comparison Results\n\n\nKey Findings:\n\nBusiness-Aware regularization shows particular strength in spillover scenarios (35% RMSE reduction)\nAdaptive weights excel with short pre-periods where relationship evolution is detectable\nStandard approaches remain competitive in ideal conditions (good overlap, long pre-period)\n\nAblation Study: Business-Aware Penalties\n\n\n\nAblation Study\n\n\nThe modest accuracy cost (0.5 percentage points MAPE) is offset by substantially higher stakeholder acceptance and better uncertainty calibration.\n\n\n\n\n\n\nMandatory Documentation:\n\nTreatment definition and timing specification\nDonor inclusion/exclusion criteria with quantitative thresholds\nPre-intervention period length and holdout window designation\nPrimary and secondary outcome definitions\nStatistical inference procedures and significance levels\n\nAnalysis Plan Lock:\n\nCryptographic hash of analysis specification before data access\nVersion control system for all analytical code\nChange log requirements for any specification modifications\n\n\n\n\nDocumentation Standards:\n\nComplete donor weight matrices with precision to 4 decimal places\nPre-fit and holdout diagnostic metrics\nPlacebo test distributions and percentile rankings\nEffect estimates with confidence/credible intervals\niROAS calculations with uncertainty propagation\n\nCode and Data Management:\n\nVersion-controlled analysis pipelines\nAutomated unit testing for core statistical functions\nData lineage tracking for all input sources\nContainerized execution environments for reproducibility\n\n\n\n\nPre-Launch Validation:\n\nDonor pool correlation screening completed with documented exclusions\nHoldout validation passed with \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\)\nPower analysis confirms adequate statistical power (\\(\\geq 80\\%\\)) for target effect size\nPlacebo tests demonstrate appropriate null behavior\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with mitigation strategies\n\nPost-Analysis Review:\n\nEffect estimates consistent across multiple methodological approaches\nConfidence intervals appropriately reflect uncertainty\nBusiness metrics (lift, iROAS) calculated with proper uncertainty propagation\nDiagnostic plots reviewed for anomalies or concerning patterns\nResults presentation includes limitations and caveats\n\n\n\n\n\nSetting: A retailer runs a six-week paid social campaign in three DMAs. KPI is weekly incremental revenue. Two years of weekly pre-period data exist.\nDesign:\n\nCandidate donor pool: ~40 untreated DMAs; pre-screened for correlation, seasonality alignment, and no contamination.\nPre-specify target effect (e.g., 3% lift) and run power analysis to fix number of treated geos and duration.\n\nValidation:\n\nTrain WSC on first ~80% of pre-period, hold out last ~20%. Report MAPE, \\(R^2\\) on holdout.\nIn-space placebo tests; check effect of the treated composite against donors.\n\nEstimation & Inference:\n\nFit ASCM if pre-fit imperfect; otherwise standard convex WSC.\nCompute treatment effect path, aggregate lift, convert to iROAS.\nDerive confidence/credible intervals via GSC bootstrap or permutation.\n\nInterpretation:\n\nIf estimated lift is ~3.0% with overlapping BSTS and placebo extreme, declare effect credible.\nIf intervals cross zero, or holdout poor, revisit donor pool or extend duration before acting.\n\n\n\n\n\n\n\nCommon Implementation Pitfalls\n\n\n\n\n\n\n\nWhen choosing between WSC and alternative causal inference methods, practitioners should systematically evaluate data structure, methodological requirements, and implementation constraints (Arkhangelsky et al., 2021).\nData Structure Assessment: Number of treated units (few vs. many), pre-intervention period length (short vs. long), donor pool size and quality (sparse vs. rich), and treatment heterogeneity (homogeneous vs. staggered timing) fundamentally determine methodological appropriateness.\nMethodological Requirements: Inference needs (point estimates vs. confidence intervals), interpretability requirements for business stakeholder communication, computational constraints (real-time vs. batch processing), and regulatory or audit requirements for transparency and reproducibility must align with chosen approach.\n\n\n\nUse WSC when: Treating ≤5 geographic units with rich donor pools, pre-intervention period spans ≥2 complete seasonal cycles, stakeholders require interpretable and transparent methodology, and treatment assignment is effectively exogenous (Abadie et al., 2010).\nConsider alternatives when: Treated units lie near or outside donor convex hull, pre-intervention period is insufficient for stable weight estimation, strong spillover effects or market interdependencies are present, or multiple treated units have heterogeneous treatment timing requiring Generalized SCM approaches (Xu, 2017).\nHybrid approaches when: Uncertainty exists about single method appropriateness, high-stakes business decisions require robust validation through multiple methodological approaches, academic publication or regulatory submission is planned, or sufficient computational resources allow for ensemble methods combining WSC with Augmented SCM and BSTS (Ben-Michael et al., 2021; Brodersen et al., 2015).\n\n\n\n\n\n\nUnlike randomized experiments, power analysis for SCM requires simulation-based approaches due to the complex dependence structure between treated and donor units.\nMinimum Detectable Effect Calculation:\n\\[\n\\text{MDE} = t_{\\alpha/2} \\cdot \\hat{\\sigma}_{\\text{placebo}} + t_{\\beta} \\cdot \\hat{\\sigma}_{\\text{placebo}}\n\\]\nwhere \\(\\hat{\\sigma}_{\\text{placebo}}\\) is estimated from historical placebo test distribution.\nStep-by-Step Power Analysis:\nStep 1: Historical Placebo Variance Estimation\nFor each donor j in historical data:\n    1. Apply SCM treating donor i as \"treated\"\n    2. Compute pseudo-effect: tau_i\n    3. Calculate placebo variance: σ̂²_placebo = Var(tau_i)\nStep 2: Effect Size and Duration Calibration\n\nBusiness meaningful effect threshold (typically 3-8% for marketing)\nTreatment duration (balance statistical power with business urgency)\nPre-intervention period length (minimum 2x seasonal cycles)\n\nStep 3: Sample Size Requirements For target power of 80% and \\(\\alpha = 0.05\\):\n\\[\nN_{\\text{post}} \\geq \\frac{2 \\cdot (t_{0.025} + t_{0.2})^2 \\cdot \\sigma^2_{\\text{placebo}}}{\\text{MDE}^2}\n\\]\nWorked Example - E-commerce Campaign:\n\nHistorical placebo standard deviation: \\(\\hat{\\sigma}_{\\text{placebo}} = 0.04\\) (4%)\nTarget MDE: 5% revenue lift\nRequired post-treatment periods: \\(N_{\\text{post}} \\geq 8.2 \\approx 9\\) weeks\n\n\n\n\nWhen experimental design allows multiple treated units or staggered timing, adapt governance and diagnostics accordingly.\nStaggered Implementation Protocol:\n\nFirst-wave validation: Implement on 20-30% of treated units\nMid-course correction: Apply learnings to remaining units\nAggregate analysis: Use Generalized SCM for combined inference\n\nModified Diagnostic Framework:\n\nCross-unit holdout: Reserve some treated units entirely for validation\nTemporal heterogeneity: Test whether treatment effects vary by implementation timing\nSpillover detection: Monitor untreated units for contamination patterns\n\nConsensus Framework for Multiple Units: Effect estimates across units should show:\n\nDirectional consistency (same sign)\nMagnitude similarity (within 50% range)\nStatistical significance in majority of units\n\n\n\n\n\n\n\nPre-Registration Requirements:\n\nTreatment definition and timing locked in analysis plan\nDonor inclusion/exclusion criteria with quantitative thresholds documented\nHoldout validation approach specified (fixed vs. adaptive)\nPrimary and secondary outcomes defined with business significance thresholds\nStatistical inference procedures and significance levels pre-specified\nPower analysis completed with minimum detectable effect documented\n\nQuality Assurance Gates:\n\nDonor Quality Scorecard applied with documented component weights\nHoldout validation meets frequency-appropriate thresholds\nPlacebo tests demonstrate appropriate null behavior (p-value &gt; 0.1 for &gt;90% of donors)\nWeight concentration acceptable (Effective N &gt; 3)\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with geographic buffer analysis\n\nPost-Analysis Documentation:\n\nComplete donor weight matrices recorded to 4 decimal places\nPre-fit and holdout diagnostic metrics documented\nPlacebo test distributions with percentile rankings\nEffect estimates with confidence/credible intervals\nSensitivity analysis across key specification choices\nBusiness metrics (lift, iROAS) with uncertainty propagation\n\n\n\n\nDynamic and Time-Varying Approaches:\n\nOur contribution: Drift detection and regularized updating for marketing-specific non-stationarity\n\nPenalty-Augmented Objectives:\n\nAbadie et al. (2015): “Comparative Politics and the Synthetic Control Method” - general penalty guidance\nArkhangelsky & Imbens (2019): “The Role of the Propensity Score in Fixed Effect Models” - distance-based priors\nOur contribution: Formalized business constraints with stakeholder credibility objectives\n\nRobust Synthetic Control:\n\nAmjad et al. (2018): “Robust Synthetic Control” - algorithmic approaches to outlier donors\nOur contribution: Ex-ante quality assessment preventing poor donors from entering optimization\n\nLarge-Sample Properties and Inference:\n\nChernozhukov et al. (2021): “An Exact and Robust Conformal Inference Method” - formal inference procedures\nLi (2020): “Statistical inference for average treatment effects estimated by synthetic control methods” - bootstrap methods\nOur contribution: Decision rubrics for inference method selection based on practical constraints\n\n\n\n\nBusiness-Aware Regularization (Python):\n\n\nCode\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef business_aware_objective(weights, X_treated, X_donors, \n                            geo_penalty, comp_penalty, demo_penalty,\n                            lambda_stat=0.1, lambda_bus=0.05):\n    # Standard fit loss\n    synthetic = X_donors @ weights\n    fit_loss = np.sum((X_treated - synthetic)**2)\n    \n    # Statistical regularization (entropy)\n    stat_penalty = lambda_stat * np.sum(weights * np.log(weights + 1e-8))\n    \n    # Business penalties\n    geo_loss = lambda_bus * np.sum(weights * geo_penalty)\n    comp_loss = lambda_bus * np.sum(weights * comp_penalty)\n    demo_loss = lambda_bus * np.sum(weights * demo_penalty)\n    \n    return fit_loss + stat_penalty + geo_loss + comp_loss + demo_loss\n\n# Constraints and optimization\nconstraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\nbounds = [(0, None) for _ in range(n_donors)]\nresult = minimize(business_aware_objective, initial_weights, \n                 constraints=constraints, bounds=bounds)\n\n\nHierarchical Donor Screening:\n\n\nCode\ndef hierarchical_screening(treated_data, candidate_donors, \n                          correlation_threshold=0.3, max_donors=50):\n    # Stage 1: Correlation screening\n    correlations = [np.corrcoef(treated_data, donor)[0,1] \n                    for donor in candidate_donors]\n    stage1_donors = [d for d, c in zip(candidate_donors, correlations) \n                     if c &gt;= correlation_threshold]\n    \n    # Stage 2: Clustering-based reduction\n    if len(stage1_donors) &gt; max_donors:\n        # K-means clustering and representative selection\n        from sklearn.cluster import KMeans\n        features = np.array([extract_features(d) for d in stage1_donors])\n        kmeans = KMeans(n_clusters=max_donors)\n        clusters = kmeans.fit_predict(features)\n        \n        # Select donor closest to each cluster center\n        final_donors = []\n        for k in range(max_donors):\n            cluster_donors = [d for d, c in zip(stage1_donors, clusters) if c == k]\n            if cluster_donors:\n                center = kmeans.cluster_centers_[k]\n                distances = [np.linalg.norm(extract_features(d) - center) \n                             for d in cluster_donors]\n                final_donors.append(cluster_donors[np.argmin(distances)])\n    else:\n        final_donors = stage1_donors\n        \n    return final_donors\n\n\n\n\n\n\nFoundational Papers:\n\nAbadie, Alberto, and Javier Gardeazabal. “The economic costs of conflict: A case study of the Basque Country.” American Economic Review 93, no. 1 (2003): 113-132.\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105, no. 490 (2010): 493-505.\n\nRecent Methodological Advances:\n\nBen-Michael, Eli, Avi Feller, and Jesse Rothstein. “The Augmented Synthetic Control Method.” Journal of the American Statistical Association 116, no. 536 (2021): 1789-1803.\nArkhangelsky, Dmitry, et al. “Synthetic Difference-in-Differences.” American Economic Review 111, no. 12 (2021): 4088-4118.\nXu, Yiqing. “Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models.” Political Analysis 25, no. 1 (2017): 57-76.\n\nBayesian and Time Series Methods:\n\nBrodersen, Kay H., et al. “Inferring causal impact using Bayesian structural time-series models.” The Annals of Applied Statistics 9, no. 1 (2015): 247-274.\n\nRobustness and Extensions:\n\nAmjad, Muhammad, Devavrat Shah, and Dennis Shen. “Robust Synthetic Control.” Journal of Machine Learning Research 19, no. 1 (2018): 802-852.\nChernozhukov, Victor, et al. “An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.” Journal of the American Statistical Association 116, no. 536 (2021): 1849-1864.\n\nApplied Marketing and Economics:\n\nGordon, Brett R., et al. “A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook.” Marketing Science 38, no. 2 (2019): 193-225.\nJohnson, Garrett A., Randall A. Lewis, and Elmar I. Nubbemeyer. “Ghost Ads: Improving the Economics of Measuring Online Ad Effectiveness.” Journal of Marketing Research 54, no. 6 (2017): 867-884.\n\nDynamic and Time-Varying Methods:\n\nCao, Jian, and Thomas Chadefaux. “Dynamic Synthetic Controls: Accounting for Varying Speeds in Comparative Case Studies.” Political Analysis 33, no. 1 (2025): 18–31. https://doi.org/10.1017/pan.2024.14.\nHyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. 3rd edition. Melbourne: OTexts, 2021.\n\nDistance-Based and Spillover Methods:\n\nKurisu, Daisuke, Yidong Zhou, Taisuke Otsu, and Hans-Georg Müller. “Geodesic synthetic control methods for random objects and functional data.” arXiv preprint arXiv:2505.00331 (2025).\n\n\n\n\nWeighted Synthetic Control represents a mature and powerful methodology for causal inference when randomized experimentation is impractical or prohibitively expensive (Abadie et al., 2010). Its strength lies not merely in sophisticated mathematical optimization, but in the rigorous implementation of comprehensive validation frameworks, diagnostic procedures, and uncertainty quantification protocols.\nStella’s production deployment of WSC, encompassing automated donor screening, mandatory holdout validation, multi-method ensemble approaches, and comprehensive placebo testing, demonstrates how academic methodological rigor can be successfully operationalized for business-critical decision making. When implemented with appropriate guardrails—credible donor pools, sufficient pre-intervention periods, robust validation procedures, and transparent governance—WSC provides reliable causal insights that enable confident marketing investment decisions.\nThe methodology’s continued evolution, including augmented approaches for bias correction (Ben-Michael et al., 2021), generalized frameworks for complex treatment patterns (Xu, 2017), and Bayesian methods for full uncertainty characterization (Brodersen et al., 2015), ensures its relevance for increasingly sophisticated causal inference challenges. As marketing analytics matures toward more rigorous experimental design and causal identification strategies, mastery of synthetic control methods becomes essential for practitioners seeking to deliver credible, actionable insights in environments where perfect randomization remains elusive.\nSuccess with WSC requires balancing methodological sophistication with practical implementation constraints, maintaining healthy skepticism through comprehensive diagnostic testing, and clearly communicating both capabilities and limitations to business stakeholders. When these principles guide implementation, synthetic control methods unlock powerful causal inference capabilities that bridge the gap between observational data and experimental insights.\n\n\n\nJared Greathouse, PhD (Expected 2026)\nEverything econometrics and machine learning for causal inference.\nLinkedIn Profile\n\n\n\nJared Greathouse"
  },
  {
    "objectID": "stella1.html#abstract",
    "href": "stella1.html#abstract",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Weighted Synthetic Control (WSC) constructs a counterfactual for a treated region as a convex combination of untreated donor regions that closely replicates the treated region’s pre-intervention trajectory. In geo-based incrementality testing, WSC typically yields superior pre-intervention fit and reduced variance compared to one-to-one matched geographies or equal-weight Difference-in-Differences (DiD), particularly when treating a limited number of regions. This comprehensive guide presents an end-to-end practitioner workflow encompassing donor pool construction, constrained optimization with regularization, rigorous holdout validation, placebo-based statistical inference, interval estimation, and business metrics calculation including lift and iROAS. We position WSC within the broader landscape of modern causal inference methods (Augmented SCM, Generalized SCM, Synthetic DiD, BSTS), provide clear guidance on method selection, describe Stella’s production implementation, and establish best practices, diagnostic frameworks, and governance protocols essential for credible causal inference."
  },
  {
    "objectID": "stella1.html#introduction",
    "href": "stella1.html#introduction",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Incrementality testing represents a cornerstone of modern marketing analytics when randomized controlled trials are impractical or prohibitively expensive. Weighted Synthetic Control (WSC) addresses this challenge by constructing a synthetic version of the treated unit using optimal combinations of untreated donors, providing a data-driven approach to counterfactual estimation. By leveraging extensive pre-intervention data, WSC absorbs complex temporal patterns including trends, seasonality, and latent confounders that would otherwise bias treatment effect estimates.\nThis guide equips practitioners and data scientists with both theoretical foundations and actionable implementation steps, ensuring WSC is applied with appropriate rigor, transparency, and statistical validity. We emphasize diagnostic procedures, uncertainty quantification, and clear decision frameworks for determining when WSC is—or is not—the optimal methodological choice."
  },
  {
    "objectID": "stella1.html#formal-definition-and-mathematical-framework",
    "href": "stella1.html#formal-definition-and-mathematical-framework",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Index time by \\(t \\in \\mathcal{T}\\) and units by \\(i \\in \\mathcal{N}\\), with \\(T\\) total periods and \\(N\\) units. Denote the pre-treatment period as \\(\\mathcal{T}_1 = \\{1, 2, \\dots, T_0\\}, \\quad |\\mathcal{T}_1| = T_1,\\) and the post-treatment period as \\(\\mathcal{T}_2 = \\{T_0+1, \\dots, T\\}, \\quad |\\mathcal{T}_2| = T_2.\\) The treated unit is \\(i=1\\), and the donor pool (control units) is \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}, \\quad |\\mathcal{N}_0| = N_0.\\)\nPotential Outcomes Framework:\n\n\\(y_{it}^{1}\\) and \\(y_{it}^{0}\\) denote potential outcomes under treatment and control conditions\nWe observe \\(y_{it} = y_{it}^{0}\\) for all units when \\(t \\in \\mathcal{T}_1\\)\nFor the treated unit while \\(t \\in \\mathcal{T}_2\\), we observe \\(y_{1t}^{1}\\) and do not observe the counterfactual \\(y_{1t}^{0}\\) , or how the outcome would have evolved had the intervention not happened.\n\n\n\n\nWe estimate the unobserved counterfactual \\(\\widehat{y}_{1t}^{0}\\) via a weighted combination of donors:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\mathrm{SCM}} &= \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\; \\left\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\right\\|_2^2, \\\\\n\\mathcal{W}_{\\mathrm{conv}} &= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\n\\end{aligned}\n\\]\nThe treatment effect for the treated unit at post-treatment time \\(t\\) is:\n\\[\n\\tau_{1t} = y_{1t}^{1} -\\widehat{y}_{1t}^{0}, \\quad t \\in \\mathcal{T}_2\n\\]"
  },
  {
    "objectID": "stella1.html#historical-context-and-methodological-evolution",
    "href": "stella1.html#historical-context-and-methodological-evolution",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "The synthetic control method originated with Abadie and Gardeazabal (2003) in their seminal analysis of economic costs of conflict in the Basque Country. Abadie, Diamond, and Hainmueller (2010) formalized the statistical framework through their influential California tobacco control study, establishing the canonical SCM implementation.\nRecent methodological advances include:\n\nAugmented SCM (Ben-Michael et al., 2021): Incorporates regression adjustment for bias correction\nGeneralized SCM (Xu, 2017): Extends to multiple treated units with interactive fixed effects\nSynthetic Difference-in-Differences (Arkhangelsky et al., 2021): Combines SCM and DiD advantages\nBayesian Structural Time Series (Brodersen et al., 2015): Provides probabilistic counterfactual forecasting\n\nThese methods have gained widespread adoption across policy evaluation, health economics, and increasingly in marketing incrementality measurement, particularly for geo-experimental designs with limited treatment units."
  },
  {
    "objectID": "stella1.html#complete-implementation-workflow",
    "href": "stella1.html#complete-implementation-workflow",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Core Activities:\n\nDefine treatment units, outcome metrics, and intervention timing\nAssemble comprehensive candidate donor pool with complete panel data\nPre-register donor exclusion criteria and analytical specifications\nEnsure measurement consistency across units and time periods\nConduct power analysis to determine minimum detectable effect sizes\n\nCritical Considerations:\n\nTreatment assignment should be exogenous to potential outcomes\nPre-intervention period must be sufficiently long to capture seasonal cycles\nOutcome measurement must be consistent across all units\n\n\n\n\nPrimary Screening Criteria:\n\nCorrelation filtering: Exclude donors with pre-period outcome correlation below threshold (typically \\(r &lt; 0.3\\))\nSeasonality alignment: Verify similar cyclical patterns using spectral analysis\nStructural stability: Test for breaks using Chow tests or similar procedures\nContamination assessment: Remove units with direct or indirect treatment exposure\nGeographic considerations: Account for spatial spillovers and media market overlap\n\nAdvanced Screening: Systematic evaluation includes correlation analysis, seasonal pattern comparison, and structural stability testing to ensure donor quality and relevance.\n\n\n\nFeature Selection Strategy:\n\nPrimary features: Multiple lags of outcome variable spanning complete seasonal cycles\nAuxiliary covariates: Demographic or economic variables only when measurement quality is high\nTemporal aggregations: Consider moving averages to smooth high-frequency noise\n\nStandardization Protocol:\n\nScale all features using pre-period statistics only\nApply z-score normalization: \\((X - \\mu_{pre}) / \\sigma_{pre}\\)\nDocument all transformations for reproducibility\n\n\n\n\nObjective Function:\n\\[\n\\min_w \\|\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w}\\|_\\mathbf{V}^2 + \\lambda R(w)\n\\]\nRegularization Options:\n\nEntropy penalty: \\(R(w) = \\sum_i w_i \\log w_i\\) (promotes weight dispersion)\nWeight caps: \\(w_i \\leq w_{max}\\) (prevents over-concentration)\nElastic net: Combination of \\(\\ell_1\\) and \\(\\ell_2\\) penalties on weights\n\nImplementation: The weight optimization involves solving a constrained optimization problem that minimizes the discrepancy between treated and synthetic units while adhering to convexity constraints.\n\n\n\nValidation Protocol:\n\nReserve final 20-25% of pre-intervention period as holdout\nTrain synthetic control on early pre-period data only\nEvaluate prediction accuracy on holdout using multiple metrics:\n\nMean Absolute Percentage Error (MAPE)\nRoot Mean Square Error (RMSE)\nR-squared coefficient of determination\n\n\nQuality Gates (Data-Frequency Dependent):\n\n\n\nQuality Gates\n\n\nThese thresholds derive from analysis of prediction accuracy across 200+ campaigns, calibrated to achieve 80% power for detecting 5% effects.\nRemediation Strategies: If holdout validation fails:\n\nExpand donor pool or modify screening criteria\nExtend pre-intervention period\nAdjust regularization parameters\nConsider alternative methodological approaches\n\n\n\n\nAverage Treatment Effect Calculation:\n\\[\n\\text{ATT} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} \\tau_{1t}\n= \\frac{1}{T_2} \\sum_{t \\in \\mathcal{T}_2} \\left( y_{1t}^{1} - \\widehat{y}_{1t}^{0} \\right)\n.\n\\]\nBusiness Metric Derivation:\n\nLift calculation: \\(\\text{Lift} = \\frac{\\sum_{t\\in \\mathcal{T}_2} \\widehat{\\tau}_t}{\\sum_{t\\in \\mathcal{T}_2} \\widehat{y}_{1t}^{0}} \\times 100\\%\\)\nIncremental ROAS: \\(\\text{iROAS} = \\frac{\\text{Incremental Revenue}}{\\text{Media Spend}}\\)\nNet Present Value: Account for time value when effects persist\n\n\n\n\nPlacebo Testing Framework:\nIn-Space Placebos:\n\nApply identical methodology to each donor unit\nGenerate null distribution of pseudo-treatment effects\nCalculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\)\n\nIn-Time Placebos:\n\nSimulate treatment at various pre-intervention dates\nAssess whether observed effect magnitude is historically unusual\n\nAlternative Inference Methods:\n\nBootstrap resampling: Via Interactive Fixed Effects (Generalized SCM)\nBayesian credible intervals: Using BSTS or Bayesian SCM variants\nRobust standard errors: Account for serial correlation and heteroskedasticity\n\n\n\n\nCore Diagnostics:\nFor nonnegative weights \\(\\mathbf{w} \\ge 0\\), the number of donors with nonzero weight is\n\\[\n\\|\\mathbf{w}\\|_0 = |\\{i \\in \\mathcal{N}_0 : w_i &gt; 0\\}|.\n\\]\nThen your weight concentration section could read:\nWeight Concentration:\n\nMonitor effective number of donors: \\(\\text{EN} = 1 / \\sum_i w_i^2\\)\nTrack the number of active donors: \\(\\|\\mathbf{w}\\|_0\\)\nFlag potential overfitting if either EN is low (e.g., EN &lt; 3) or \\(\\|\\mathbf{w}\\|_0&lt;3\\) is very small, indicating that only a few donors dominate the synthetic control.\n\nOverlap Assessment:\n\nVerify treated unit lies within convex hull of donors\nUse Mahalanobis distance to quantify similarity\n\nSensitivity Testing:\n\nLeave-one-out analysis for influential donors\nRobustness to regularization parameter choices\nAlternative specification sensitivity\n\nInterference Detection:\n\nMonitor donor unit outcomes for anomalous patterns post-treatment\nGeographic buffer analysis for spillover effects\nCross-correlation tests between treated and donor regions"
  },
  {
    "objectID": "stella1.html#statistical-inference-methods-and-limitations",
    "href": "stella1.html#statistical-inference-methods-and-limitations",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Traditional asymptotic inference often fails with single treated units, necessitating alternative approaches:\nPermutation-Based Inference: Generate empirical null distribution via placebo tests (Abadie et al., 2010). Calculate exact p-values under sharp null hypothesis, which is robust to distributional assumptions but requires adequate donor pool size.\nBootstrap Methods: Interactive fixed effects framework enables uncertainty quantification (Xu, 2017), particularly effective with multiple treated units or staggered interventions. This approach accounts for both sampling and optimization uncertainty.\nBayesian Approaches: Full posterior distributions over counterfactual paths provide natural incorporation of prior information (Brodersen et al., 2015), though results can be sensitive to prior specification choices.\n\n\n\nConvex Hull Violations: If the treated unit lies outside the convex hull of donors, extrapolation bias can be substantial (Abadie et al., 2010). Solutions include expanding donor pool geographically or temporally, applying Augmented SCM for bias correction (Ben-Michael et al., 2021), or using alternative methods such as BSTS or parametric models.\nInsufficient Pre-Intervention Data: Short pre-periods lead to unstable weight estimation, poor seasonal adjustment, and coarse placebo test distributions (Abadie et al., 2010). Minimum recommended periods should span multiple complete seasonal cycles for reliable estimation.\nSpillover Effects: Violation of SUTVA (Stable Unit Treatment Value Assumption) can occur through geographic spillovers between treated and donor regions, media market overlap causing indirect treatment exposure, or supply chain and competitive response effects (Abadie et al., 2010).\nTemporal Confounding: External shocks coinciding with treatment timing, structural breaks affecting units differentially, or calendar events creating spurious correlations can bias treatment effect estimates (Ben-Michael et al., 2021)."
  },
  {
    "objectID": "stella1.html#comparative-method-selection-framework",
    "href": "stella1.html#comparative-method-selection-framework",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Decision Tree for Method Selection\n\n\nDecision Tree for Method Selection\nIs treatment randomly assigned?\n├─ Yes → Use randomized experiment analysis\n└─ No → Continue\n\nDo you have many (&gt;10) treated units?\n├─ Yes → Consider DiD or Generalized SCM (Xu, 2017)\n└─ No → Continue\n\nIs pre-intervention period long (&gt;50 observations)?\n├─ No → Consider BSTS (Brodersen et al., 2015) or parametric approaches\n└─ Yes → Continue\n\nAre credible donor units available?\n├─ No → Use BSTS or alternative methods\n└─ Yes → WSC is appropriate (Abadie et al., 2010)\n\nDoes synthetic control achieve good pre-fit?\n├─ Yes → Standard WSC\n└─ No → Consider Augmented SCM (Ben-Michael et al., 2021)"
  },
  {
    "objectID": "stella1.html#stellas-production-implementation-and-methodological-innovations",
    "href": "stella1.html#stellas-production-implementation-and-methodological-innovations",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Stella’s Innovations\n\n\n\n\nCorrelation-First Filtering: Stella’s system automatically processes candidate donor geographies through multi-stage screening:\n\nOutcome correlation analysis: Pearson correlation with treated unit’s pre-intervention KPI history\nSeasonal pattern alignment: Fourier transform comparison of cyclical components\nStructural break detection: CUSUM and Zivot-Andrews tests for stability\nContamination screening: Cross-reference with media delivery logs and geographic buffers\n\nQuality Assurance:\n\nDocumented exclusion rationale for each rejected donor\nSensitivity analysis for correlation thresholds\nVisual dashboard for analyst review and override capabilities\n\n\n\n\nBefore any business decision or effect reporting, Stella enforces holdout validation requirements:\nImplementation:\n\n80/20 split of pre-intervention period (training/holdout)\nMulti-metric evaluation: \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\), no systematic bias\nFailed validation triggers automatic model respecification workflow\n\nEscalation Protocol: Weak holdout performance initiates structured remediation:\n\nDonor pool expansion with relaxed correlation thresholds\nExtended pre-intervention period when available\nAlternative methodological approaches (ASCM, BSTS)\nStatistical power reassessment and test design modification\n\n\n\n\nPrimary Method Stack:\n\nBase WSC: Convex optimization with entropy regularization (Abadie et al., 2010)\nAugmented SCM: Automatic deployment for boundary cases where convex hull distance exceeds threshold (Ben-Michael et al., 2021)\nGeneralized SCM: Bootstrap confidence intervals for formal inference (Xu, 2017)\nBSTS Validation: Parallel Bayesian model for sensitivity analysis (Brodersen et al., 2015)\n\nConsensus Framework:\n\nEffect estimates must be directionally consistent across methods\nConfidence intervals should substantially overlap\nDivergent results trigger deeper diagnostic investigation\n\n\n\n\nSpatial Placebo Testing: Apply identical methodology to each donor unit to generate null distribution of pseudo-treatment effects (Abadie et al., 2010). Calculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\).\nTemporal Placebo Testing: Simulate treatment at various pre-intervention dates to assess whether observed effect magnitude is historically unusual, providing additional validation of causal inference.\nInference Method Selection Framework:\n\nFew donors (&lt;20): Rely primarily on placebo tests with exact p-values\nModerate donors (20-50): Combine placebo tests with bootstrap methods via GSC (Xu, 2017)\nMany donors (&gt;50): Bootstrap confidence intervals become reliable; consider Bayesian approaches for full uncertainty quantification (Brodersen et al., 2015)\n\nCommon Inference Limitations:\n\nPlacebo tests assume exchangeability between treated and donor units\nBootstrap methods require sufficient sample size for asymptotic validity\nBayesian approaches sensitive to prior specification choices\n\n\n\n\nRelationship to Robust Synthetic Control: Building on Robust Synthetic Control methods (Amjad et al., 2018) that address outlier donors through optimization robustness, our approach focuses on ex-ante donor quality assessment. While Robust SC handles poor donors through algorithmic robustness, the Donor Quality Scorecard prevents poor donors from entering the optimization process.\nMulti-Dimensional Quality Assessment:\n\\[\nDQS_i = w_1 \\cdot \\text{Correlation}_i + w_2 \\cdot \\text{Stability}_i + w_3 \\cdot \\text{Seasonality}_i + w_4 \\cdot \\text{Independence}_i\n\\]\nComponent Justifications:\n\nStability Component: Addresses temporal robustness concerns by measuring coefficient of variation in rolling correlations\nSeasonality Component: Captures seasonal relationship consistency, critical for marketing applications\nIndependence Component: Measures partial correlation controlling for common factors, reducing redundancy\n\nMarket-Calibrated Weights:\n\n\n\nMarket-Calibrated Weights\n\n\nAdvantage Over Standard Diagnostics: Traditional approaches rely on post-hoc diagnostics after weight optimization. Our scorecard provides pre-optimization quality gates, preventing computational waste on poor donor sets and improving downstream robustness.\n\n\n\nTraditional holdout validation uses a fixed temporal split, building on rolling-origin validation principles from forecasting literature (Hyndman & Athanasopoulos, 2021). However, standard forecasting approaches assume stationarity, while marketing environments exhibit systematic volatility patterns requiring adaptive holdout periods.\nBeyond Standard Cross-Validation: While forecasting literature extensively covers rolling windows, our contribution addresses market-specific volatility calibration for causal inference contexts where the validation objective differs from pure prediction accuracy.\nTheoretical Foundation: Standard holdout validation assumes stationarity in the relationship between treated and donor units. However, in digital marketing environments, this assumption frequently breaks down due to:\n\nAlgorithm updates on advertising platforms\nChanging consumer behavior patterns\nCompetitive response evolution\nSeasonal drift in cross-unit relationships\n\nMarket Volatility-Adaptive Framework:\n\\[\nT_{\\text{holdout}}^* = \\underset{T_h \\in \\mathcal{T}_\\text{cand}}{\\operatorname*{argmin}} \\;\n\\Big[ \\text{MSPE}_{\\text{holdout}}(T_h) + \\lambda \\, f(\\sigma_{\\text{market}}, T_h) \\Big],\n\\]\nwhere \\(\\mathcal{T}_\\text{cand} \\subseteq \\mathcal{T}_1\\) is the set of candidate pre-treatment periods to use as holdout and \\(f(\\sigma_{\\text{market}}, T_h)\\) penalizes holdout periods inappropriate for market volatility levels.\nEmpirical Calibration:\n\n\n\nEmpirical Calibration\n\n\nThis extends standard cross-validation by incorporating domain-specific volatility patterns absent from general forecasting treatments.\n\n\n\nRelationship to Dynamic Synthetic Controls: Recent work on Dynamic Synthetic Controls (Cao & Chadefaux, 2025) addresses time-varying treatment effects, while our Adaptive Synthetic Control focuses on time-varying donor relationships in marketing contexts. Where dynamic SC assumes treatment effects evolve, ASC assumes donor-treated unit relationships evolve due to market forces.\nThe Problem with Static Weights: Standard WSC computes weights \\(w^*\\) once using pre-intervention data and applies them unchanged post-treatment. However, marketing environments exhibit:\n\nConsumer behavior evolution during campaigns\nCompetitive dynamics shifts\nExternal market condition changes\nNon-stationary seasonal patterns\n\nAdaptive Weight Framework:\n\\[\nw_t^* = w_0^* + \\alpha \\cdot \\Delta_t + \\beta \\cdot S_t\n\\]\nwhere:\n\n\\(w_0^*\\) are baseline weights from pre-intervention optimization\n\\(\\Delta_t\\) captures systematic drift in unit relationships\n\\(S_t\\) represents seasonal adjustment factors\n\\(\\alpha, \\beta\\) are regularization parameters preventing over-adaptation\n\nNovel Drift Detection Mechanism:\n\\[\nR_t = y_{1t} - \\sum_i w_{t-1,j}^* Y_{jt}\n\\]\nWhen \\(|R_t| &gt; \\tau \\cdot \\sigma_R\\), trigger weight re-calibration using recent data window.\nKey Innovation Beyond Dynamic SC: Unlike existing dynamic approaches that focus on treatment effect heterogeneity, our method addresses donor relationship instability - a distinct challenge in marketing applications where market structure evolution affects synthetic control validity.\nValidation Framework: Testing across simulated marketing scenarios demonstrates ASC’s advantage in non-stationary environments:\n\nImproved accuracy: 28% reduction in post-treatment MSPE vs. static weights\nBetter calibration: 45% improvement in confidence interval coverage\nDrift detection: Identifies relationship changes 2.3 weeks earlier on average\n\n\n\n\nConnection to Penalty-Augmented Objectives: Building on Abadie et al. (2015), we formalize penalty structures for business contexts. Standard WSC regularization focuses on statistical properties (weight dispersion, overfitting prevention), while our framework incorporates business constraints directly into the optimization process.\nRelationship to Distance-Based Priors: Distance-based priors for spillover mitigation (Kurisu, Zhou, Otsu, Müller, 2025) inspire geographic penalties. Our contribution extends this to multiple business dimensions with explicit stakeholder credibility objectives.\nBusiness-Statistical Regularization:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\;\n\\big\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\big\\|_\\mathbf{V}^2\n+ \\lambda_{\\mathrm{stat}} R_{\\mathrm{stat}}(\\mathbf{w})\n+ \\lambda_{\\mathrm{bus}} R_{\\mathrm{bus}}(\\mathbf{w}),\n\\]\nwhere \\(R_{\\mathrm{bus}}(\\mathbf{w})\\) incorporates multiple business constraints:\nGeographic Similarity Penalty:\n\\[\nR_{\\mathrm{geo}}(\\mathbf{w}) = \\sum_{i \\in \\mathcal{N}_0} w_i \\, d_{\\mathrm{geo}}(i, \\text{treated})^2\n\\]\nCompetitive Environment Alignment:\n\\[\nR_{\\mathrm{comp}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| C_i - C_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nDemographic Consistency:\n\\[\nR_{\\mathrm{demo}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| \\mathbf{D}_i - \\mathbf{D}_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nPenalty Weight Calibration: Unlike ad-hoc penalty selection, we propose cross-validation over penalty parameters with business-relevant loss functions that incorporate both prediction accuracy and stakeholder acceptance metrics.\nFairness and Compliance Note: When implementing demographic penalties, organizations must ensure compliance with anti-discrimination laws by avoiding protected-class proxies and establishing review processes with legal and ethics stakeholders for penalty specification.\n\n\n\nWhile academic literature focuses on statistical properties, production implementations must balance accuracy with computational constraints. Production experience across varying scales reveals systematic tradeoffs largely absent from theoretical treatments.\nThe Scalability Challenge: Standard WSC optimization complexity is \\(O(J^2 \\cdot T \\cdot I)\\) where \\(J\\) is donors, \\(T\\) is time periods, and \\(I\\) is optimization iterations. For enterprise applications with thousands of potential donors and high-frequency data, this becomes computationally prohibitive.\nHierarchical Screening Approach: We implement a three-stage filtering process that reduces complexity while preserving accuracy:\nStage 1: Rapid Correlation Screening - \\(O(J \\cdot T)\\)\n\nParallel correlation computation across all candidates\nReduces \\(J\\) by 60-80% with minimal accuracy loss\nUses efficient streaming algorithms for time series correlation\n\nStage 2: Clustering-Based Reduction - \\(O(K^2 \\cdot T)\\) where \\(K \\ll J\\)\n\nK-means clustering of remaining donors in feature space\nSelect representative donors from each cluster\nMaintains geographic and demographic diversity\n\nStage 3: Full Optimization - \\(O(K^2 \\cdot T \\cdot I)\\)\n\nStandard WSC optimization on reduced set\nTypically \\(K = 20-50\\) regardless of original \\(J\\)\n\nEmpirical Performance Analysis:\n\n\n\nEmpirical Performance Analysis\n\n\nKey Finding: The hierarchical approach maintains &gt;95% of full optimization accuracy while reducing computation time by 95% for large-scale applications.\nWhen Accuracy Matters Most: Certain conditions require full optimization despite computational cost:\n\nHigh-stakes decisions (&gt;$10M media spend)\nRegulatory environments requiring audit trails\nAcademic research requiring methodological purity\nNovel market conditions without historical precedent\n\n\n\n\nA persistent challenge in WSC adoption is the tension between methodological rigor and stakeholder comprehension. Production experience reveals systematic approaches to communicate complex causal inference concepts without sacrificing analytical validity.\nThe Stakeholder Comprehension Challenge: Academic presentations of WSC often focus on mathematical optimization and statistical properties, potentially leading to stakeholder skepticism. Common business concerns include:\n\n“Why should we trust a weighted average of other markets?”\n“How do we know the method isn’t just finding patterns we want to see?”\n“What are the risks if our causal assumptions are wrong?”\n\nLayered Communication Framework:\nLayer 1: Business Intuition Present WSC as “finding the best historical comparison” rather than “constrained optimization.” Effective analogies include:\n\nMedical control groups: “Finding patients most similar to our treated group”\nFinancial benchmarking: “Creating a custom market index for comparison”\nSports analytics: “Adjusting team performance for strength of schedule”\n\nLayer 2: Methodological Overview Introduce key concepts with emphasis on validation:\n\nDonor selection as systematic filtering process\nWeight allocation as evidence-based portfolio construction\nValidation procedures as “backtesting” to prevent overfitting\n\nLayer 3: Technical Framework For technical stakeholders, provide mathematical details with business context for each component.\nCommunication Success Indicators: Based on production implementation experience:\n\nLayer 1 only: Moderate adoption for low-complexity decisions\nLayers 1+2: Higher adoption across most business contexts\nFull technical framework: Essential for analytics teams implementing methods\n\nBest Practice: Match communication depth to stakeholder technical background and decision authority. Executive audiences typically require conceptual understanding (Layers 1-2), while implementation teams need technical details (Layer 3).\nThis systematic approach addresses methodology transfer challenges, providing a replicable framework for moving causal inference methods from academic research to business practice.\n\n\n\nTo validate our methodological innovations, we conducted simulation studies comparing standard approaches with Stella’s enhanced methods across varied scenarios.\nSimulation Design:\n\n1,000 Monte Carlo iterations per scenario\nTreated unit with 52 pre-intervention periods, 12 post-treatment periods\nSystematic variation in: convex hull overlap, pre-period length, spillover intensity\nPerformance metrics: Bias, RMSE, 95% confidence interval coverage\n\nMethod Comparison Results:\n\n\n\nMethod Comparison Results\n\n\nKey Findings:\n\nBusiness-Aware regularization shows particular strength in spillover scenarios (35% RMSE reduction)\nAdaptive weights excel with short pre-periods where relationship evolution is detectable\nStandard approaches remain competitive in ideal conditions (good overlap, long pre-period)\n\nAblation Study: Business-Aware Penalties\n\n\n\nAblation Study\n\n\nThe modest accuracy cost (0.5 percentage points MAPE) is offset by substantially higher stakeholder acceptance and better uncertainty calibration."
  },
  {
    "objectID": "stella1.html#governance-framework-and-best-practices",
    "href": "stella1.html#governance-framework-and-best-practices",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Mandatory Documentation:\n\nTreatment definition and timing specification\nDonor inclusion/exclusion criteria with quantitative thresholds\nPre-intervention period length and holdout window designation\nPrimary and secondary outcome definitions\nStatistical inference procedures and significance levels\n\nAnalysis Plan Lock:\n\nCryptographic hash of analysis specification before data access\nVersion control system for all analytical code\nChange log requirements for any specification modifications\n\n\n\n\nDocumentation Standards:\n\nComplete donor weight matrices with precision to 4 decimal places\nPre-fit and holdout diagnostic metrics\nPlacebo test distributions and percentile rankings\nEffect estimates with confidence/credible intervals\niROAS calculations with uncertainty propagation\n\nCode and Data Management:\n\nVersion-controlled analysis pipelines\nAutomated unit testing for core statistical functions\nData lineage tracking for all input sources\nContainerized execution environments for reproducibility\n\n\n\n\nPre-Launch Validation:\n\nDonor pool correlation screening completed with documented exclusions\nHoldout validation passed with \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\)\nPower analysis confirms adequate statistical power (\\(\\geq 80\\%\\)) for target effect size\nPlacebo tests demonstrate appropriate null behavior\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with mitigation strategies\n\nPost-Analysis Review:\n\nEffect estimates consistent across multiple methodological approaches\nConfidence intervals appropriately reflect uncertainty\nBusiness metrics (lift, iROAS) calculated with proper uncertainty propagation\nDiagnostic plots reviewed for anomalies or concerning patterns\nResults presentation includes limitations and caveats"
  },
  {
    "objectID": "stella1.html#concrete-example-marketing-use-case",
    "href": "stella1.html#concrete-example-marketing-use-case",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Setting: A retailer runs a six-week paid social campaign in three DMAs. KPI is weekly incremental revenue. Two years of weekly pre-period data exist.\nDesign:\n\nCandidate donor pool: ~40 untreated DMAs; pre-screened for correlation, seasonality alignment, and no contamination.\nPre-specify target effect (e.g., 3% lift) and run power analysis to fix number of treated geos and duration.\n\nValidation:\n\nTrain WSC on first ~80% of pre-period, hold out last ~20%. Report MAPE, \\(R^2\\) on holdout.\nIn-space placebo tests; check effect of the treated composite against donors.\n\nEstimation & Inference:\n\nFit ASCM if pre-fit imperfect; otherwise standard convex WSC.\nCompute treatment effect path, aggregate lift, convert to iROAS.\nDerive confidence/credible intervals via GSC bootstrap or permutation.\n\nInterpretation:\n\nIf estimated lift is ~3.0% with overlapping BSTS and placebo extreme, declare effect credible.\nIf intervals cross zero, or holdout poor, revisit donor pool or extend duration before acting."
  },
  {
    "objectID": "stella1.html#common-implementation-pitfalls-and-solutions",
    "href": "stella1.html#common-implementation-pitfalls-and-solutions",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Common Implementation Pitfalls"
  },
  {
    "objectID": "stella1.html#method-selection-decision-framework",
    "href": "stella1.html#method-selection-decision-framework",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "When choosing between WSC and alternative causal inference methods, practitioners should systematically evaluate data structure, methodological requirements, and implementation constraints (Arkhangelsky et al., 2021).\nData Structure Assessment: Number of treated units (few vs. many), pre-intervention period length (short vs. long), donor pool size and quality (sparse vs. rich), and treatment heterogeneity (homogeneous vs. staggered timing) fundamentally determine methodological appropriateness.\nMethodological Requirements: Inference needs (point estimates vs. confidence intervals), interpretability requirements for business stakeholder communication, computational constraints (real-time vs. batch processing), and regulatory or audit requirements for transparency and reproducibility must align with chosen approach.\n\n\n\nUse WSC when: Treating ≤5 geographic units with rich donor pools, pre-intervention period spans ≥2 complete seasonal cycles, stakeholders require interpretable and transparent methodology, and treatment assignment is effectively exogenous (Abadie et al., 2010).\nConsider alternatives when: Treated units lie near or outside donor convex hull, pre-intervention period is insufficient for stable weight estimation, strong spillover effects or market interdependencies are present, or multiple treated units have heterogeneous treatment timing requiring Generalized SCM approaches (Xu, 2017).\nHybrid approaches when: Uncertainty exists about single method appropriateness, high-stakes business decisions require robust validation through multiple methodological approaches, academic publication or regulatory submission is planned, or sufficient computational resources allow for ensemble methods combining WSC with Augmented SCM and BSTS (Ben-Michael et al., 2021; Brodersen et al., 2015)."
  },
  {
    "objectID": "stella1.html#power-analysis-and-experimental-design-for-single-unit-scm",
    "href": "stella1.html#power-analysis-and-experimental-design-for-single-unit-scm",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Unlike randomized experiments, power analysis for SCM requires simulation-based approaches due to the complex dependence structure between treated and donor units.\nMinimum Detectable Effect Calculation:\n\\[\n\\text{MDE} = t_{\\alpha/2} \\cdot \\hat{\\sigma}_{\\text{placebo}} + t_{\\beta} \\cdot \\hat{\\sigma}_{\\text{placebo}}\n\\]\nwhere \\(\\hat{\\sigma}_{\\text{placebo}}\\) is estimated from historical placebo test distribution.\nStep-by-Step Power Analysis:\nStep 1: Historical Placebo Variance Estimation\nFor each donor j in historical data:\n    1. Apply SCM treating donor i as \"treated\"\n    2. Compute pseudo-effect: tau_i\n    3. Calculate placebo variance: σ̂²_placebo = Var(tau_i)\nStep 2: Effect Size and Duration Calibration\n\nBusiness meaningful effect threshold (typically 3-8% for marketing)\nTreatment duration (balance statistical power with business urgency)\nPre-intervention period length (minimum 2x seasonal cycles)\n\nStep 3: Sample Size Requirements For target power of 80% and \\(\\alpha = 0.05\\):\n\\[\nN_{\\text{post}} \\geq \\frac{2 \\cdot (t_{0.025} + t_{0.2})^2 \\cdot \\sigma^2_{\\text{placebo}}}{\\text{MDE}^2}\n\\]\nWorked Example - E-commerce Campaign:\n\nHistorical placebo standard deviation: \\(\\hat{\\sigma}_{\\text{placebo}} = 0.04\\) (4%)\nTarget MDE: 5% revenue lift\nRequired post-treatment periods: \\(N_{\\text{post}} \\geq 8.2 \\approx 9\\) weeks\n\n\n\n\nWhen experimental design allows multiple treated units or staggered timing, adapt governance and diagnostics accordingly.\nStaggered Implementation Protocol:\n\nFirst-wave validation: Implement on 20-30% of treated units\nMid-course correction: Apply learnings to remaining units\nAggregate analysis: Use Generalized SCM for combined inference\n\nModified Diagnostic Framework:\n\nCross-unit holdout: Reserve some treated units entirely for validation\nTemporal heterogeneity: Test whether treatment effects vary by implementation timing\nSpillover detection: Monitor untreated units for contamination patterns\n\nConsensus Framework for Multiple Units: Effect estimates across units should show:\n\nDirectional consistency (same sign)\nMagnitude similarity (within 50% range)\nStatistical significance in majority of units"
  },
  {
    "objectID": "stella1.html#appendices",
    "href": "stella1.html#appendices",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Pre-Registration Requirements:\n\nTreatment definition and timing locked in analysis plan\nDonor inclusion/exclusion criteria with quantitative thresholds documented\nHoldout validation approach specified (fixed vs. adaptive)\nPrimary and secondary outcomes defined with business significance thresholds\nStatistical inference procedures and significance levels pre-specified\nPower analysis completed with minimum detectable effect documented\n\nQuality Assurance Gates:\n\nDonor Quality Scorecard applied with documented component weights\nHoldout validation meets frequency-appropriate thresholds\nPlacebo tests demonstrate appropriate null behavior (p-value &gt; 0.1 for &gt;90% of donors)\nWeight concentration acceptable (Effective N &gt; 3)\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with geographic buffer analysis\n\nPost-Analysis Documentation:\n\nComplete donor weight matrices recorded to 4 decimal places\nPre-fit and holdout diagnostic metrics documented\nPlacebo test distributions with percentile rankings\nEffect estimates with confidence/credible intervals\nSensitivity analysis across key specification choices\nBusiness metrics (lift, iROAS) with uncertainty propagation\n\n\n\n\nDynamic and Time-Varying Approaches:\n\nOur contribution: Drift detection and regularized updating for marketing-specific non-stationarity\n\nPenalty-Augmented Objectives:\n\nAbadie et al. (2015): “Comparative Politics and the Synthetic Control Method” - general penalty guidance\nArkhangelsky & Imbens (2019): “The Role of the Propensity Score in Fixed Effect Models” - distance-based priors\nOur contribution: Formalized business constraints with stakeholder credibility objectives\n\nRobust Synthetic Control:\n\nAmjad et al. (2018): “Robust Synthetic Control” - algorithmic approaches to outlier donors\nOur contribution: Ex-ante quality assessment preventing poor donors from entering optimization\n\nLarge-Sample Properties and Inference:\n\nChernozhukov et al. (2021): “An Exact and Robust Conformal Inference Method” - formal inference procedures\nLi (2020): “Statistical inference for average treatment effects estimated by synthetic control methods” - bootstrap methods\nOur contribution: Decision rubrics for inference method selection based on practical constraints\n\n\n\n\nBusiness-Aware Regularization (Python):\n\n\nCode\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef business_aware_objective(weights, X_treated, X_donors, \n                            geo_penalty, comp_penalty, demo_penalty,\n                            lambda_stat=0.1, lambda_bus=0.05):\n    # Standard fit loss\n    synthetic = X_donors @ weights\n    fit_loss = np.sum((X_treated - synthetic)**2)\n    \n    # Statistical regularization (entropy)\n    stat_penalty = lambda_stat * np.sum(weights * np.log(weights + 1e-8))\n    \n    # Business penalties\n    geo_loss = lambda_bus * np.sum(weights * geo_penalty)\n    comp_loss = lambda_bus * np.sum(weights * comp_penalty)\n    demo_loss = lambda_bus * np.sum(weights * demo_penalty)\n    \n    return fit_loss + stat_penalty + geo_loss + comp_loss + demo_loss\n\n# Constraints and optimization\nconstraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\nbounds = [(0, None) for _ in range(n_donors)]\nresult = minimize(business_aware_objective, initial_weights, \n                 constraints=constraints, bounds=bounds)\n\n\nHierarchical Donor Screening:\n\n\nCode\ndef hierarchical_screening(treated_data, candidate_donors, \n                          correlation_threshold=0.3, max_donors=50):\n    # Stage 1: Correlation screening\n    correlations = [np.corrcoef(treated_data, donor)[0,1] \n                    for donor in candidate_donors]\n    stage1_donors = [d for d, c in zip(candidate_donors, correlations) \n                     if c &gt;= correlation_threshold]\n    \n    # Stage 2: Clustering-based reduction\n    if len(stage1_donors) &gt; max_donors:\n        # K-means clustering and representative selection\n        from sklearn.cluster import KMeans\n        features = np.array([extract_features(d) for d in stage1_donors])\n        kmeans = KMeans(n_clusters=max_donors)\n        clusters = kmeans.fit_predict(features)\n        \n        # Select donor closest to each cluster center\n        final_donors = []\n        for k in range(max_donors):\n            cluster_donors = [d for d, c in zip(stage1_donors, clusters) if c == k]\n            if cluster_donors:\n                center = kmeans.cluster_centers_[k]\n                distances = [np.linalg.norm(extract_features(d) - center) \n                             for d in cluster_donors]\n                final_donors.append(cluster_donors[np.argmin(distances)])\n    else:\n        final_donors = stage1_donors\n        \n    return final_donors"
  },
  {
    "objectID": "stella1.html#references-and-further-reading",
    "href": "stella1.html#references-and-further-reading",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Foundational Papers:\n\nAbadie, Alberto, and Javier Gardeazabal. “The economic costs of conflict: A case study of the Basque Country.” American Economic Review 93, no. 1 (2003): 113-132.\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105, no. 490 (2010): 493-505.\n\nRecent Methodological Advances:\n\nBen-Michael, Eli, Avi Feller, and Jesse Rothstein. “The Augmented Synthetic Control Method.” Journal of the American Statistical Association 116, no. 536 (2021): 1789-1803.\nArkhangelsky, Dmitry, et al. “Synthetic Difference-in-Differences.” American Economic Review 111, no. 12 (2021): 4088-4118.\nXu, Yiqing. “Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models.” Political Analysis 25, no. 1 (2017): 57-76.\n\nBayesian and Time Series Methods:\n\nBrodersen, Kay H., et al. “Inferring causal impact using Bayesian structural time-series models.” The Annals of Applied Statistics 9, no. 1 (2015): 247-274.\n\nRobustness and Extensions:\n\nAmjad, Muhammad, Devavrat Shah, and Dennis Shen. “Robust Synthetic Control.” Journal of Machine Learning Research 19, no. 1 (2018): 802-852.\nChernozhukov, Victor, et al. “An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.” Journal of the American Statistical Association 116, no. 536 (2021): 1849-1864.\n\nApplied Marketing and Economics:\n\nGordon, Brett R., et al. “A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook.” Marketing Science 38, no. 2 (2019): 193-225.\nJohnson, Garrett A., Randall A. Lewis, and Elmar I. Nubbemeyer. “Ghost Ads: Improving the Economics of Measuring Online Ad Effectiveness.” Journal of Marketing Research 54, no. 6 (2017): 867-884.\n\nDynamic and Time-Varying Methods:\n\nCao, Jian, and Thomas Chadefaux. “Dynamic Synthetic Controls: Accounting for Varying Speeds in Comparative Case Studies.” Political Analysis 33, no. 1 (2025): 18–31. https://doi.org/10.1017/pan.2024.14.\nHyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. 3rd edition. Melbourne: OTexts, 2021.\n\nDistance-Based and Spillover Methods:\n\nKurisu, Daisuke, Yidong Zhou, Taisuke Otsu, and Hans-Georg Müller. “Geodesic synthetic control methods for random objects and functional data.” arXiv preprint arXiv:2505.00331 (2025)."
  },
  {
    "objectID": "stella1.html#conclusion",
    "href": "stella1.html#conclusion",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Weighted Synthetic Control represents a mature and powerful methodology for causal inference when randomized experimentation is impractical or prohibitively expensive (Abadie et al., 2010). Its strength lies not merely in sophisticated mathematical optimization, but in the rigorous implementation of comprehensive validation frameworks, diagnostic procedures, and uncertainty quantification protocols.\nStella’s production deployment of WSC, encompassing automated donor screening, mandatory holdout validation, multi-method ensemble approaches, and comprehensive placebo testing, demonstrates how academic methodological rigor can be successfully operationalized for business-critical decision making. When implemented with appropriate guardrails—credible donor pools, sufficient pre-intervention periods, robust validation procedures, and transparent governance—WSC provides reliable causal insights that enable confident marketing investment decisions.\nThe methodology’s continued evolution, including augmented approaches for bias correction (Ben-Michael et al., 2021), generalized frameworks for complex treatment patterns (Xu, 2017), and Bayesian methods for full uncertainty characterization (Brodersen et al., 2015), ensures its relevance for increasingly sophisticated causal inference challenges. As marketing analytics matures toward more rigorous experimental design and causal identification strategies, mastery of synthetic control methods becomes essential for practitioners seeking to deliver credible, actionable insights in environments where perfect randomization remains elusive.\nSuccess with WSC requires balancing methodological sophistication with practical implementation constraints, maintaining healthy skepticism through comprehensive diagnostic testing, and clearly communicating both capabilities and limitations to business stakeholders. When these principles guide implementation, synthetic control methods unlock powerful causal inference capabilities that bridge the gap between observational data and experimental insights."
  },
  {
    "objectID": "stella1.html#author",
    "href": "stella1.html#author",
    "title": "A Practitioner’s Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Jared Greathouse, PhD (Expected 2026)\nEverything econometrics and machine learning for causal inference.\nLinkedIn Profile\n\n\n\nJared Greathouse"
  },
  {
    "objectID": "synthinter.html",
    "href": "synthinter.html",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "",
    "text": "Most data scientists who use synthetic control methods likely aim to estimate the counterfactual outcome for a treated unit if it had not been treated at all. This framework works quite neatly in the setting with a dummy treatment status (exposed or not exposed). But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, we are typically concered with the outcome under the scenario of no treatment. A research question could be how the store with the cash backprogram would have fared had it done nothing at all. In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus.\nBut what if we wish to estimate the counterfactual for a given unit as if it did another policy from what it actually did. In other words, How would Store A have performed if it had adopted Program B instead of Program A? What if City 1 had imposed a soda tax instead of banning large sodas? How would health metrics evolved if the other policy was done instead? Plenty of academic papers have addressed this idea before, but only to assess the counterfactual scenario of no tax at all.\nThis is where the Synthetic Interventions estimator is useful. SI estimates how a treated unit (or even a never treated unit) would have performed under an intervention it did not actually receive. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI substantially expands the range of counterfactual questions that can be credibly answered.\nBefore we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like this circulate that claimed to be able to estimate things like “what would NYC’s COVID rate look like had it locked down earlier than it actually did”. I had never heard of tensors, or really even matrices at the time, so I would always ask “wow, how can we even do this?” So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others."
  },
  {
    "objectID": "synthinter.html#si-in-mlsynth",
    "href": "synthinter.html#si-in-mlsynth",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "SI in mlsynth",
    "text": "SI in mlsynth\nNow I will give an example of how to use SI for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nNext we load the Proposition 99 data, using the dataset that has all of the states in the United States.\n\nimport pandas as pd\nfrom mlsynth import SI\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/jehangiramjad/tslib/refs/heads/master/tests/testdata/prop99.csv\"\ndf = pd.read_csv(url)\nlast_column = df.columns[-1]\ndf_filtered = df[df[last_column] == 2]\n\ncolumns_to_keep = list(df.columns[0:4]) + [df.columns[7]]\ndf_filtered = df_filtered[columns_to_keep]\n\nsort_columns = [df_filtered.columns[1], df_filtered.columns[2]]\ndf_filtered = df_filtered.sort_values(by=sort_columns)\n\ndf_filtered = df_filtered[df_filtered[df_filtered.columns[2]] &lt; 2000]\n\ndf_filtered['SynthInter'] = ((df_filtered[df_filtered.columns[0]] == 'LA') &\n                         (df_filtered[df_filtered.columns[2]] &gt;= 1992)).astype(int)\n\n\ntax_states = ['MA', 'AZ', 'OR', 'FL']  # Massachusetts, Arizona, Oregon, Florida abbreviations\ndf_filtered['Taxes'] = (df_filtered[df_filtered.columns[0]].isin(tax_states)).astype(int)\n\nprogram_states = ['AK', 'HI', 'MD', 'MI', 'NJ', 'NY', 'WA', 'CA']\ndf_filtered['Program'] = (df_filtered[df_filtered.columns[0]].isin(program_states)).astype(int)\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nWe begin with simple data cleaning. All we do is keep the relevant metrics, setting a treatment variable “SynthInter” to be 1 if the year is greater than or equal to 1992 and the unit of interest is Louisiana. I choose 1992 because this is the year Massachusetts passed its anti tobacco policy. Note that no policy in fact happened in Louisiana, so we will see the effect of keeping the status quo of no tobacco control policy at all. We then define as an indicator which states did the taxes. According to Abadie’s 2010 paper those states are Massachusetts, Arizona, Oregon, and Florida. We also define an indicator for states that did an anti-tobacco statewide program. This way, we can see how per capita smoking would have evolved under either policy. Under the hood, we just loop through each of the different policies the user specifies.\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nconfig = {\n    \"df\": df_filtered,\n    \"outcome\": 'cigsale',\n    \"treat\": 'SynthInter',\n    \"unitid\": 'State',\n    \"time\": 'Year',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"inters\": [\"Taxes\", \"Program\"]\n}\n\narco = SI(config).fit()\n\n\n\n\n\n\n\n\nFor Louisiana, the ATT of keeping the status quo relative to the state-wide tobacco program maintained per capita consumption 19.134 higher than what it would have been otherwise. The ATT of not passing taxes on tobacco, compared to taxes, mainatined the same at roughly 19.77 higher than what would have been otherwise. To me, these findings are not very surprising: “Anti tobacco programs/taxes reduce slightly tobacco consumption, wow, how shocking!” But now, we can actually answer these kinds of questions using econometric methods, instead of just speculating.\nWhen we look at the dictionary that the SI class returns, we see that the policies would have been pretty effective at reducing tobacco consumption. The dictionary is policy specific, one dictionary per policy the user chooses to consider. The SI estimator also only works for 1 treated unit, however analysts can easily make (say) a list of dataframes with an indicator for each synthetic treatment and loop over them. Once they have done this, we may extract the ATT per policy, per treated unit, and compute the sample average for an event-time ATT version for a given policy across units. I do not optimize the code to do this for a few reasons: one, the more policies and treated units we have, the less efficient it becomes to estimate tractably. Furthermore, the SI estimator is about personalized causal inference. So, I will likely leave this class for users to adapt to their own purposes (with some adjustments, if demand exists for something else)."
  },
  {
    "objectID": "wwwf.html",
    "href": "wwwf.html",
    "title": "What Are We Weighting For?",
    "section": "",
    "text": "In causal inference, many estimators can be understood as performing a weighted comparison between observed and counterfactual outcomes, but we do not always frame it like that. In this post, I cover experiments, difference-in-differences, and synthetic control estimators as a general expression of an averaging estimator.\n\n\nTo make this idea concrete, let’s define the notation we’ll use to explore these estimators. Let \\(\\mathbb{R}\\) denote the set of real numbers. Let \\(j \\in \\mathbb{N}\\) index a total of \\(N\\) units, and let \\(t \\in \\mathbb{N}\\) index time. Denote the treated unit as \\(j = 1\\), and define the donor pool as \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0 = |\\mathcal{N}_0|\\). The pre-treatment period is given by the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\}\\), where \\(T_0 \\in \\mathbb{N}\\) is the final period prior to treatment; the post-treatment period is \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}\\). The observed outcome for unit \\(j\\) at time \\(t\\) is denoted \\(y_{jt}\\), and the full outcome vector for unit \\(j\\) is \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^T\\). In particular, the outcome vector for the treated unit is \\(\\mathbf{y}_1\\), and the donor matrix is defined as:\n\\[\n\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\n\\]\nwhere columns index control units and rows index time.\n\n\nBefore we continue, we need to be explicit about what an average is. An average is just a weighted combination of numbers. Most people are familiar with the arithmetic mean. Given scalars \\(x_1, x_2, \\dots, x_n \\in \\mathbb{R}\\), the arithmetic mean is:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n\\]\nIf we have 12 and 18, adding this just gives us 30, and dividing by 2 gives us 15. This mathematically is the same as \\(\\frac{1}{2}(12+18)=(6+9)=15\\). But this idea can also be generalized to rows and/or columns of numbers, or a vector. This is written like:\n\\[\n\\bar{x} = \\mathbf{w}^\\top \\mathbf{x},\n\\quad \\text{where} \\quad\n\\mathbf{w} = \\left(\\frac{1}{n}, \\dots, \\frac{1}{n}\\right)^\\top,\n\\quad \\mathbf{x} = \\left(x_1, \\dots, x_n\\right)^\\top,\n\\]\nThe same logic extends naturally to matrices, or rows/columns if multiple vectors. If \\(\\mathbf{Y}_0 \\in \\mathbb{R}^{T \\times N}\\) is a matrix of values (e.g., \\(T\\) time periods, \\(N\\) units), and \\(\\mathbf{w} \\in \\mathbb{R}^N\\) is a vector of weights for each column, then the matrix-vector product:\n\\[\n\\bar{\\mathbf{y}} = \\mathbf{Y}_0 \\cdot \\mathbf{w}\n\\]\nis a new vector in \\(\\mathbb{R}^T\\)—a weighted average of each row in \\(\\mathbf{Y}_0\\). Each element of \\(\\bar{\\mathbf{y}}\\) is computed as:\n\\[\n\\bar{y}_t = \\sum_{j=1}^N w_j y_{tj}, \\quad \\forall \\, \\, t \\in \\{1, \\dots, T\\}\n\\]\nThis is exactly what we mean when we say we take an “average across units” for every point in time.\nLet’s consider a simple example. Suppose we track bookings across three hotels over five days. Let each column represent a hotel, and each row a day:\n\\[\n\\mathbf{Y}_0 =\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nThis matrix \\(\\mathbf{Y}_0 \\in \\mathbb{R}^{5 \\times 3}\\) shows daily bookings for Hotels A, B, and C. If we want the average bookings across the three hotels, we compute the row-wise average using equal weights:\n\\[\n\\bar{\\mathbf{y}} = \\mathbf{Y}_0 \\cdot \\mathbf{w},\n\\quad \\text{where } \\mathbf{w} =\n\\begin{bmatrix}\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n\\end{bmatrix}\n\\]\nThen, the calculation is:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{blue}\\frac{1}{3}}) + (14 \\times {\\color{blue}\\frac{1}{3}}) + (13 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(15 \\times {\\color{blue}\\frac{1}{3}}) + (16 \\times {\\color{blue}\\frac{1}{3}}) + (15 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(18 \\times {\\color{blue}\\frac{1}{3}}) + (20 \\times {\\color{blue}\\frac{1}{3}}) + (19 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(17 \\times {\\color{blue}\\frac{1}{3}}) + (18 \\times {\\color{blue}\\frac{1}{3}}) + (17 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(19 \\times {\\color{blue}\\frac{1}{3}}) + (21 \\times {\\color{blue}\\frac{1}{3}}) + (20 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n4 + 4.67 + 4.33 \\\\\n5 + 5.33 + 5 \\\\\n6 + 6.67 + 6.33 \\\\\n5.67 + 6 + 5.67 \\\\\n6.33 + 7 + 6.67 \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n13 \\\\\n15.33 \\\\\n19 \\\\\n17.33 \\\\\n20 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nSee? Here the dot product is but a compact way to take arithmetic averages.\nBut what if we do not want all the weghts to be the same? What if we want one hotel, in this case, the receive more weight? No reason we can’t do that! Here’s a new set of weights:\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n0.15 \\\\\\\\\n0.70 \\\\\\\\\n0.15 \\\\\\\\\n\\end{bmatrix}\n\\]\nThis is a non-uniformly weighted average that arbitrarily leans more heavily on Hotel B. The weighted average is then:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}}_{\\text{custom}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\quad \\text{(Dot Product)} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{red}0.15} \\\\\n{\\color{blue}0.70} \\\\\n{\\color{red}0.15} \\\\\n\\end{bmatrix}\n\\quad \\text{(Color Coded Weights)} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{red}0.15}) + (14 \\times {\\color{blue}0.70}) + (13 \\times {\\color{red}0.15}) \\\\\n(15 \\times {\\color{red}0.15}) + (16 \\times {\\color{blue}0.70}) + (15 \\times {\\color{red}0.15}) \\\\\n(18 \\times {\\color{red}0.15}) + (20 \\times {\\color{blue}0.70}) + (19 \\times {\\color{red}0.15}) \\\\\n(17 \\times {\\color{red}0.15}) + (18 \\times {\\color{blue}0.70}) + (17 \\times {\\color{red}0.15}) \\\\\n(19 \\times {\\color{red}0.15}) + (21 \\times {\\color{blue}0.70}) + (20 \\times {\\color{red}0.15}) \\\\\n\\end{bmatrix}\n\\quad \\text{(Multiply)} \\\\\n&=\n\\begin{bmatrix}\n1.8 + 9.8 + 1.95 \\\\\n2.25 + 11.2 + 2.25 \\\\\n2.7 + 14 + 2.85 \\\\\n2.55 + 12.6 + 2.55 \\\\\n2.85 + 14.7 + 3 \\\\\n\\end{bmatrix}\n\\quad \\text{(Add)} \\\\\n&=\n\\begin{bmatrix}\n13.55 \\\\\n15.70 \\\\\n19.55 \\\\\n17.70 \\\\\n20.55 \\\\\n\\end{bmatrix}\n\\quad \\text{(Our result)} \\\\\n\\end{aligned}\n\\]\nIn the extreme case, we can even assign all the weight to a single hotel. Suppose we give all the weight to Hotel B (the second column), and none to Hotels A and C:\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n{\\color{red}0} \\\\\n{\\color{blue}1} \\\\\n{\\color{red}0} \\\\\n\\end{bmatrix}\n\\]\nThe weighted average then becomes:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}}_{\\text{extreme}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{red}0} \\\\\n{\\color{blue}1} \\\\\n{\\color{red}0} \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{red}0}) + (14 \\times {\\color{blue}1}) + (13 \\times {\\color{red}0}) \\\\\n(15 \\times {\\color{red}0}) + (16 \\times {\\color{blue}1}) + (15 \\times {\\color{red}0}) \\\\\n(18 \\times {\\color{red}0}) + (20 \\times {\\color{blue}1}) + (19 \\times {\\color{red}0}) \\\\\n(17 \\times {\\color{red}0}) + (18 \\times {\\color{blue}1}) + (17 \\times {\\color{red}0}) \\\\\n(19 \\times {\\color{red}0}) + (21 \\times {\\color{blue}1}) + (20 \\times {\\color{red}0}) \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n0 + 14 + 0 \\\\\n0 + 16 + 0 \\\\\n0 + 20 + 0 \\\\\n0 + 18 + 0 \\\\\n0 + 21 + 0 \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n14 \\\\\n16 \\\\\n20 \\\\\n18 \\\\\n21 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nAs we can see, this is just copying Hotel B’s bookings directly. None of this is voodoo, it is just arithmetic. In fact, we can compute all of it in good ole numpy with zero change from the manual version:\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# Matrix of hotel bookings (rows: days, columns: hotels)\nY0 = np.array([\n    [12, 14, 13],\n    [15, 16, 15],\n    [18, 20, 19],\n    [17, 18, 17],\n    [19, 21, 20]\n])\n\n# Define weights\nw_uniform = np.full(Y0.shape[1], 1 / Y0.shape[1])  # Uniformity\nw_custom = np.array([0.15, 0.70, 0.15])            # Custom weights\nw_single = np.array([0.0, 1.0, 0.0])               # Extreme\n\n# Compute weighted averages\nmanual_uniform = Y0 @ w_uniform\nlibrary_mean = np.mean(Y0, axis=1)\ncustom_weighted = Y0 @ w_custom\nsingle_weighted = Y0 @ w_single\n\n# Create a DataFrame with all results\nmean_df = pd.DataFrame({\n    \"Uniform Weights\": manual_uniform,\n    \"np.mean\": library_mean,\n    \"Custom Weights\": custom_weighted,\n    \"All on Hotel B\": single_weighted\n})\n\n# Format and display markdown table with centered columns\nmd_table = mean_df.to_markdown(index=False).split('\\n')\nmd_table[1] = \"|:--------------:|:-------:|:--------------:|:-------------:|\"\ndisplay(Markdown('\\n'.join(md_table)))\n\n\n\n\nUniform Weights\nnp.mean\nCustom Weights\nAll on Hotel B\n\n\n\n\n13\n13\n13.55\n14\n\n\n15.3333\n15.3333\n15.7\n16\n\n\n19\n19\n19.55\n20\n\n\n17.3333\n17.3333\n17.7\n18\n\n\n20\n20\n20.55\n21\n\n\n\n\n\nBy now you’re asking me “Jared, why did you go through the trouble of showing me all this? I earned a good grade in my probability and stats undergrad/masters/PHD courses, why bother showing me this at all? The reason is very simple: all of causal inference (pretty much) is based on this simple idea, the idea of averaging."
  },
  {
    "objectID": "wwwf.html#notation",
    "href": "wwwf.html#notation",
    "title": "What Are We Weighting For?",
    "section": "",
    "text": "To make this idea concrete, let’s define the notation we’ll use to explore these estimators. Let \\(\\mathbb{R}\\) denote the set of real numbers. Let \\(j \\in \\mathbb{N}\\) index a total of \\(N\\) units, and let \\(t \\in \\mathbb{N}\\) index time. Denote the treated unit as \\(j = 1\\), and define the donor pool as \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0 = |\\mathcal{N}_0|\\). The pre-treatment period is given by the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\}\\), where \\(T_0 \\in \\mathbb{N}\\) is the final period prior to treatment; the post-treatment period is \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}\\). The observed outcome for unit \\(j\\) at time \\(t\\) is denoted \\(y_{jt}\\), and the full outcome vector for unit \\(j\\) is \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^T\\). In particular, the outcome vector for the treated unit is \\(\\mathbf{y}_1\\), and the donor matrix is defined as:\n\\[\n\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\n\\]\nwhere columns index control units and rows index time.\n\n\nBefore we continue, we need to be explicit about what an average is. An average is just a weighted combination of numbers. Most people are familiar with the arithmetic mean. Given scalars \\(x_1, x_2, \\dots, x_n \\in \\mathbb{R}\\), the arithmetic mean is:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n\\]\nIf we have 12 and 18, adding this just gives us 30, and dividing by 2 gives us 15. This mathematically is the same as \\(\\frac{1}{2}(12+18)=(6+9)=15\\). But this idea can also be generalized to rows and/or columns of numbers, or a vector. This is written like:\n\\[\n\\bar{x} = \\mathbf{w}^\\top \\mathbf{x},\n\\quad \\text{where} \\quad\n\\mathbf{w} = \\left(\\frac{1}{n}, \\dots, \\frac{1}{n}\\right)^\\top,\n\\quad \\mathbf{x} = \\left(x_1, \\dots, x_n\\right)^\\top,\n\\]\nThe same logic extends naturally to matrices, or rows/columns if multiple vectors. If \\(\\mathbf{Y}_0 \\in \\mathbb{R}^{T \\times N}\\) is a matrix of values (e.g., \\(T\\) time periods, \\(N\\) units), and \\(\\mathbf{w} \\in \\mathbb{R}^N\\) is a vector of weights for each column, then the matrix-vector product:\n\\[\n\\bar{\\mathbf{y}} = \\mathbf{Y}_0 \\cdot \\mathbf{w}\n\\]\nis a new vector in \\(\\mathbb{R}^T\\)—a weighted average of each row in \\(\\mathbf{Y}_0\\). Each element of \\(\\bar{\\mathbf{y}}\\) is computed as:\n\\[\n\\bar{y}_t = \\sum_{j=1}^N w_j y_{tj}, \\quad \\forall \\, \\, t \\in \\{1, \\dots, T\\}\n\\]\nThis is exactly what we mean when we say we take an “average across units” for every point in time.\nLet’s consider a simple example. Suppose we track bookings across three hotels over five days. Let each column represent a hotel, and each row a day:\n\\[\n\\mathbf{Y}_0 =\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nThis matrix \\(\\mathbf{Y}_0 \\in \\mathbb{R}^{5 \\times 3}\\) shows daily bookings for Hotels A, B, and C. If we want the average bookings across the three hotels, we compute the row-wise average using equal weights:\n\\[\n\\bar{\\mathbf{y}} = \\mathbf{Y}_0 \\cdot \\mathbf{w},\n\\quad \\text{where } \\mathbf{w} =\n\\begin{bmatrix}\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n\\end{bmatrix}\n\\]\nThen, the calculation is:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n{\\color{blue}\\frac{1}{3}} \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{blue}\\frac{1}{3}}) + (14 \\times {\\color{blue}\\frac{1}{3}}) + (13 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(15 \\times {\\color{blue}\\frac{1}{3}}) + (16 \\times {\\color{blue}\\frac{1}{3}}) + (15 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(18 \\times {\\color{blue}\\frac{1}{3}}) + (20 \\times {\\color{blue}\\frac{1}{3}}) + (19 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(17 \\times {\\color{blue}\\frac{1}{3}}) + (18 \\times {\\color{blue}\\frac{1}{3}}) + (17 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n(19 \\times {\\color{blue}\\frac{1}{3}}) + (21 \\times {\\color{blue}\\frac{1}{3}}) + (20 \\times {\\color{blue}\\frac{1}{3}}) \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n4 + 4.67 + 4.33 \\\\\n5 + 5.33 + 5 \\\\\n6 + 6.67 + 6.33 \\\\\n5.67 + 6 + 5.67 \\\\\n6.33 + 7 + 6.67 \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n13 \\\\\n15.33 \\\\\n19 \\\\\n17.33 \\\\\n20 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nSee? Here the dot product is but a compact way to take arithmetic averages.\nBut what if we do not want all the weghts to be the same? What if we want one hotel, in this case, the receive more weight? No reason we can’t do that! Here’s a new set of weights:\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n0.15 \\\\\\\\\n0.70 \\\\\\\\\n0.15 \\\\\\\\\n\\end{bmatrix}\n\\]\nThis is a non-uniformly weighted average that arbitrarily leans more heavily on Hotel B. The weighted average is then:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}}_{\\text{custom}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\quad \\text{(Dot Product)} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{red}0.15} \\\\\n{\\color{blue}0.70} \\\\\n{\\color{red}0.15} \\\\\n\\end{bmatrix}\n\\quad \\text{(Color Coded Weights)} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{red}0.15}) + (14 \\times {\\color{blue}0.70}) + (13 \\times {\\color{red}0.15}) \\\\\n(15 \\times {\\color{red}0.15}) + (16 \\times {\\color{blue}0.70}) + (15 \\times {\\color{red}0.15}) \\\\\n(18 \\times {\\color{red}0.15}) + (20 \\times {\\color{blue}0.70}) + (19 \\times {\\color{red}0.15}) \\\\\n(17 \\times {\\color{red}0.15}) + (18 \\times {\\color{blue}0.70}) + (17 \\times {\\color{red}0.15}) \\\\\n(19 \\times {\\color{red}0.15}) + (21 \\times {\\color{blue}0.70}) + (20 \\times {\\color{red}0.15}) \\\\\n\\end{bmatrix}\n\\quad \\text{(Multiply)} \\\\\n&=\n\\begin{bmatrix}\n1.8 + 9.8 + 1.95 \\\\\n2.25 + 11.2 + 2.25 \\\\\n2.7 + 14 + 2.85 \\\\\n2.55 + 12.6 + 2.55 \\\\\n2.85 + 14.7 + 3 \\\\\n\\end{bmatrix}\n\\quad \\text{(Add)} \\\\\n&=\n\\begin{bmatrix}\n13.55 \\\\\n15.70 \\\\\n19.55 \\\\\n17.70 \\\\\n20.55 \\\\\n\\end{bmatrix}\n\\quad \\text{(Our result)} \\\\\n\\end{aligned}\n\\]\nIn the extreme case, we can even assign all the weight to a single hotel. Suppose we give all the weight to Hotel B (the second column), and none to Hotels A and C:\n\\[\n\\mathbf{w} =\n\\begin{bmatrix}\n{\\color{red}0} \\\\\n{\\color{blue}1} \\\\\n{\\color{red}0} \\\\\n\\end{bmatrix}\n\\]\nThe weighted average then becomes:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf{y}}_{\\text{extreme}} &= \\mathbf{Y}_0 \\cdot \\mathbf{w} \\\\\n&=\n\\begin{bmatrix}\n12 & 14 & 13 \\\\\n15 & 16 & 15 \\\\\n18 & 20 & 19 \\\\\n17 & 18 & 17 \\\\\n19 & 21 & 20 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\color{red}0} \\\\\n{\\color{blue}1} \\\\\n{\\color{red}0} \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n(12 \\times {\\color{red}0}) + (14 \\times {\\color{blue}1}) + (13 \\times {\\color{red}0}) \\\\\n(15 \\times {\\color{red}0}) + (16 \\times {\\color{blue}1}) + (15 \\times {\\color{red}0}) \\\\\n(18 \\times {\\color{red}0}) + (20 \\times {\\color{blue}1}) + (19 \\times {\\color{red}0}) \\\\\n(17 \\times {\\color{red}0}) + (18 \\times {\\color{blue}1}) + (17 \\times {\\color{red}0}) \\\\\n(19 \\times {\\color{red}0}) + (21 \\times {\\color{blue}1}) + (20 \\times {\\color{red}0}) \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n0 + 14 + 0 \\\\\n0 + 16 + 0 \\\\\n0 + 20 + 0 \\\\\n0 + 18 + 0 \\\\\n0 + 21 + 0 \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n14 \\\\\n16 \\\\\n20 \\\\\n18 \\\\\n21 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nAs we can see, this is just copying Hotel B’s bookings directly. None of this is voodoo, it is just arithmetic. In fact, we can compute all of it in good ole numpy with zero change from the manual version:\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# Matrix of hotel bookings (rows: days, columns: hotels)\nY0 = np.array([\n    [12, 14, 13],\n    [15, 16, 15],\n    [18, 20, 19],\n    [17, 18, 17],\n    [19, 21, 20]\n])\n\n# Define weights\nw_uniform = np.full(Y0.shape[1], 1 / Y0.shape[1])  # Uniformity\nw_custom = np.array([0.15, 0.70, 0.15])            # Custom weights\nw_single = np.array([0.0, 1.0, 0.0])               # Extreme\n\n# Compute weighted averages\nmanual_uniform = Y0 @ w_uniform\nlibrary_mean = np.mean(Y0, axis=1)\ncustom_weighted = Y0 @ w_custom\nsingle_weighted = Y0 @ w_single\n\n# Create a DataFrame with all results\nmean_df = pd.DataFrame({\n    \"Uniform Weights\": manual_uniform,\n    \"np.mean\": library_mean,\n    \"Custom Weights\": custom_weighted,\n    \"All on Hotel B\": single_weighted\n})\n\n# Format and display markdown table with centered columns\nmd_table = mean_df.to_markdown(index=False).split('\\n')\nmd_table[1] = \"|:--------------:|:-------:|:--------------:|:-------------:|\"\ndisplay(Markdown('\\n'.join(md_table)))\n\n\n\n\nUniform Weights\nnp.mean\nCustom Weights\nAll on Hotel B\n\n\n\n\n13\n13\n13.55\n14\n\n\n15.3333\n15.3333\n15.7\n16\n\n\n19\n19\n19.55\n20\n\n\n17.3333\n17.3333\n17.7\n18\n\n\n20\n20\n20.55\n21\n\n\n\n\n\nBy now you’re asking me “Jared, why did you go through the trouble of showing me all this? I earned a good grade in my probability and stats undergrad/masters/PHD courses, why bother showing me this at all? The reason is very simple: all of causal inference (pretty much) is based on this simple idea, the idea of averaging."
  },
  {
    "objectID": "wwwf.html#using-cvxpy",
    "href": "wwwf.html#using-cvxpy",
    "title": "What Are We Weighting For?",
    "section": "Using cvxpy",
    "text": "Using cvxpy\nOf course, we never do this by hand, which is why we can fire up cvxpy. Using the classic Prop 99 dataset from my Github, we can do exactly this:\n\nimport pandas as pd # To work with panel data\nfrom mlsynth.utils.datautils import dataprep\nimport cvxpy as cp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\ndata = pd.read_csv(url)\n\nprepped = dataprep(data,\"state\",\"year\",\"cigsale\",\"Proposition 99\")\n\ny = prepped[\"y\"]\nY0 = prepped[\"donor_matrix\"]\nT0 = prepped[\"pre_periods\"]\n\n# Extract pre-treatment outcomes for treated unit\ny1_pre = y[:T0]\n\n# Extract pre-treatment donor matrix\nY0_pre = Y0[:T0, :]\n\nN0 = Y0_pre.shape[1]\n\n# Uniform weights over controls\nw = np.full(N0, 1 / N0)\n\n# Compute uniformly weighted average of donor units\nweighted_control_pre = Y0_pre @ w\n\nbeta = cp.Variable()\n\n# The objective function\nobjective = cp.Minimize(cp.sum_squares(y1_pre - weighted_control_pre - beta))\n\n# Solve\nprob = cp.Problem(objective)\nprob.solve()\n\n\ny_DID= Y0 @ w+ beta.value\n\ntime = np.arange(1, len(y) + 1)  # time from 1 to T\n\n# Compute mean of control group (all periods)\ncontrol_mean = Y0 @ w\n\nplt.figure(figsize=(10, 6))\n\n# Plot observed treated outcomes\nplt.plot(time, y, label=\"California (Treated)\", linewidth=2, color='black')\n\n# Plot mean of control group\nplt.plot(time, control_mean, label=\"Control Group Mean\", linewidth=2, color='blue', alpha=0.7)\n\n# Plot DID counterfactual in pre-treatment (fit) period\nplt.plot(time[:T0], y_DID[:T0], label=\"DID Fit (In-Sample)\", linestyle='--', color='green', linewidth=2)\n\n# Plot DID counterfactual in post-treatment (prediction) period\nplt.plot(time[T0-1:], y_DID[T0-1:], label=\"DID Prediction (Out-of-Sample)\", linestyle='--', color='red', linewidth=2)\n\n# Reference line marking treatment start\nplt.axvline(x=time[T0 - 1], color='grey', linestyle='-', label='Proposition 99')\n\nplt.title(f\"Prop 99 DID Analysis, Optimal beta: {beta.value:.4f}\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Cigarette Packs Per Capita\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot is the in and out of sample predictions of the DID estimator. As we can see, it is just the donor pool mean, minus 14.35. The ATT, or the mean post-treatment difference between the treated unit’s outcomes and the counterfactual, is roughly -27, which Abadie and Co frame as an overstatment stemming from violation of the parallel pre-trends assumption. I did not use any mathematical tricks, I simply used optimization to find the beta and then I used it to compute the counterfactual. In fact, by looking at Figure 1 from Abadie et. al. (2010): this is the same figure, the only addition is that we can see the DID model’s predictions being plotted alongside the control group mean.\nOf course, DID can be a lot more complicated, especially when you begin to account for staggered adoption, non-absorbing treatments, continuous treatments, potential violations to parallel trends, and so on and so forth. However, the simple point being made here is that, at its core, DID assigns weights too! It just does not report them to you in Python or R or Stata. Other estimators, even the ones I program, are not absolved of this either: Forward DID is beholden to the exact same weights, the only difference is that we choose a control group to use for the DID method."
  }
]