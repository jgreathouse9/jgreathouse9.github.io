---
title: 'Data Science for Policy Analysts: On Importance of Data Generating Processes'
date: 2025-02-14
categories: [Econometrics, Causal Inference]
---

# All ATTs Are Not Created Equal

I began my dissertation last week. The first chapter studies the causal impact of Texas' repeal of the tampon tax on demand as well as consumer price savings, naturally, using synthetic control methods as one may have guessed. I was doing the literature review, [my favorite part](https://andrewpwheeler.com/2020/12/16/lit-reviews-are-almost-functionally-worthless/) of the research process, and upon doing some digging, I _had to_ write a post about this.

I found [a paper](https://doi.org/10.1038/s41562-024-01996-4) called "Why current menstrual policies do not work". Much of the framing of the paper is fine, until we get to the part about evidence regarding the tampon tax. King writes

> Similarly, a recent UK campaign to abolish a 5% ‘tampon tax’ unintentionally boosted the profits of disposable product manufacturers by about £15 million per year, without substantially reducing the cost of products for consumers,

citing [this paper](https://taxpolicy.org.uk/wp-content/assets/tampon_tax_report.pdf) by Tax Policy Associates, with the web version being at [this link](https://taxpolicy.org.uk/2022/11/10/tampontax/). I read this and though "Hmm, this is an empirical claim, I wonder what their causal methodology was. So I went to look the paper up as a bit of professional curiosity.

Their paper studies the pass through rate of abolishing the tax, in the U.K., on savings to consumers. I grab the data from their Github and plot the treated unit versus the trajectory of the controls. We can see that this is an extremely high dimensional dataset. There are 646 control units and only 36 pre-treatment periods.

```{python}
#| fig-align: center

import requests
import pandas as pd
from io import StringIO
from mlsynth.mlsynth import PDA, FDID, dataprep
import os
import matplotlib
import numpy as np
import matplotlib.pyplot as plt

def plot_treated_vs_controls(donor_matrix, treated_vector, pre_periods, title):
    """
    Plots a single treated unit against the control group.
    
    Parameters:
    - donor_matrix (numpy.ndarray): A 2D array where each column represents a control unit.
    - treated_vector (numpy.ndarray): A 1D array representing the treated unit.
    - pre_periods (int): The cutoff time index for the pre-treatment period.
    - title (str): The title of the plot.
    """
    plt.figure(figsize=(8, 5))

    # Plot control group trajectories
    plt.plot(donor_matrix, color='gray', linewidth=0.35, alpha=0.35, label='_nolegend_')

    # Plot the treated unit
    plt.plot(treated_vector, color='black', linewidth=2, label='Tampons')

    # Plot the average of control units
    average_controls = donor_matrix.mean(axis=1)
    plt.plot(average_controls, color='red', linewidth=2, label='Mean of Controls')

    # Indicate pre-treatment period cutoff
    plt.axvline(x=pre_periods, color='blue', linestyle='--', linewidth=1.5, label='Pre-Treatment Cutoff')

    # Labels and legend
    plt.title(title)
    plt.xlabel('Time Periods')
    plt.ylabel('Outcome')
    plt.legend()
    
    plt.show()


jared_theme = {
    "axes.grid": False,
    "grid.linestyle": "-",
    "grid.color": "black",
    "legend.framealpha": 1,
    "legend.facecolor": "white",
    "legend.shadow": True,
    "legend.fontsize": 14,
    "legend.title_fontsize": 16,
    "xtick.labelsize": 11,
    "ytick.labelsize": 14,
    "axes.labelsize": 14,
    "axes.titlesize": 20,
    "figure.dpi": 120,
    "axes.facecolor": "white",
    "figure.figsize": (10, 5.5),
}

matplotlib.rcParams.update(jared_theme)

def fetch_and_combine_github_csvs(owner, repo, directory):
    """
    Fetches all CSV files in the given GitHub directory that start with 'upload',
    combines them into a single DataFrame, and processes it.

    Parameters:
        owner (str): GitHub username or organization name.
        repo (str): Repository name.
        directory (str): Directory path within the repository.

    Returns:
        pd.DataFrame: Combined and processed DataFrame.
    """
    # GitHub API URL to list contents of the directory
    api_url = f'https://api.github.com/repos/{owner}/{repo}/contents/{directory}'

    # Get the directory contents
    response = requests.get(api_url)
    files = response.json()

    # Filter for CSV files that start with 'upload'
    csv_files = [file for file in files if file['name'].startswith('upload') and file['name'].endswith('.csv')]

    # Base URL for raw file content
    raw_base_url = f'https://raw.githubusercontent.com/{owner}/{repo}/main/{directory}/'

    # List to hold DataFrames
    df_list = []

    # Download and read each CSV file
    for file in csv_files:
        csv_url = raw_base_url + file['name']
        csv_response = requests.get(csv_url)
        df = pd.read_csv(StringIO(csv_response.text))
        df_list.append(df)

    # Concatenate all DataFrames
    combined_df = pd.concat(df_list, ignore_index=True)

    combined_df['INDEX_DATE'] = pd.to_datetime(combined_df['INDEX_DATE'], format='%Y%m')

    # Sort by panel and time
    combined_df = combined_df.sort_values(by=['ITEM_ID', 'INDEX_DATE'])

    # Create 'Tax' column and set to 1 if ITEM_ID == 520206 and INDEX_DATE >= Jan 2021
    combined_df['Tax'] = 0
    combined_df.loc[(combined_df['ITEM_ID'] == 520206) & (combined_df['INDEX_DATE'] >= '2021-01-01'), 'Tax'] = 1

    # Select only specific columns by index
    selected_columns = [0, 1, 2, 5, -1]  # Column indices to keep
    combined_df = combined_df.iloc[:, selected_columns]

    # Reset index
    combined_df = combined_df.reset_index(drop=True)

    # Filter only ITEM_IDs that have exactly 53 observations,  to balance our panel
    counts = combined_df['ITEM_ID'].value_counts()
    balanced_item_ids = counts[counts == 53].index  # Get ITEM_IDs with 53 observations
    combined_df = combined_df[combined_df['ITEM_ID'].isin(balanced_item_ids)]

    combined_df['ITEM_ID'] = 'Unit ' + combined_df['ITEM_ID'].astype(str)

    combined_df = combined_df.reset_index(drop=True)

    return combined_df


owner = 'DanNeidle'
repo = 'tampontax'
directory = 'ONS_data'
df = fetch_and_combine_github_csvs(owner, repo, directory)

prepped = dataprep(df, 'ITEM_ID', "INDEX_DATE", "ITEM_INDEX", 'Tax')

plot_treated_vs_controls(prepped["donor_matrix"], prepped["y"], prepped["pre_periods"], "Tampons vs. Controls")

```


To have a better sense of what is what, I'll quote the paper directly, where the authors say

> We used Office for National Statistics data to analyse tampon price changes around 1 January 2021, the date that the “tampon tax” was abolished. We were able to do this because the ONS includes tampons (but not other menstrual products) in the price quotes it samples every month to compile the consumer prices index. Since 2017, the ONS has published the full datasets for its price sampling.

Okay fine. No problems so far. The authors find "Overall, the average price for [tampons in] the period after the VAT abolition is about 1.5% less than it was beforehand." Still no issues so far. But then we check the methodology that they [link to](https://github.com/DanNeidle/tampontax/blob/main/tampon_tax_indexes_all_goods.py)... and the results were less than exciting, putting it quite politely. Why? The authors do a simple t-test. That is, a simple pre-post test which compares the mean difference of tampon prices before the abolition of the tax and after the abolition of the tax. Precisely, they write

> Apply [sic] statistical techniques to these datasets is not straightforward given the limited number of datapoints and very high degree of volatility. It was, however, thought appropriate to run an unequal variance one-sided t-test (using the python SciPy library) to compare the pricing datasets for the six months before 1 January 2021 with those for the subsequent six months.

_Thought appropriate_? By who? Who said this was a good idea? Who would look at this and say it is okay? Look, I know that real data are often messy and that we have to take steps to compensate for noise, corruption, and overall lack of cleanliness, but I must be clear about something: _no matter what_ the extant difficulties are, this is not at all the correct way to do things, and the point of this post is to explain why. To really understand what is wrong here, we have to discuss data generating processes.

## Data Generating Processes

### DID

The parallel trends assumption in DID follows directly from the two-way fixed effects model. In this setup, the outcome $y_{jt}$ is generated by:

$$
y_{jt} = \lambda_j + \delta_t + \epsilon_{jt},
$$

where $\lambda_j$ is the unit-specific fixed effect, $\delta_t$ is the time-specific fixed effect, and $\epsilon_{jt}$ is the idiosyncratic error term. For the purposes of DID, we focus on the pre-treatment period, where no treatment occurs and we observe the untreated potential outcome. Taking expectations removes the error term (assuming it has mean zero), so we have:

$$
\mathbb{E}[y_{jt}(0)] = \lambda_j + \delta_t.
$$

Consider two periods, $t$ and $t^{\prime}$, in the pre-treatment period. The expected untreated outcome at time $t$ is $\lambda_j + \delta_t$, and at time $t^{\prime}$ it is $\lambda_j + \delta_{t^{\prime}}$. The difference between these expected outcomes is:

$$
\mathbb{E}[y_{jt}(0)] - \mathbb{E}[y_{jt^{\prime}}(0)] = (\lambda_j + \delta_t) - (\lambda_j + \delta_{t^{\prime}}) = \delta_t - \delta_{t^{\prime}}.
$$

Notice that the unit fixed effect $\lambda_j$ cancels out, and the difference in expected untreated outcomes depends only on the time effects $\delta_t$ and $\delta_{t^{\prime}}$. This result applies both to the treated unit and to the control group. For the treated unit, denoted as $j = 1$, the expected difference is:

$$
\mathbb{E}[y_{1t}(0)] - \mathbb{E}[y_{1t^{\prime}}(0)] = \delta_t - \delta_{t^{\prime}}.
$$

For the control group, taking the average over all units $j \in \mathcal{N}_0$, we obtain:

$$
\mathbb{E}[y_{\mathcal{N}_0 t}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t^{\prime}}(0)] = \delta_t - \delta_{t^{\prime}}.
$$

Since the time difference is identical for both groups, the difference between the treated unit and the control group would be some constant $\alpha$ in the absence of treatment. This means that, in the absence of treatment, the difference between the treated and control units would remain constant over time. The groups do not need to have the same levels of outcomes; they only need to share the same average trends over time. We use the average of the controls as a proxy for the unit fixed effect.

### T Test

We can follow the exact same logic for the t-test under an even simpler setup. In the pre-treatment period, the expected outcome for unit $j$is:

$$
{\color{red} \mathbb{E}[y_{jt} \mid d_{jt} = 0]} = \mathbb{E}[\lambda_j + \delta_t + \epsilon_{jt} \mid d_{jt} = 0]
$$

Since $\lambda_j$and $\delta_t$are both constant for a given unit and time, the expected outcome simplifies to:

$$
{\color{red} \mathbb{E}[y_{jt} \mid d_{jt} = 0]} = \lambda_j + \delta_t + {\color{red} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]}
$$

Similarly, in the post-treatment period, the expected outcome for unit $j$is:

$$
{\color{blue} \mathbb{E}[y_{jt} \mid d_{jt} = 1]} = \mathbb{E}[\lambda_j + \delta_t + \epsilon_{jt} \mid d_{jt} = 1]
$$

Since $\lambda_j$and $\delta_t$are again constant, this simplifies to:

$$
{\color{blue} \mathbb{E}[y_{jt} \mid d_{jt} = 1]} = \lambda_j + \delta_t + {\color{blue} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]}
$$

Now, we compute the difference in expected outcomes between the pre-treatment and post-treatment periods for the treated unit

$$
{\color{blue} \mathbb{E}[y_{jt} \mid d_{jt} = 1]} -
{\color{red} \mathbb{E}[y_{jt} \mid d_{jt} = 0]}
$$
This is just the t-test estimator expressed in terms of expectations. Substituting the expressions above into this, we get:

$$
\left( \lambda_j + \delta_t + {\color{blue} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]} \right)
-
\left( \lambda_j + \delta_t + {\color{red} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]} \right)
$$

Since both $\lambda_j$(unit fixed effect) and $\delta_t$(time fixed effect) are present in both terms, they cancel each other out:

$$
{\color{blue} \mathbb{E}[y_{jt} \mid d_{jt} = 1]} -
{\color{red} \mathbb{E}[y_{jt} \mid d_{jt} = 0]}
=
{\color{blue} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]} -
{\color{red} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]}
$$

Note that this would holds even for an interactive fixed effects model.

For the pre-post t-test to be valid, we essentially must assume that the error terms $\epsilon_{jt}$are independent and identically distributed (i.i.d.) between the pre-treatment and post-treatment periods. This means that any difference in outcomes should be due solely to the treatment effect, not due to other unobserved factors.

The assumption is that the expected difference in error terms is zero:

$$
{\color{blue} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 1]} - {\color{red} \mathbb{E}[\epsilon_{jt} \mid d_{jt} = 0]} = 0
$$

Thus, the difference in expected outcomes for the treated unit simplifies to:

$$
{\color{blue} \mathbb{E}[y_{jt} \mid d_{jt} = 1]} - {\color{red} \mathbb{E}[y_{jt} \mid d_{jt} = 0]} = 0
$$

This result implies that any observed difference in outcomes between the pre- and post-treatment periods must be entirely due to the treatment effect and noise, assuming no confounding factors or time-specific shocks.

Why did I bother to derive all of this? Why should policy analysts care? Well, we care about having unbiased and consistent estimates of our treatment effect estimand. Estimators are grounded in some form of data generating process which has implications for their ability to do the task we care about. Parallel trends made by DID, while strong, is a much more defensible assumption than the one you'd need to make for a truly valid pre-post treatment t-test. DID says (even for multiple treated units under staggered adoption) that so long as our control group is parallel with respect to the treatment group, our ATT is identified. The assumptions of the t-test takes even time based confounding away- which basically is _never_ true in real life.

In fairness, the Tax Policy Associates authors do recognize these shortcomings, writing

> The prices of tampons and the other consumer goods considered in this paper will be affected by numerous factors... this makes it difficult to separate real trends from noise. More sophisticated statistical methods than a t-test are therefore not helpful (difference-in-difference and synthetic control methods were attempted, but did not produce meaningful results).

They do not share their code for either the latter, but I will share mine. I agree with them that there are plenty of confounding factors (mostly unobserved) and noise that we have to attempt to account for. In such situations, we should turn to modern econometrics instead fitting the data to a method.

## A Re-Analysis with ```mlsynth```

Here is a re-analysis of this question [using](https://mlsynth.readthedocs.io) ```mlsynth```. I use the $\ell_2$ relaxer and Forward DID (FDID).

```{python}
#| fig-align: center

treat = "Tax"
outcome = "ITEM_INDEX"
unitid = "ITEM_ID"
time = "INDEX_DATE"

config = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True,
    "method": "l2", "tau": .25
}

model = PDA(config)

arco = model.fit()
```

I won't go over the details of $\ell_2$relaxation here, as I've done that [elsewhere](https://jgreathouse9.github.io/docs/scdense.html), and I'll have to do a post onm FDID at some point. The returned ATT for $\ell_2$ relaxation we get with a tau equal to 0.25 is -2.027 with a 95% CI of [-3.01,-1.05]. For context, the authors estimate an ATT of -1.5. Next consider FDID. The ATT for FDID is -1.618, with the 95% CI being [-1.91,-1.32]. For DID, the ATT is -2.863, and the 95% CI being [-3.685,-2.041]. We can clearly see here that one, parallel trends holds for Forward DID and not for DID. The $R^2=0.83$, and for DID the $R^2=-0.312$, or, DID being even WORSE than just the empirical average of controls. We can clearly see that that the FDID method is improved by the forward-selection algorithm used to select the control group for the DID estimator. FDID selects 54 controls of the 646 unit control group. The key issue here is that my analysis is more causal. Both analyses advantage of the full control group (646 controls) instead of the dozen or so controls the original folks sort of subjectively use. $\ell_2$ relaxation and FDID in their own way addresses variance estimation using heteoskedasticity and autocorrelation adjusted CI, so we can do proper inference about the ATT.

```{python}
#| fig-align: center

# Fit Forward DID Model
config_fdid = {
    "df": df,
    "treat": treat,
    "time": time,
    "outcome": outcome,
    "unitid": unitid,
    "display_graphs": True
}

model_fdid = FDID(config_fdid)
arco_fdid = model_fdid.fit()

```

The point of this post is that we can't just shove a dataset into ```scipy``` and (metaphorically) pray for valid results. We can't say "well, parallel trends for DID doesn't hold, let's go with the old school t-test and report that" (again, we can see how PTA DOES hold when we use the improved control group). We need a little more care to be put into these questions if we mean to even try to answer them.

Writing this, I was reminded of [a post](https://statmodeling.stat.columbia.edu/2023/10/12/debate-over-effect-of-reduced-prosecutions-on-urban-homicides-also-larger-questions-about-synthetic-control-methods-in-causal-inference/) by Andrew Gelman, who writes pretty much what I think about the report above. It isn't that the question under study is silly- the question matters, but

> The problem is that the data are observational, the data are sparse and highly variable; that is, the problem is hard. And it doesn’t help when researchers are under the impression that these real difficulties can be easily resolved using canned statistical identification techniques.

Said another way, the answer you get to the question is less important than the mathematical reasoning process you use to get to the answer. All of the tests we run and models we estimate have assumptions. These assumptions fulfill identification criteria, and are themselves are usually underpinned by some theoretically motivated DGP. For objective causal inference, researchers should respect this idea, even if it means we have to employ more advanced estimators.
