[
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, many datasets we work with come in pretty csv files that are clean. And while that‚Äôs great‚Ä¶ oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible code/script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we‚Äôd need to ask AAA and pay thousands of dollars for an extended time series‚Ä¶ but now we don‚Äôt need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for the scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA‚Äôs website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is ‚Äúhttps://gasprices.aaa.com/?state=MA‚Äù. For Florida, the URL is ‚Äúhttps://gasprices.aaa.com/?state=FL‚Äù. See the pattern? There‚Äôs a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python‚Äôs requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we‚Äôve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year‚Äôs worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I‚Äôve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "\nJared Greathouse\n",
    "section": "",
    "text": "Jared Greathouse\n\n\nEconometrician. Consultant.\n\n\nüëã Welcome\nI am Jared Amani Greathouse. I offer consulting services in data science and econometrics, especially with synthetic control methods.\n\n\nüìÑ Intake Form\nTo offer me a project:\n\nDownload the Request Intake Form (DOCX). If the form does not download, copy the link and put it in your browser.\nWhen it is filled out, email it to me with a short description of the project.\nFees begin at at $250/hour.\n\n\n\nüì¨ Contact\nSchool Email: jgreathouse3@student.gsu.edu (this is the best way to reach me)\nPersonal Email: j.greathouse200@gmail.com\nWebsite: jgreathouse9.github.io"
  },
  {
    "objectID": "fscm.html",
    "href": "fscm.html",
    "title": "Forward Selected Synthetic Control",
    "section": "",
    "text": "Interpolation bias is a known issue with synthetic control models (SCMs) For valid counterfactual prediction, the donor units, or the set of units that were never exposed to an intervention, should be as similar as possible to the treated unit in the pre-treatment periods. Selecting an appropriate donor pool is therefore critical, for practitioners. However, this can be challenging in settings with many potential controls, potentially many more control units than pre-treatment periods. Practically, researchers may wish to use this method when they have a high-dimensional donor pool and may be unsure as to which donors to include to reduce the impact of interpolation biases. To this end, this blog post introduces users the Forward Selected SCM. This applies Forward Selection (FS) to choose the donor pool for a SCM before estimating out-of-sample predictions."
  },
  {
    "objectID": "fscm.html#notation",
    "href": "fscm.html#notation",
    "title": "Forward Selected Synthetic Control",
    "section": "Notation",
    "text": "Notation\nLet \\(\\mathbb{R}\\) denote the set of real numbers. A calligraphic letter, such as \\(\\mathcal{S}\\), represents a discrete set with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) represent indices for a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Let \\(j = 1\\) be the treated unit, with the set of controls being \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0\\). The pre-treatment period consists of the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\},\\) where \\(T_0\\) is the final period before treatment. Similarly, the post-treatment period is given by \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}.\\) The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), where a generic outcome vector for a given unit in the dataset is \\(\\mathbf{y}_j \\in \\mathbb{R}^T\\), where \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^{T}\\). The outcome vector for the treated unit specifically is \\(\\mathbf{y}_1\\). The donor matrix, similarly, is defined as \\(\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\), where each column indexes a donor unit and each row is indexed to a time period.\nSCM estimates the counterfactual outcome for the treated unit by solving the program\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} \\|_2^2 \\: \\forall t \\in \\mathcal{T}_1.\n\\]\nWe seek the weight vector, \\(\\mathbf{w}\\), that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. For our purposes, the space of SC weights is the \\(N_0\\)-dimensional probability simplex \\(\\Delta^{N_0} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{N_0} : \\|\\mathbf{w}\\|_1 = 1 \\right\\}.\\) Practically this means that the post-intervention predictions will never be greater than the maximum outcome of the donor pool or lower than the minimum outcome of the donor pool."
  },
  {
    "objectID": "fscm.html#step-1",
    "href": "fscm.html#step-1",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 1",
    "text": "Step 1\nFS proceeds over \\(K \\in \\mathbb{N}\\) iterations, builds a sequence of tuples, \\(\\mathbb{T} = \\{(\\mathcal{S}_K, \\text{MSE}_K) \\}_{K=1}^{N_0}\\). The tuple contains two elements: the selected donor set for the \\(K\\)-th iteration and its corresponding \\(\\text{MSE}_K\\) (or the pre-treatment mean squared error). We begin by minimizing the SCM objective function as above, cycling through each donor unit vector one at a time instead of using the full control group. We denote these as submodels, which returns \\(N_0\\) one unit SCM models. We choose the single donor unit (the nearest neighbor in this specific case) that minimizes the MSE among all the \\(N_0\\) submodels. Our first tuple, then, is built with this single donor unit and the model‚Äôs corresponding MSE\n\\[\n\\mathcal{S}_1 = \\{j^\\ast\\}, \\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0}{\\operatorname*{argmin}} \\ \\text{MSE}(\\{j\\}).\n\\]"
  },
  {
    "objectID": "fscm.html#step-2",
    "href": "fscm.html#step-2",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 2",
    "text": "Step 2\nFor \\(K=2\\), we now estimate \\(N_0-1\\) two-unit SCMs. We include the originally selected donor along with the remaining controls, one remaining donor at a time. As above, the first and second elements of the second tuple, respectively, are\n\\[\n\\mathcal{S}_2 = \\mathcal{S}_1 \\cup \\{j^\\ast\\},\\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}_{K-1}}{\\operatorname*{argmin}} \\ \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j\\}).\n\\]\nNow, we have two tuples and two MSEs to choose from."
  },
  {
    "objectID": "fscm.html#generalizing",
    "href": "fscm.html#generalizing",
    "title": "Forward Selected Synthetic Control",
    "section": "Generalizing",
    "text": "Generalizing\nThis process generalizes, continuing for the rest of the donor pool. The general form for this algorithm, then, is\n\\[\n(\\mathcal{S}_K, \\text{MSE}_K) = (\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\}, \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\})).\n\\]\nThe algorithm continues until \\(S_K=N_0\\), when there are no more donors to add. FSCM chooses the tuple with the lowest \\(\\text{MSE}\\):\n\\[\n\\mathcal{S}^{\\ast} = \\underset{(\\mathcal{S}_K, \\text{MSE}_K) \\in \\mathbb{T}}{\\operatorname*{argmin}} \\ \\text{MSE}_K.\n\\]\nas the optimal donor set, \\(\\mathcal{S}^{\\ast}\\). Note that even within \\(\\mathcal{S}^{\\ast}\\) (as we will see below), some donors may receive zero weight in the final solution. The selected donors are just the units selected for inclusion in the donor pool in the first place, they are no guarantee of the unit having positive weight. This is in contrast to methods such as Forward Difference-in-Differences or the FS panel data method. Both of these designs are available in mlsynth too, in the FDID class and PDA class with the method of fs (the default). The main difference here is that FDID can never overfit because it estimates only one parameter, whereas (in theory) FSCM and fsPDA can overfit if they end up including too many parameters in the regression model. Unclear how likely this is, since as we see below, teh FS method reduces the full donor pool to just under half of the originally selected donor units."
  },
  {
    "objectID": "fscm.html#fscm-in-mlsynth",
    "href": "fscm.html#fscm-in-mlsynth",
    "title": "Forward Selected Synthetic Control",
    "section": "FSCM in mlsynth",
    "text": "FSCM in mlsynth\nNow I will give an example of how to use FSCM for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nAnd then we load the Proposition 99 dataset and fit the model in the ususal mlsynth fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.\n\nimport pandas as pd # To work with panel data\n\nfrom IPython.display import display, Markdown # To create the table\n\nfrom mlsynth import FSCM # The method of interest\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\n# Feel free to change \"smoking\" with \"basque\" above in the URL\n\ndata = pd.read_csv(url)\n\n# Our method inputs\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = FSCM(config).fit()\n\n\n\n\n\n\n\n\nAfter estimation, we can get the weights. These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the SC, with only 6 being assigned positive weight. The ATT of Prop 99 as estimated by FSCM is -19.51 and the pre-treatment Root Mean Squared Error for FSCM is 1.66. I compared these results to the same results we get in Stata, which includes the covariates that Abadie, Diamond, and Hainmuller originally adjusted for as well as customizes the period over which to minimize the MSE. The ATT using the original method is -19.0018, and the RMSE for the pre-treatment period is 1.76. The corresponding weights using the full donor pool are 0.334 for Utah, 0.235 for Nevada, 0.2020 for Montana, Colorado 0.161, and Connecticut 0.068. So as we can see, the ATTs are very similar, and the pre-treatment prediction errors are pretty much the same. When we estimate this in Stata (omitting the auxilary covariate predictors and estimate synth2 cigsale cigsale(1988) cigsale(1980) cigsale(1975) , trunit(3) trperiod(1989) xperiod(1980(1)1988) nested fig, we get a RMSE of 4.33 and an ATT of -22.88. Furthermore, with this specification, the weights are no longer a sparse vector.\nThe point of this article is very simple. The original SCM works well, however it can be very sensitive to the inclusion of covariates, which covariates are included, what their lags are, and so on and so forth. Furthermore, there is also an issue of covariate selection in settings where we have multiple covariates that can potentially inform our selection of the donor pool. Furthermore, collecting a rich list of covariates may also not be possible in some settings. In such situations, especially without some pre-existing grount truth donor pool, analysts may apply the FSCM algorithm to guard against interpolation biases.\nAt least with the California example (and West Germany and Basque datasets, which I also tested), we can sometimes get comparable results to the baseline estimates which used multiple covariates for acceptable results (in all three of the standard test cases, FSCM actually get lower MSE than the original applications). In the Proposition 99 example, we select some of the same donor units, get a slightly better MSE and a very similar ATT without needing to fit to the covariates originally specified in the JASA paper.\nThe promise of machine-learning methods in this space is to automate away donor/predictor selection to some acceptable degree. The key thing of interest (for me, from an econometric theory perspective anyways) is which methods are best suited for this task, when do they perform well, and why. For example, it might be useful to derive bias bounds for this estimator to quantify how much the MSE should improve by compared to the original SCM and Forward DID, as has been done with clustering based methods, for example.\nA final caveat: in the original paper, Giovanni uses cross-validation to estimate this model, and he also employs the same covariates. I have not done the cross validation yet on my end, but I will very soon. As ususal, email me with questions or comments."
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we‚Äôre scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it‚Äôs just ‚Äúscrape‚Äù, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that‚Äôs the language I use, but I‚Äôm certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don‚Äôt add them and then the job ends (this is what happens if I try to run the action after it‚Äôs ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I‚Äôm updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I‚Äôm scraping (unless you‚Äôre a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I‚Äôm home to personally oversee it. The value here is that I‚Äôve manually gotten my computer to do a specific task every single day, the correct way (assuming you‚Äôve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA‚Äôs site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it‚Äôs worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you‚Äôre a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we‚Äôve automated our tasks correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Analysis, Data Science, and Causal Inference",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "nsc.html",
    "href": "nsc.html",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "",
    "text": "Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where the non-linear synthetic control method comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works using the Proposition 99 dataset."
  },
  {
    "objectID": "nsc.html#optimization-problem",
    "href": "nsc.html#optimization-problem",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Optimization Problem",
    "text": "Optimization Problem\nLet \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the outcome vector for the treated unit over the pre-treatment period \\(\\mathcal{T}_1\\), and let \\(\\mathbf{Y}_0^{\\text{pre}} \\in \\mathbb{R}^{T_0 \\times N_0}\\) be the matrix of pre-treatment outcomes for the control units indexed by \\(\\mathcal{N}_0\\). We seek a weight vector \\(\\mathbf{w} \\in \\mathbb{R}^{N_0}\\) that minimizes the following objective:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{N_0}} \\quad \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\right\\|_2^2 + a \\sum_{j \\in \\mathcal{N}_0} \\delta_j |w_j| + b \\|\\mathbf{w}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}\\|_1 = 1\n\\]\nThe first term is the standard squared reconstruction error. The second term introduces a discrepancy-weighted \\(\\ell_1\\) penalty, where the discrepancy vector \\(\\boldsymbol{\\delta} \\in \\mathbb{R}^{N_0}\\) is defined by:\n\\[\n\\delta_j = \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{y}_j^{\\text{pre}} \\right\\|_2, \\quad \\forall j \\in \\mathcal{N}_0\n\\]\nThis term penalizes control units that differ more from the treated unit in pre-treatment trends. The final term is a Tikhonov regularization penalty that shrinks the weight vector toward zero to improve stability. The parameters \\(a &gt; 0\\) and \\(b &gt; 0\\) control the strength of the \\(\\ell_1\\) and \\(\\ell_2\\) penalties, respectively. The constraint \\(\\|\\mathbf{w}\\|_1 = 1\\) retains the SC to the probability simplex \\(\\Delta^{N_0 - 1}\\), forming an affine combination of the control units."
  },
  {
    "objectID": "nsc.html#cross-validation-to-tune-a-and-b",
    "href": "nsc.html#cross-validation-to-tune-a-and-b",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Cross-Validation to Tune \\(a\\) and \\(b\\)",
    "text": "Cross-Validation to Tune \\(a\\) and \\(b\\)\nTo select optimal regularization parameters \\((a, b)\\), we perform \\(k\\)-fold cross-validation using a pseudo-treated framework. Each control unit \\(j \\in \\mathcal{N}_0\\) is treated as if it were the treated unit, and we aim to reconstruct \\(\\mathbf{y}_j^{\\text{pre}}\\) from the remaining controls. For a given pseudo-treated unit \\(j\\), let \\(\\mathcal{N}_0^{(-j)} = \\mathcal{N}_0 \\setminus \\{j\\}\\) and define the donor matrix \\(\\mathbf{Y}_0^{\\text{pre}, (-j)}\\) by removing column \\(j\\) from \\(\\mathbf{Y}_0^{\\text{pre}}\\).\nFor each fold and pseudo-treated unit, we solve:\n\\[\n\\min_{\\mathbf{w}^{(j)} \\in \\mathbb{R}^{N_0 - 1}} \\quad \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)} \\right\\|_2^2 + a \\sum_{k \\in \\mathcal{N}_0^{(-j)}} \\delta_k^{(j)} |w_k^{(j)}| + b \\|\\mathbf{w}^{(j)}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}^{(j)}\\|_1 = 1\n\\]\nwhere\n\\[\n\\delta_k^{(j)} = \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{y}_k^{\\text{pre}} \\right\\|_2, \\quad \\forall k \\in \\mathcal{N}_0^{(-j)}\n\\]\nLet \\(\\widehat{\\mathbf{y}}_j^{\\text{pre}} = \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)}\\) denote the predicted outcome. The validation error for unit \\(j\\) is given by:\n\\[\n\\text{MSE}^{(j)} = T_0^{-1} \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\widehat{\\mathbf{y}}_j^{\\text{pre}} \\right\\|_2^2\n\\]\nThe cross-validated error is averaged over all pseudo-treated units and all folds. We then select:\n\\[\n(a^\\ast, b^\\ast) = \\arg\\min_{a \\in \\mathcal{A},\\, b \\in \\mathcal{B}} \\; \\text{CVError}(a, b)\n\\]\nHere, \\(k\\) indexes the donor units in the pseudo-treated optimization problem. \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are grids of candidate values. In practice, I employ sklearn‚Äôs cross-validation utilities to accelerate computation. I had to go rogue from the original approach because on my end it was computationally intensive. This modification seems to perform comparably while dramatically improving runtime. So while the empirical results will not be the same, they are about what we‚Äôd expect."
  },
  {
    "objectID": "sccare.html",
    "href": "sccare.html",
    "title": "Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?",
    "section": "",
    "text": "I was talking with someone recently about applying SCM, and I mentioned the idea of using a richer donor pool with different kinds of donors to answer a question. I had to articulate myself kind of carefully, since they were kind of puzzled about what I meant at first. For me, the idea comes very naturally to me, where we literally do things like this all the time. My intuition however comes from thinking about these models a lot (and empirical proof/evidence), and I think it may help others.\nMany people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant‚ÄôAnna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.\nHowever, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like mlsynth or geolift on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantly, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory that leads them to using sub-optimal estimators or not doing basic checks. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective in the very first place. This is much more crucial because if you do not know when your methods are expected to work well, you will not know if they fail or why beyond ‚Äúthat does not look right‚Äù. In a sense, you are sort of blindly applying calculus and algebra without much thought as it how any of it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.\nThis is kind of a broader problem in data science in many respects. Ostensibly, there are many myths about SCM, one of which is ‚Äúthe myth that SCM/[Augmented] SCM[s] don‚Äôt have assumptions like parallel trends‚Äù. I do not know who believes this myth, and I am inclined to think that widespread belief of this myth‚Ä¶ is itself a myth. After all, even your most applied economists will tell you ‚Äúyeah, SCM definitely has assumptions‚Äù. They may not be able to articulate what they are as formally as they could OLS, for example, but everybody in my experience knows SCM has assumptions that need to be met. Of course, whether they verify them is another matter entirely (if you know anybody who truly holds these beliefs about SCM not having assumptions, direct them to me so I can chat with them).\nEither way, even there are not many people who confidently say/think ‚ÄúSCM has no assumptions‚Äù, a much better argument I think can be made that even if people do not literally think this‚Ä¶ this is certainly the way many researchers act in practice. And that is the point of this post. Knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain. As with cars, you may not know how to build an engine, but you will be well served if you know how and when to change spark plugs or fill your tire with air.\n\nBasic SCM\nPeople often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this has been revised substantially in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or latent variable model) that often serves as a motivating data generating process. The linear factor model takes the form:\n\\[\nY_{it}(0) = \\boldsymbol{\\lambda}_t^\\top \\boldsymbol{\\mu}_i + \\varepsilon_{it}.\n\\]\nWhat does this mean? It simply means that our outcome observations are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units, plus some error term. These common factors may indeed affect each unit differently (in DID, the relationship is additive, not multiplicative), but the key idea is that we can use their similarity in terms of how they interact with the factor loadings to our advantage. Econometricians all the time tell us that SCM fundamentally is about matching our treated unit‚Äôs common factors to the common factors embedded in the donor set (also see conditons 1-6 here). This idea is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual for one unit or group using only the control units that behave similarly to the treated unit. If we get a good match (usually), we are likely also matching close to the factor loadings. This idea holds regardless of what kind of control units they are.\nA more flexible idea is the fine-grained potential outcomes model. This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit‚Äôs population.\nThe practical implication of this is that we are allowed to use different donor types (comparing cities to states, for example) on the condition that they help us learn about the trajectory of the target unit in the pre-intervention period. SCM does not care about what kind of donors you use, so long as they are informative donors.\n\n\nApplication\nUnconvinced? We can replicate some results to show this using mlsynth.\nReaders likely know the classic example of Prop 99, where California‚Äôs anti-tobacco program was compared to 38 donor states to estimate California‚Äôs counterfactual per capita cigarette consumption. But California is a ridiculously large economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units?\nIt turns out that we can do just that. Here, I compare California to the donor divisions of the United States. The outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level. California‚Äôs remains as is. Just for demonstration, I compare the standard results (around an ATT of -17) to what we get when we use other estimators, except I aggregate the outcomes as I just described. Comparing across estimators, we can visually assess how well each method captures the pre and post-treatment trajectory for California using the aggregated division data, relative to the baseline result that uses the states as donors.\n\nimport pandas as pd\nfrom mlsynth import FDID, TSSC, PDA\n\n# URL of the .dta file\nurl = \"http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta\"\n\n# Load the Stata file directly from the URL\ndf = pd.read_stata(url)\n\ndf[\"Proposition 99\"] =  ((df[\"region\"] == \"California\") & (df[\"year\"].dt.year &gt;= 1989)).astype(int)\n\n# Base configuration\nbase_config = {\n    \"df\": df,\n    \"outcome\": df.columns[2],\n    \"treat\": df.columns[-1],\n    \"unitid\": df.columns[0],\n    \"time\": df.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\", \"red\"]\n}\n\n# TSSC model\ntssc_config = base_config.copy()  # Start with base and modify if necessary\narco = TSSC(tssc_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/estimators/tssc.py:408: UserWarning: Warning: Mismatch between number of donor weights (9) and names (8) for method MSCa. Donor weights will not be populated for this method.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/estimators/tssc.py:408: UserWarning: Warning: Mismatch between number of donor weights (9) and names (8) for method MSCc. Donor weights will not be populated for this method.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nHere is TSSC estimator, where we adjust for baseline differences using an intercept.\n\n# PDA model with method 'l2'\npda_config = base_config.copy()\npda_config[\"method\"] = \"l2\"\narcol2 = PDA(pda_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n\n\n\n\n\n\n\n\n\nHere is \\(\\ell_2\\)-PDA estimator, where we allow for negative weights and an intercept.\n\n# FDID model\nfdid_config = base_config.copy()\narcofdid = FDID(fdid_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  observations_per_unit = df.groupby(unit_id_column_name)[time_period_column_name].nunique() # Changed from .count() to .nunique() for robustness\n\n\n\n\n\n\n\n\n\nHere is the predictions of the FDID estimator, where we employ Forward Selection to choose the donor pool for the DID method.\n\n\nTakeaway\nThe qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California‚Äôs pre-treatment trends, and so long as we are doing that, we may generally use as many relevant donors as we like. The issue is not that donors in general do not matter. I wrote mlsynth precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit, especially in high-dimensional settings. The key issue though is that the ‚Äúright donors‚Äù do not necessarily need to be the same type of unit as the kind you are using, meaing we can be a little creative as to what we use as a donor. This is a philosophical question, not an econometric one. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.\nNote that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy. Or, you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the theoretical proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you will be able to use these methods more confidently, and with much more clarity than before.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scmo.html",
    "href": "scmo.html",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "",
    "text": "Sometimes analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may possibly predict a target/outcome variable that we care about, and plenty of other papers have commented on this fact before. Some analysts even argue for the surrogate approach, where we treat other outcomes or affected units as a kind of instrument for the counterfactual trajcectory of the target unit.\nHowever, all of this is most uncommon. As it turns out, most people in academia and industry who use synthetic controls use only a single focal outcome in their analyses. Perhaps they will adjust their unit weights by some diagonal matrix, a diagonal matrix \\(\\mathbf{V}\\) in most applications. The point of this matrix is basically to assist the main optimization in choosing the unit weights. However, even this is limited by the number of pretreatment periods you have- if you have more covariates than you have pretreatment periods, you cannot estimate the regression. Recent papers by econometricians have tried to get around this, though. This blog post covers a few recent recent papers which have advocated for this. I explain the econometric method and apply it in a simulated setting."
  },
  {
    "objectID": "scmo.html#standard-synthetic-control",
    "href": "scmo.html#standard-synthetic-control",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Standard Synthetic Control",
    "text": "Standard Synthetic Control\nBefore introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator minimizes\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThis is a constrained least squares program in which we regress the treated unit‚Äôs pre-treatment outcomes onto the control matrix under the constraint that \\(\\mathbf{w}\\) lies in the simplex \\(\\Delta^{N_0}\\). For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights \\(\\mathbf{w}^\\ast\\) are estimated, the out-of-sample estimates are obtained by applying the same weights to the control matrix\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0^{\\text{post}} \\mathbf{w}^\\ast,\n\\]\nwith the concatenation between the in and out of sample vectors corresponding to the full predictions of the model. The estimated treatment effect at each post-treatment time point is then given by the difference between observed and out-of-sample outcomes: \\(\\hat{\\tau}_{1t} = y_{1t} - \\hat{y}_{1t}\\) for \\(t \\in \\mathcal{T}_2\\)."
  },
  {
    "objectID": "scmo.html#model-averaging",
    "href": "scmo.html#model-averaging",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Model Averaging",
    "text": "Model Averaging\nWe may also model average these models together, which sometimes results in better fit than using either model alone. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have \\(\\mathbf{y}_1^{\\text{CAT}} \\in \\mathbb{R}^{T_0}\\), which denotes the pre-treatment fit from the concatenated model by Tian, Lee, and Panchenko, and on the other hand, we have \\(\\mathbf{y}_1^{\\text{AVG}} \\in \\mathbb{R}^{T_0}\\), the corresponding fit from the demeaned model by Sun, Ben-Michael, and Feller. As before, we observe the treated unit‚Äôs pre-treatment trajectory, \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\).\nTo begin, we stack the two counterfactuals into a single matrix:\n\\[\n\\mathbf{Y}^{\\text{MA}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT}} & \\mathbf{y}_1^{\\text{AVG}}\n\\end{bmatrix} \\in \\mathbb{R}^{T_0 \\times 2}.\n\\]\nWe define the model-averaged pre-treatment fit as a convex combination of the two predictions\n\\[\n\\mathbf{y}_1^{\\text{MA}}(\\boldsymbol{\\lambda}) = \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda},\n\\]\nwhere \\(\\boldsymbol{\\lambda} \\in \\Delta^2\\) is a 2-dimensional simplex weight vector\n\\[\n\\Delta^2 = \\left\\{ \\boldsymbol{\\lambda} \\in \\mathbb{R}_{\\geq 0}^2 : \\| \\boldsymbol{\\lambda} \\|_1 = 1 \\right\\}.\n\\]\nThe model averaged objective function minimizes\n\\[\n\\boldsymbol{\\lambda}^\\ast = \\underset{\\boldsymbol{\\lambda} \\in \\Delta^2}{\\operatorname{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda} \\right\\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThe interpretation of the convex hull remains the same as in the traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the global minimum and maximum of the two individual estimators\n\\[\n\\mathbf{y}_1^{\\text{MA}} \\in \\left[\n\\min\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right),\n\\max\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right)\n\\right]\n\\quad \\forall t \\in \\mathcal{T}.\n\\]\nOnce \\(\\boldsymbol{\\lambda}^\\ast\\) is found, the model-averaged out-of-sample predictions are estimated like\n\\[\n\\mathbf{Y}^{\\text{MA, post}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT, post}} & \\mathbf{y}_1^{\\text{AVG, post}}\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_1^{\\text{MA, post}} = \\mathbf{Y}^{\\text{MA, post}} \\boldsymbol{\\lambda}^\\ast.\n\\]\nEssentially, this is a mixture of both models."
  },
  {
    "objectID": "scmo.html#conformal-prediction-via-agnostic-means",
    "href": "scmo.html#conformal-prediction-via-agnostic-means",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Conformal Prediction via Agnostic Means",
    "text": "Conformal Prediction via Agnostic Means\nNow a final word on infernece. I use conformal prediction intervals to conduct inference here, developed in this paper. Precisely, I use the agnostic approach (yes, I know other approaches exist; users of mlsynth will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as \\(\\mathbf{u}_{\\text{pre}} = \\mathbf{y}_{1,\\text{pre}} - \\mathbf{y}^{\\text{SC}}_{1,\\text{pre}}\\), or just the pretreatment difference betwixt the observed values and its counterfactual. Furthermore, let \\(\\hat{\\sigma}^2 = \\frac{1}{T_0 - 1} \\left\\| \\mathbf{u}_{\\text{pre}} - \\bar{u} \\mathbf{1} \\right\\|^2\\) be the unbiased estimator of the residual variance, where \\(\\bar{u} = \\frac{1}{T_0} \\sum_{t=1}^{T_0} u_t\\) is the mean residual.\nWe aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period \\(\\mathbf{y}^{\\text{SC}}_{1,\\text{post}} \\in \\mathbb{R}^{T_1}\\) be the post-treatment SC predictions for some generic estimator. Assuming that the out-of-sample error is sub-Gaussian given the history \\(\\mathscr{H}\\) (in plain English, this just means that large errors are unlikely, which makes sense given that SC is less biased in a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via concentration inequalities. Specifically, we have \\(\\delta_\\alpha = \\sqrt{2 \\hat{\\sigma}^2 \\log(2 / \\alpha)}\\). The conformal prediction intervals are then defined as \\(\\mathbf{p}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} - \\delta_\\alpha \\mathbf{1}\\), \\(\\mathbf{u}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} + \\delta_\\alpha \\mathbf{1}\\). These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will likely be incorporated in the future."
  },
  {
    "objectID": "scmo.html#simulation",
    "href": "scmo.html#simulation",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Simulation",
    "text": "Simulation\nSuppose we are working at Airbnb, and we wish to see the causal effect of the introduction of Airbnb Experiences on Gross Booking Value (GBV), a metric which is defined as ‚Äò‚Äôthe total revenue generated by room or property rentals before any costs or expenses are subtracted‚Äô‚Äô. Airbnb Experiences connects users of the platform to local tour guides or other local attractions. It serves as a kind of competition to Travelocity, Viator and other booking/ travel services. In other words, this program may make makes this city an attraction, and we may see an increase in GBV as a result.\nWell, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy. The point of this simulation is to use the SCMO (synthetic control multiple outcomes) estimator to measure the causal impact.\nFor each unit, the observed outcome \\(\\mathbf{Y}_{jtk}\\) evolves according to an autoregressive process with latent structure for time, place, and seasonality\n\\[\n\\mathbf{Y}_{jtk} =\n\\rho_k \\mathbf{Y}_{jt-1k} +\n(1 - \\rho_k) \\left(\n\\alpha_{jk} + \\beta_{tk} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{tk} + \\mathbf{S}_{jt} + \\delta_k\n\\right) + \\varepsilon_{jtk}, \\quad \\text{for } t &gt; 1,\n\\]\nwith initial condition\n\\[\n\\mathbf{Y}_{j1k} =\n\\alpha_{jk} + \\beta_{1k} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{1k} + \\mathbf{S}_{j1} + \\delta_k + \\varepsilon_{j1k}.\n\\]\nHere, \\(\\alpha_{jk} \\sim \\mathcal{N}(0, 1)\\) and \\(\\beta_{tk} \\sim \\mathcal{N}(0, 1)\\) represent unit-outcome and time-outcome fixed effects, respectively. Each unit \\(j\\) possesses latent attributes \\(\\boldsymbol{\\phi}_j \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\), while each time-outcome pair \\((tk)\\) has associated latent loadings \\(\\boldsymbol{\\mu}_{tk} \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\). The seasonal component \\(\\mathbf{S}_{jt}\\) captures unit-specific periodicity and is defined as \\(\\gamma_j \\cos\\left( \\frac{4\\pi(t - \\tau_j)}{T_{\\text{season}}} \\right)\\), with \\(\\gamma_j \\sim \\text{Unif}(0, \\bar{\\gamma})\\) representing the amplitude and \\(\\tau_j \\sim \\text{Unif}\\{0, \\dots, T_{\\text{season}} - 1\\}\\) the phase shift. Each outcome \\(k\\) has a baseline shift \\(\\delta_k \\sim \\text{Unif}(200, 500)\\), an autocorrelation parameter \\(\\rho_k \\in (0, 1)\\), and an idiosyncratic noise component \\(\\varepsilon_{jtk} \\sim \\mathcal{N}(0, \\sigma^2)\\). One unit (Iquique, Chile in this draw) is designated as treated. To introduce selection bias, the unit with the second-largest realization on the first latent factor dimension is treated, meaning methods like difference-in-differences or interrupted time series methods will not perform well. The target unit‚Äôs GBV receives an additive treatment effect of \\(+5\\) during all post-treatment periods."
  },
  {
    "objectID": "scmo.html#results",
    "href": "scmo.html#results",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Results",
    "text": "Results\nWhen we run this estimator, we need to specify one of three estimators: TLP, SBMF, or both, where the abbreviations are obviouslyfor the surnames of the authors. We also need to supply a dictionary entry to mlsynth called addout. This is either a string or a list which lists the additional outcomes we care about in the dataframe. When we run the estimator, we get:\n\n# Run simulation\n\ndf = simulate(seed=10000, r=3)\n\nconfig = {\n    \"df\": df,\n    \"outcome\": 'Gross Booking Value',\n    \"treat\": 'Experiences',\n    \"unitid\": 'Market',\n    \"time\": 'Week',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\"], \"addout\": list(df.columns[4:]),\n    \"method\": \"BOTH\"\n}\n\narco = SCMO(config).fit()\n\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/mlsynth/estimators/scmo.py:644: UserWarning: An unexpected error occurred during SCMO plotting: plot_estimates() got an unexpected keyword argument 'df'\n  warnings.warn(f\"An unexpected error occurred during SCMO plotting: {e_plot_general}\", UserWarning)\n\n\nUsing the model averaging estimator, our pre-treatment Root Mean Squared Error is 0.276. The ATT is 5.046. The weights are also a sparse vector. The model averaged estimator returns Arequipa (0.237), Bogot√° (0.232), San Salvador (0.218), Santiago de Chile (0.174), San Luis Potos√≠ (0.081), Montevidio (0.032), and Manzanillo (0.026), as the contributing units or only 7 of the 98 donor units. The optimal mixing between the models is 0.538 for the intercept-shifted estimator and 0.461 for the concatenated method. For DID, the RMSE is 0.878 and the ATT is 5.2, meaning that the intercept adjusted average of all donor units is clearly a biased estimator.\nCompare to Forward DID, this is NOT true: we have an ATT of 5.063 and a pre-intervention RMSE of 0.289, selecting Antofagasta, Bocas del Toro, Punta del Este, San Pedro Sula, and Santiago as the optimal donor pool (this method uses no additional outcomes, only the GBV metric). When I compare to the clustered PCR method, the positively weighted donors are San Pedro Sula (0.296), Punta del Este (0.255), Santiago (0.199), Antofagasta (0.190), and La Plata (0.060)."
  },
  {
    "objectID": "spillsynth.html",
    "href": "spillsynth.html",
    "title": "The Iterative Synthetic Control Method",
    "section": "",
    "text": "This will be a short blog post. I‚Äôve spent the last two months doing a little industry work in the marketing realm, and in the meantime I made some substantial changes to mlsynth. This blog post simply shows one of the new key features I have implemented.\nChances are if you‚Äôre reading this, you know what I mean by the notion of SUTVA, or the stable unit treatment value assumption. It is the idea that if we care about the causal impact of a treatment on one unit, but other units are affected by the treatment or otherwise experience a similar treatment, that this exposure confounds our treatment effect with respect to the original unit we do care about. Say we wish to study the impact of German Reunification on West Germany‚Äôs GDP. We know West Germany was exposed, but what about neighboring nations like Austria or France? What if the reunification had regional effects? Analysts therefore have a problem: Austria and France may be very similar to West Germany, and therefore informative of West Germany‚Äôs counterfactual, but we are concerned they are exposed or affected by the main treatment of interest. What do we do? Before, researchers would need to drop these units or argue for their inclusion/exclusion, despite them being treated. Now, we do not need to do that, as SCM has a few approaches that deal with spillovers (this post covers just one). The approach, called iterative synthetic controls, is deceptively simple.\nSuppose Austria and France are partly treated. Step one of iSCM is to estimate a synthetic control for Austria, including France as a donor but excluding West Germany. Which SCM flavor you ask? Any one you like! For the purposes of this post, we will be using the Robsut SCM and the Robust PCA SCM methods from mlsynth. The precise details are not really important, but you may read the docs should you like. We then take the model predictions for Austria across the full pre and post period and replace the original Austria with the synthetic control values that the model predicts for Austria.\nNext, we do France: using the cleaned up Austria as a donor, we estimate the synthetic control for France, using the now-cleaned up Austria and the remaining donor pool units. As before, we replace the values for the original France (in the original dataset) with the new synthetic France. We now have cleaned up our two donors that may be exposed to the treatment.\nNow, with these two cleaned donors, we estimate the counterfactual for West Germany, with our 14 totally unexposed donors and the two now cleaned up donors that were once partially exposed.\n\nEstimation in Python\n‚ÄúBut Jared!‚Äù, you will say, this seems like a lot of looping and lots of donor tracking. Well fear not, that is what mlsynth is for. In order to get these results, you need Python (3.9 or greater) and mlsynth, which you may install from the Github repo. You‚Äôll need the most recent version.\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nFirst we estimate the orignal model. You will find our handy-dandy util function iterative_scm is now imported.\n\nimport pandas as pd\nfrom mlsynth import CLUSTERSC\nfrom mlsynth.utils.spillover import iterative_scm\n\n# Load the reunification dataset\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/main/basedata/german_reunification.csv\"\ndf = pd.read_csv(url)\n\n# Define configuration for CLUSTERSC\nconfig = {\n    \"df\": df,\n    \"outcome\": \"gdp\",          # per capita GDP\n    \"treat\": \"Reunification\",     # binary treatment indicator\n    \"unitid\": \"country\",          # country name\n    \"time\": \"year\",               # time variable\n    \"display_graphs\": True,       # display counterfactual plots\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"Frequentist\": True, \"method\": \"BOTH\"\n}\n\noriginalresult = CLUSTERSC(config).fit()\n\n\n\n\n\n\n\n\nThese are the original results., You may check them against my coworker‚Äôs dissertation if you wish. Now we see how sensitive the results are to adjusting for spillover effects.\n\nfrom IPython.display import display, Markdown\n\niSCM_result = iterative_scm(CLUSTERSC(config), spillover_unit_identifiers=[\"Austria\", \"France\"], method=\"BOTH\")\n\ndef summarize_att_rmse_markdown(iSCM_result):\n    rows = []\n    for method in ['PCR', 'RPCA']:\n        sub = iSCM_result[method].sub_method_results[method]\n        att = sub.effects.additional_effects['ATT']\n        percent_att = sub.effects.additional_effects.get('Percent ATT', None)\n        t0_rmse = sub.fit_diagnostics.additional_metrics['T0 RMSE']\n\n        rows.append({\n            \"Method\": method,\n            \"ATT\": f\"{att:,.0f}\",\n            \"Percent ATT\": f\"{percent_att:.2f}%\" if percent_att is not None else \"‚Äî\",\n            \"T0 RMSE\": f\"{t0_rmse:,.1f}\",\n        })\n\n    df = pd.DataFrame(rows)\n    md_table = df.to_markdown(index=False)\n    display(Markdown(f\"### Iterative SCM \\n\\n{md_table}\"))\n\nsummarize_att_rmse_markdown(iSCM_result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIterative SCM\n\n\n\nMethod\nATT\nPercent ATT\nT0 RMSE\n\n\n\n\nPCR\n-2,111\n-7.96%\n108.2\n\n\nRPCA\n-1,477\n-5.71%\n88.1\n\n\n\n\n\nYou pass the original config to the util function, and you pass a list of strings that denote which units we believe are partly treated. Form there, the algorithm under the hood handles the donor cleaning, and returns back the results of the final SCM run with both donors cleaned. Note that if you want both PCR and RPCA cleaning, we pass it as method=\"BOTH\" to iterative_scm. We can see that the results are very similar to the original SCM. The pre-treatment fits degrade only very slightly (even more so with the Robust PCA method, highlighting its robustness to tiny tweaks in the model). Speaking of RPCA, the new weights are ‚ÄòBelgium‚Äô: 0.271, ‚ÄòNorway‚Äô: 0.552, ‚ÄòNew Zealand‚Äô: 0.34, whereas before they were ‚ÄòAustria‚Äô: 0.023, ‚ÄòFrance‚Äô: 0.354, ‚ÄòNorway‚Äô: 0.485, ‚ÄòNew Zealand‚Äô: 0.296. Austria goes away as a weighed donor, but there‚Äôs not very much change in the pre-treatment fit or the practical conclusions we draw from the analysis. When we use only Austria as the cleaned unit, RPCA‚Äôs weights are ‚ÄòUK‚Äô: 0.237, ‚ÄòFrance‚Äô: 0.607, ‚ÄòNorway‚Äô: 0.191, ‚ÄòNew Zealand‚Äô: 0.12 with an ATT of -1536.355. When we use only France, the ATT is -1490.636 and the weights are ‚ÄòAustria‚Äô: 0.262, ‚ÄòDenmark‚Äô: 0.004, ‚ÄòNorway‚Äô: 0.517, ‚ÄòNew Zealand‚Äô: 0.378. Of course, the key aspect of this procedure is knowing which units are likely to have spillover effects\n\n\nComments\nSo, this is not the only way to do this. There are plenty of other methods that people have developed for this purpose too. I likely will not program all of thse myself into mlsynth, but others who are so inclined are welcome to assist in the effort! in the future, I‚Äôll also allow you to switch between the options for each kind of spillover management (the inclusive method versus the iterative method, for example). But, now you know how to use this for your own work. As usual, comments or suggestions are always appreciated.\n\n\n\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nPolicy Analysis, Data Science, and Causal Inference\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "synthinter.html",
    "href": "synthinter.html",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "",
    "text": "Most data scientists who use synthetic control methods likely aim to estimate the counterfactual outcome for a treated unit if it had not been treated at all. This framework works quite neatly in the setting with a dummy treatment status (exposed or not exposed). But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, we are typically concered with the outcome under the scenario of no treatment. A research question could be how the store with the cash backprogram would have fared had it done nothing at all. In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus.\nBut what if we wish to estimate the counterfactual for a given unit as if it did another policy from what it actually did. In other words, How would Store A have performed if it had adopted Program B instead of Program A? What if City 1 had imposed a soda tax instead of banning large sodas? How would health metrics evolved if the other policy was done instead? Plenty of academic papers have addressed this idea before, but only to assess the counterfactual scenario of no tax at all.\nThis is where the Synthetic Interventions estimator is useful. SI estimates how a treated unit (or even a never treated unit) would have performed under an intervention it did not actually receive. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI substantially expands the range of counterfactual questions that can be credibly answered.\nBefore we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like this circulate that claimed to be able to estimate things like ‚Äúwhat would NYC‚Äôs COVID rate look like had it locked down earlier than it actually did‚Äù. I had never heard of tensors, or really even matrices at the time, so I would always ask ‚Äúwow, how can we even do this?‚Äù So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others."
  },
  {
    "objectID": "synthinter.html#si-in-mlsynth",
    "href": "synthinter.html#si-in-mlsynth",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "SI in mlsynth",
    "text": "SI in mlsynth\nNow I will give an example of how to use SI for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nNext we load the Proposition 99 data, using the dataset that has all of the states in the United States.\n\nimport pandas as pd\nfrom mlsynth import SI\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/jehangiramjad/tslib/refs/heads/master/tests/testdata/prop99.csv\"\ndf = pd.read_csv(url)\nlast_column = df.columns[-1]\ndf_filtered = df[df[last_column] == 2]\n\ncolumns_to_keep = list(df.columns[0:4]) + [df.columns[7]]\ndf_filtered = df_filtered[columns_to_keep]\n\nsort_columns = [df_filtered.columns[1], df_filtered.columns[2]]\ndf_filtered = df_filtered.sort_values(by=sort_columns)\n\ndf_filtered = df_filtered[df_filtered[df_filtered.columns[2]] &lt; 2000]\n\ndf_filtered['SynthInter'] = ((df_filtered[df_filtered.columns[0]] == 'LA') &\n                         (df_filtered[df_filtered.columns[2]] &gt;= 1992)).astype(int)\n\n\ntax_states = ['MA', 'AZ', 'OR', 'FL']  # Massachusetts, Arizona, Oregon, Florida abbreviations\ndf_filtered['Taxes'] = (df_filtered[df_filtered.columns[0]].isin(tax_states)).astype(int)\n\nprogram_states = ['AK', 'HI', 'MD', 'MI', 'NJ', 'NY', 'WA', 'CA']\ndf_filtered['Program'] = (df_filtered[df_filtered.columns[0]].isin(program_states)).astype(int)\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nWe begin with simple data cleaning. All we do is keep the relevant metrics, setting a treatment variable ‚ÄúSynthInter‚Äù to be 1 if the year is greater than or equal to 1992 and the unit of interest is Louisiana. I choose 1992 because this is the year Massachusetts passed its anti tobacco policy. Note that no policy in fact happened in Louisiana, so we will see the effect of keeping the status quo of no tobacco control policy at all. We then define as an indicator which states did the taxes. According to Abadie‚Äôs 2010 paper those states are Massachusetts, Arizona, Oregon, and Florida. We also define an indicator for states that did an anti-tobacco statewide program. This way, we can see how per capita smoking would have evolved under either policy. Under the hood, we just loop through each of the different policies the user specifies.\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nconfig = {\n    \"df\": df_filtered,\n    \"outcome\": 'cigsale',\n    \"treat\": 'SynthInter',\n    \"unitid\": 'State',\n    \"time\": 'Year',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"inters\": [\"Taxes\", \"Program\"]\n}\n\narco = SI(config).fit()\n\n\n\n\n\n\n\n\nFor Louisiana, the ATT of keeping the status quo relative to the state-wide tobacco program maintained per capita consumption 19.134 higher than what it would have been otherwise. The ATT of not passing taxes on tobacco, compared to taxes, mainatined the same at roughly 19.77 higher than what would have been otherwise. To me, these findings are not very surprising: ‚ÄúAnti tobacco programs/taxes reduce slightly tobacco consumption, wow, how shocking!‚Äù But now, we can actually answer these kinds of questions using econometric methods, instead of just speculating.\nWhen we look at the dictionary that the SI class returns, we see that the policies would have been pretty effective at reducing tobacco consumption. The dictionary is policy specific, one dictionary per policy the user chooses to consider. The SI estimator also only works for 1 treated unit, however analysts can easily make (say) a list of dataframes with an indicator for each synthetic treatment and loop over them. Once they have done this, we may extract the ATT per policy, per treated unit, and compute the sample average for an event-time ATT version for a given policy across units. I do not optimize the code to do this for a few reasons: one, the more policies and treated units we have, the less efficient it becomes to estimate tractably. Furthermore, the SI estimator is about personalized causal inference. So, I will likely leave this class for users to adapt to their own purposes (with some adjustments, if demand exists for something else)."
  }
]