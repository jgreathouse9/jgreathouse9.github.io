---
title: 'What is An Average?'
date: 2026-01-04
categories: [Econometric Theory]
---


## Averaging in Statistics

Averages arise naturally when we try to find a single value that best represents a collection of numbers. In the old days, Egyptians and Babylonians used averages to summarize construction and financial matters. In modern times, we use averages to summarize things like test grades, incomes, and other variables we care about.  To formalize this, suppose we have a sequence of data points $(x_1, x_2, \ldots, x_n)$ of length $n$. We want to choose a single number $\mu$ that is as close as possible to all of them. Here, $\mu$ is a placeholder variable representing a potential candidate for the ‘best’ summary value; we will later find that the minimizing value, denoted $\bar{x}$, is the arithmetic mean. Because we seek a single number as close as possible to all values, this becomes our first example of an optimization problem—minimizing a function with respect to a variable. Recalling the definition of an objective function, let our observed values be $x_i$ and the number that "is as close as possible to all the values" be the Greek letter $\mu$ (m-yoo). Because we are looking for the value that minimizes the distance, we can write $x_i - \mu$ as a shorthand for this deviation. To minimize the total deviation across all points, we sum over each element of the sequence $i$: $\sum_{i=1}^{n} (x_i - \mu)$. Finally, to penalize values of $\mu$ that are further away from the observed data, we square each term. This gives us the optimization problem:

$$
\min_{\mu} \sum_{i=1}^{n} (x_i - \mu)^2.
$$


This function is convex and differentiable. We can optimize this function by taking the derivative with respect to $\mu$. We begin by applying the chain rule term by term. For each term $(x_i - \mu)^2$, there are two parts to consider: the outer function and the inner function. The outer function is $f(u) = u^2$, where $u = x_i - \mu$. Its derivative (by the power rule) is

$$
\frac{\mathrm{d}}{\mathrm{d}u} u^2 = 2u = 2(x_i - \mu)
$$

The inner function is $u(\mu) = x_i - \mu$. Its derivative with respect to $\mu$ is

$$
\frac{\mathrm{d}}{\mathrm{d}\mu} (x_i - \mu) = -1
$$

Multiplying the outer and inner derivatives gives the derivative of a single term:
$$
\frac{\mathrm{d}}{\mathrm{d}\mu} (x_i - \mu)^2 = 2(x_i - \mu) \cdot (-1) = -2(x_i - \mu)
$$

Summing over all $i$ gives the derivative of the entire objective function:

$$
\frac{\mathrm{d}}{\mathrm{d}\mu} \sum_{i=1}^{n} (x_i - \mu)^2 = \sum_{i=1}^{n} [-2(x_i - \mu)] = -2 \sum_{i=1}^{n} (x_i - \mu)
$$

Setting this derivative equal to zero gives the first-order condition for the minimum

$$
-2 \sum_{i=1}^{n} (x_i - \mu)=0.
$$


We now solve for $\mu$. First, just divide both sides by $-2$:

$$
\sum_{i=1}^{n} (x_i - \mu) = 0.
$$

Next, split the sum into two parts:

$$
\sum_{i=1}^{n} x_i - \sum_{i=1}^{n} \mu = 0.
$$

Since $\mu$ is constant with respect to the index $i$, the second sum simplifies to multiplication:

$$
\sum_{i=1}^{n} x_i - n\mu = 0.
$$

Now we add $n\mu$ to both sides:

$$
\sum_{i=1}^{n} x_i = n \mu.
$$

Finally, divide both sides by $n$ to isolate $\mu$:

$$
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i.
$$

Thus, the value of $\mu$ that minimizes the total squared deviation is the arithmetic mean of the data. The arithmetic mean is thus not an arbitrary formula: it is the unique value that minimizes the total squared distance between itself and the data. 

But, recall the definition of an objective function: I mentioned how it is a **scalar valued** function, because it returns a scalar. That scalar is the value of the function where we set $\mu$ equal to the optimal value. Substituting this minimizing value back into the objective function, we get the total deviation around the mean:

$$
f(\bar{x}) = \sum_{i=1}^{n} (x_i - \bar{x})^2.
$$

Because this quantity grows with $n$, it is often more useful to consider the average squared deviation (also called the residuals):

$$
\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2,
$$

which we call the population variance. The variance measures how far the data tend to fall from their best-fitting constant value. We use squared deviations because they produce a smooth, differentiable, and convex objective that penalizes larger errors more heavily. Note that the absolute deviations, instead of the squared deviations, would return the median, not the mean.  The cost of squaring the residuals is that variance is expressed in squared units; taking the square root restores the original units and gives the standard deviation. When variance is estimated from a sample rather than the entire population, the denominator is typically replaced by $n - 1$ to obtain an unbiased estimator.

### So What?

Now, you may be saying "Hey Jared, this is overkill. Why bother doing all of this for something as simple as an average?" The derivation shows us that the average is the solution to an optimization problem. That single insight connects simple statistics to the machinery of modern econometrics. Variance, standard deviation, mean squared error, regression coefficients, and the synthetic control weights are all built on the same principle: finding a number/set of numbers that minimizes some measure of error. The arithmetic mean is the simplest, most familiar instance of this principle. Every estimator in this book is an application of averaging, and this fact will become crucial in later chapters when we take more sophisticated averages.
