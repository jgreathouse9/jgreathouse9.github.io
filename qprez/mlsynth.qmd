---
title: "Counterfactual Estimation via ```mlsynth```"
author: "Jared Greathouse"
date: today
format: 
  revealjs:
    scrollable: true
theme: simple
highlight-style: github
---


## Counterfactual Estimation

- Maximizing value for business services and crafting effective public policy depends on knowing whether policies, promotions, new web features, taxes, or other interventions _meaningfully_ affect metrics that we care about.

- Researchers commonly use Difference-in-Differences designs and Synthetic Control methods (DID and SCM) to measure causal impacts.

- Both approaches use weighted averages of control groups from the pre-treatment period to estimate counterfactuals, or the post-treatment values of the outcome we'd see absent treatment.

---

## Counterfactual Estimation cont.

Both methods conduct their optimizations over the course of the pre-treatment period.

- SCM constrains our weights to be on the simplex, $\Delta &\operatorname{:=} \{\mathbf{w} \in \mathbb{R}_{\geq 0}^{N_0} \mid \lVert \mathbf{w} \rVert_1 = 1\}$. The optimization to learn the weights is

$$
\underset{\mathbf{w} \in \Delta}{\operatorname*{argmin}} &\quad \lVert \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \rVert_2^2.
$$

- For DID, we have

$$
\begin{aligned}
    \underset{\mathbf{w} \in \mathbb{R}^{N_0}}{\operatorname*{argmin}} &\quad \lVert \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} - \boldsymbol{\beta}_0 \rVert_2^2, \\
    \text{s.t.} &\quad \mathbf{w} = \frac{1}{N_0} \mathbf{1}_{N_0}.
\end{aligned}
$$

---

## Counterfactual Estimation cont.

However, they may struggle sometimes.

- SCM struggles with high-dimensional datasets. Often, smaller donor pools are preferable to very large ones to mitigate overfitting biases. But picking a good donor pool can be challenging when there are very many controls.

- SCM solutions are often sparse, given the classical formulation of the problem's weights are nonnegative and add to 1.

- May be implausbile when the true vector coefficient is not 0.

- DID requires $\mathbb{E}[y_{1t}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t}(0)] = \mathbb{E}[y_{1t-1}(0)] - \mathbb{E}[y_{\mathcal{N}_0 t-1}(0)]$, or parallel trends. May not always hold, particularly in small $N$, large $T$ setings).

---

## Potential Solutions

- Econometricians have crafted methods which attempt to address all of these in some way.

---

## Benefits of mlsynth

- **Forward Difference-in-Differences (FDID)**
  - Better parallel trends
- **$\ell_2$ Relaxer**
  - Handles multicollinearity better than LASSO.
- **Robust PCA SCM**
  - Uses better PCA

---

## Example: FDID in Action

```{python results="markup"}
print("I am Jared")
