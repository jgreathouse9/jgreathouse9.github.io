[
  {
    "objectID": "fscm.html",
    "href": "fscm.html",
    "title": "Forward Selected Synthetic Control",
    "section": "",
    "text": "Interpolation bias is a known issue with synthetic control models (SCMs) For valid counterfactual prediction, the donor units, or the set of units that were never exposed to an intervention, should be as similar as possible to the treated unit in the pre-treatment periods. Selecting an appropriate donor pool is therefore critical, for practitioners. However, this can be challenging in settings with many potential controls, potentially many more control units than pre-treatment periods. Practically, researchers may wish to use this method when they have a high-dimensional donor pool and may be unsure as to which donors to include to reduce the impact of interpolation biases. To this end, this blog post introduces users the Forward Selected SCM. This applies Forward Selection (FS) to choose the donor pool for a SCM before estimating out-of-sample predictions."
  },
  {
    "objectID": "fscm.html#notation",
    "href": "fscm.html#notation",
    "title": "Forward Selected Synthetic Control",
    "section": "Notation",
    "text": "Notation\nLet \\(\\mathbb{R}\\) denote the set of real numbers. A calligraphic letter, such as \\(\\mathcal{S}\\), represents a discrete set with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) represent indices for a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Let \\(j = 1\\) be the treated unit, with the set of controls being \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\), with cardinality \\(N_0\\). The pre-treatment period consists of the set \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\leq T_0 \\},\\) where \\(T_0\\) is the final period before treatment. Similarly, the post-treatment period is given by \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}.\\) The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), where a generic outcome vector for a given unit in the dataset is \\(\\mathbf{y}_j \\in \\mathbb{R}^T\\), where \\(\\mathbf{y}_j = (y_{j1}, y_{j2}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^{T}\\). The outcome vector for the treated unit specifically is \\(\\mathbf{y}_1\\). The donor matrix, similarly, is defined as \\(\\mathbf{Y}_0 \\coloneqq \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\), where each column indexes a donor unit and each row is indexed to a time period.\nSCM estimates the counterfactual outcome for the treated unit by solving the program\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} \\|_2^2 \\: \\forall t \\in \\mathcal{T}_1.\n\\]\nWe seek the weight vector, \\(\\mathbf{w}\\), that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. For our purposes, the space of SC weights is the \\(N_0\\)-dimensional probability simplex \\(\\Delta^{N_0} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{N_0} : \\|\\mathbf{w}\\|_1 = 1 \\right\\}.\\) Practically this means that the post-intervention predictions will never be greater than the maximum outcome of the donor pool or lower than the minimum outcome of the donor pool."
  },
  {
    "objectID": "fscm.html#step-1",
    "href": "fscm.html#step-1",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 1",
    "text": "Step 1\nFS proceeds over \\(K \\in \\mathbb{N}\\) iterations, builds a sequence of tuples, \\(\\mathbb{T} = \\{(\\mathcal{S}_K, \\text{MSE}_K) \\}_{K=1}^{N_0}\\). The tuple contains two elements: the selected donor set for the \\(K\\)-th iteration and its corresponding \\(\\text{MSE}_K\\) (or the pre-treatment mean squared error). We begin by minimizing the SCM objective function as above, cycling through each donor unit vector one at a time instead of using the full control group. We denote these as submodels, which returns \\(N_0\\) one unit SCM models. We choose the single donor unit (the nearest neighbor in this specific case) that minimizes the MSE among all the \\(N_0\\) submodels. Our first tuple, then, is built with this single donor unit and the model‚Äôs corresponding MSE\n\\[\n\\mathcal{S}_1 = \\{j^\\ast\\}, \\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0}{\\operatorname*{argmin}} \\ \\text{MSE}(\\{j\\}).\n\\]"
  },
  {
    "objectID": "fscm.html#step-2",
    "href": "fscm.html#step-2",
    "title": "Forward Selected Synthetic Control",
    "section": "Step 2",
    "text": "Step 2\nFor \\(K=2\\), we now estimate \\(N_0-1\\) two-unit SCMs. We include the originally selected donor along with the remaining controls, one remaining donor at a time. As above, the first and second elements of the second tuple, respectively, are\n\\[\n\\mathcal{S}_2 = \\mathcal{S}_1 \\cup \\{j^\\ast\\},\\quad \\text{where} \\quad j^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}_{K-1}}{\\operatorname*{argmin}} \\ \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j\\}).\n\\]\nNow, we have two tuples and two MSEs to choose from."
  },
  {
    "objectID": "fscm.html#generalizing",
    "href": "fscm.html#generalizing",
    "title": "Forward Selected Synthetic Control",
    "section": "Generalizing",
    "text": "Generalizing\nThis process generalizes, continuing for the rest of the donor pool. The general form for this algorithm, then, is\n\\[\n(\\mathcal{S}_K, \\text{MSE}_K) = (\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\}, \\text{MSE}(\\mathcal{S}_{K-1} \\cup \\{j^\\ast\\})).\n\\]\nThe algorithm continues until \\(S_K=N_0\\), when there are no more donors to add. FSCM chooses the tuple with the lowest \\(\\text{MSE}\\):\n\\[\n\\mathcal{S}^{\\ast} = \\underset{(\\mathcal{S}_K, \\text{MSE}_K) \\in \\mathbb{T}}{\\operatorname*{argmin}} \\ \\text{MSE}_K.\n\\]\nas the optimal donor set, \\(\\mathcal{S}^{\\ast}\\). Note that even within \\(\\mathcal{S}^{\\ast}\\) (as we will see below), some donors may receive zero weight in the final solution. The selected donors are just the units selected for inclusion in the donor pool in the first place, they are no guarantee of the unit having positive weight. This is in contrast to methods such as Forward Difference-in-Differences or the FS panel data method. Both of these designs are available in mlsynth too, in the FDID class and PDA class with the method of fs (the default). The main difference here is that FDID can never overfit because it estimates only one parameter, whereas (in theory) FSCM and fsPDA can overfit if they end up including too many parameters in the regression model. Unclear how likely this is, since as we see below, teh FS method reduces the full donor pool to just under half of the originally selected donor units."
  },
  {
    "objectID": "fscm.html#fscm-in-mlsynth",
    "href": "fscm.html#fscm-in-mlsynth",
    "title": "Forward Selected Synthetic Control",
    "section": "FSCM in mlsynth",
    "text": "FSCM in mlsynth\nNow I will give an example of how to use FSCM for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nAnd then we load the Proposition 99 dataset and fit the model in the ususal mlsynth fashion. As per the ususal, we have a single unit name column which has the names of the donors, a time column (the year in this case), a numeric outcome column, and an indicator column which denotes the treatment when it is active and the unit is the name of the treated unit, else 0.\n\nimport pandas as pd # To work with panel data\n\nfrom IPython.display import display, Markdown # To create the table\n\nfrom mlsynth import FSCM # The method of interest\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv\"\n\n# Feel free to change \"smoking\" with \"basque\" above in the URL\n\ndata = pd.read_csv(url)\n\n# Our method inputs\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = FSCM(config).fit()\n\n\n\n\n\n\n\n\nAfter estimation, we can get the weights. These are the weights for all 17 units that were selected by the algorithm. As we can see, all of these even did not ultimately contribute to the SC, with only 6 being assigned positive weight. The ATT of Prop 99 as estimated by FSCM is -19.51 and the pre-treatment Root Mean Squared Error for FSCM is 1.66. I compared these results to the same results we get in Stata, which includes the covariates that Abadie, Diamond, and Hainmuller originally adjusted for as well as customizes the period over which to minimize the MSE. The ATT using the original method is -19.0018, and the RMSE for the pre-treatment period is 1.76. The corresponding weights using the full donor pool are 0.334 for Utah, 0.235 for Nevada, 0.2020 for Montana, Colorado 0.161, and Connecticut 0.068. So as we can see, the ATTs are very similar, and the pre-treatment prediction errors are pretty much the same. When we estimate this in Stata (omitting the auxilary covariate predictors and estimate synth2 cigsale cigsale(1988) cigsale(1980) cigsale(1975) , trunit(3) trperiod(1989) xperiod(1980(1)1988) nested fig, we get a RMSE of 4.33 and an ATT of -22.88. Furthermore, with this specification, the weights are no longer a sparse vector.\nThe point of this article is very simple. The original SCM works well, however it can be very sensitive to the inclusion of covariates, which covariates are included, what their lags are, and so on and so forth. Furthermore, there is also an issue of covariate selection in settings where we have multiple covariates that can potentially inform our selection of the donor pool. Furthermore, collecting a rich list of covariates may also not be possible in some settings. In such situations, especially without some pre-existing grount truth donor pool, analysts may apply the FSCM algorithm to guard against interpolation biases.\nAt least with the California example (and West Germany and Basque datasets, which I also tested), we can sometimes get comparable results to the baseline estimates which used multiple covariates for acceptable results (in all three of the standard test cases, FSCM actually get lower MSE than the original applications). In the Proposition 99 example, we select some of the same donor units, get a slightly better MSE and a very similar ATT without needing to fit to the covariates originally specified in the JASA paper.\nThe promise of machine-learning methods in this space is to automate away donor/predictor selection to some acceptable degree. The key thing of interest (for me, from an econometric theory perspective anyways) is which methods are best suited for this task, when do they perform well, and why. For example, it might be useful to derive bias bounds for this estimator to quantify how much the MSE should improve by compared to the original SCM and Forward DID, as has been done with clustering based methods, for example.\nA final caveat: in the original paper, Giovanni uses cross-validation to estimate this model, and he also employs the same covariates. I have not done the cross validation yet on my end, but I will very soon. As ususal, email me with questions or comments."
  },
  {
    "objectID": "sccare.html",
    "href": "sccare.html",
    "title": "Synthetic Controls Do Not Care What Your Donors Are. So Why Do You?",
    "section": "",
    "text": "I was talking with someone recently about applying SCM, and I mentioned the idea of using a richer donor pool with different kinds of donors to answer a question. I had to articulate myself kind of carefully, since they were kind of puzzled about what I meant at first. For me, the idea comes very naturally to me, where we literally do things like this all the time. My intuition however comes from thinking about these models a lot (and empirical proof/evidence), and I think it may help others.\nMany people do not understand the math behind the methods they use. If you asked most DSs who work with causal inference to formally prove when parallel trends holds like Pedro Sant‚ÄôAnna does, or derive the bias bound of a new SCM as Abadie might, we likely would not have very many contenders. And that is okay, that is natural. After all, DS is a very applied field; the langauge of proof and formal math are typically far flung concerns of your typical data scientist, and I am certainly not an exception to this rule.\nHowever, be that as it may, it is still important to know a little about why what you are doing is effective to begin with. Why? Well, intuition can only take us so far. Yeah, you may have a library like mlsynth or geolift on you machine (or whatever you run your code in), but how do you know which tool to use, when, and why? More importantly, what happens when something breaks and you do not get the results you expected? What if you get something wrong? Of course, some of this is black-box-y, but in my experience anyways, lots of the issues people run into are not research issues in the sense that their question is poor or the estimator is wrong; instead, they simply misunderstand or do not know econometric theory that leads them to using sub-optimal estimators or not doing basic checks. Again, this is not an issue of proof or technical depth; it is about knowing why your toolkit is effective in the very first place. This is much more crucial because if you do not know when your methods are expected to work well, you will not know if they fail or why beyond ‚Äúthat does not look right‚Äù. In a sense, you are sort of blindly applying calculus and algebra without much thought as it how any of it relates to your specific use case. Causal inference is built on assumptions. If you do not respect the assumptions and theoretical foundation of your estimator, its findings will disrespect your expectations.\nThis is kind of a broader problem in data science in many respects. Ostensibly, there are many myths about SCM, one of which is ‚Äúthe myth that SCM/[Augmented] SCM[s] don‚Äôt have assumptions like parallel trends‚Äù. I do not know who believes this myth, and I am inclined to think that widespread belief of this myth‚Ä¶ is itself a myth. After all, even your most applied economists will tell you ‚Äúyeah, SCM definitely has assumptions‚Äù. They may not be able to articulate what they are as formally as they could OLS, for example, but everybody in my experience knows SCM has assumptions that need to be met. Of course, whether they verify them is another matter entirely (if you know anybody who truly holds these beliefs about SCM not having assumptions, direct them to me so I can chat with them).\nEither way, even there are not many people who confidently say/think ‚ÄúSCM has no assumptions‚Äù, a much better argument I think can be made that even if people do not literally think this‚Ä¶ this is certainly the way many researchers act in practice. And that is the point of this post. Knowledge of econometric theory can serve as an augmentation to the skills of standard DS; even a little bit of it will not be in vain. As with cars, you may not know how to build an engine, but you will be well served if you know how and when to change spark plugs or fill your tire with air.\n\nBasic SCM\nPeople often bring up the condition of perfect pre-intervention fit as an assumption of SCM; this has been revised substantially in recent years, but there is another one that people do not comment on enough: the idea of the linear factor model (or latent variable model) that often serves as a motivating data generating process. The linear factor model takes the form:\n\\[\nY_{it}(0) = \\boldsymbol{\\lambda}_t^\\top \\boldsymbol{\\mu}_i + \\varepsilon_{it}.\n\\]\nWhat does this mean? It simply means that our outcome observations are generated by a set of unit specific factors that are time invariant, and a set of time variant factors that are common across all units, plus some error term. These common factors may indeed affect each unit differently (in DID, the relationship is additive, not multiplicative), but the key idea is that we can use their similarity in terms of how they interact with the factor loadings to our advantage. Econometricians all the time tell us that SCM fundamentally is about matching our treated unit‚Äôs common factors to the common factors embedded in the donor set (also see conditons 1-6 here). This idea is the whole motivation for synthetic control in the very first place, the notion that we are predicting a counterfactual for one unit or group using only the control units that behave similarly to the treated unit. If we get a good match (usually), we are likely also matching close to the factor loadings. This idea holds regardless of what kind of control units they are.\nA more flexible idea is the fine-grained potential outcomes model. This shifts the unit of analysis from groups (e.g., states, regions) to individuals within groups or regions. In this framework, groups are treated as distributions of individuals, and the goal is to estimate the ATT for the treated unit. This model allows for diverse donor types provided the aggregated donor outcomes reflect individuals who are exchangeable with the treated unit‚Äôs population.\nThe practical implication of this is that we are allowed to use different donor types (comparing cities to states, for example) on the condition that they help us learn about the trajectory of the target unit in the pre-intervention period. SCM does not care about what kind of donors you use, so long as they are informative donors.\n\n\nApplication\nUnconvinced? We can replicate some results to show this using mlsynth.\nReaders likely know the classic example of Prop 99, where California‚Äôs anti-tobacco program was compared to 38 donor states to estimate California‚Äôs counterfactual per capita cigarette consumption. But California is a ridiculously large economy, and is basically a small country by many metrics. So why do we need to use other states as the comparison units? Why can we not use larger, aggregated control units?\nIt turns out that we can do just that. Here, I compare California to the donor divisions of the United States. The outcome (tobacco smoking per capita) for the donor units are averaged over time at the division level. California‚Äôs remains as is. Just for demonstration, I compare the standard results (around an ATT of -17) to what we get when we use other estimators, except I aggregate the outcomes as I just described. Comparing across estimators, we can visually assess how well each method captures the pre and post-treatment trajectory for California using the aggregated division data, relative to the baseline result that uses the states as donors.\n\nimport pandas as pd\nfrom mlsynth import FDID, TSSC, PDA\n\n# URL of the .dta file\nurl = \"http://fmwww.bc.edu/repec/bocode/s/scul_p99_region.dta\"\n\n# Load the Stata file directly from the URL\ndf = pd.read_stata(url)\n\ndf[\"Proposition 99\"] =  ((df[\"region\"] == \"California\") & (df[\"year\"].dt.year &gt;= 1989)).astype(int)\n\n# Base configuration\nbase_config = {\n    \"df\": df,\n    \"outcome\": df.columns[2],\n    \"treat\": df.columns[-1],\n    \"unitid\": df.columns[0],\n    \"time\": df.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\", \"red\"]\n}\n\n# TSSC model\ntssc_config = base_config.copy()  # Start with base and modify if necessary\narco = TSSC(tssc_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is TSSC estimator, where we adjust for baseline differences using an intercept.\n\n# PDA model with method 'l2'\npda_config = base_config.copy()\npda_config[\"method\"] = \"l2\"\narcol2 = PDA(pda_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is \\(\\ell_2\\)-PDA estimator, where we allow for negative weights and an intercept.\n\n# FDID model\nfdid_config = base_config.copy()\narcofdid = FDID(fdid_config).fit()\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/utils/datautils.py:433: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nHere is the predictions of the FDID estimator, where we employ Forward Selection to choose the donor pool for the DID method.\n\n\nTakeaway\nThe qualitative takwaway is that these all produce very similar predictions to the baseline estimates. Even though we have reduced the donor pool by 30 units, we still maintain enough variation in the donor set to predict California‚Äôs pre-treatment trends, and so long as we are doing that, we may generally use as many relevant donors as we like. The issue is not that donors in general do not matter. I wrote mlsynth precisely because I take donor selection seriously, so we use methods such as Forward Selection, clustering, or other techniques to choose the right donors for a given target unit, especially in high-dimensional settings. The key issue though is that the ‚Äúright donors‚Äù do not necessarily need to be the same type of unit as the kind you are using, meaing we can be a little creative as to what we use as a donor. This is a philosophical question, not an econometric one. What matters is the relevance of the donor pool, as well as the richness of it so that we can construct better synthetic controls. If that means you compare buyers of one product to buyers of other products in online advertising, or an unusually large state/city to nations or states, then you may happily do that.\nNote that these conclusions are valid becasue of the underlying econometric theory that academic econometricians work on. If you do not know the basic theory of when we may or may not use SCM, you may be making wrong decisions in your modeling strategy. Or, you may not be taking advantage of existing study qualities to make your analyses better. Whether or not you can do the theoretical proofs in the papers I cited above matters less than whether you can understand what they mean for the applied econometrician/data scientist. After many years of torment and thinking it through, you will be able to use these methods more confidently, and with much more clarity than before.\n\n\n\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Test Post\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nShake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is An Average?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "gitact.html",
    "href": "gitact.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "",
    "text": "Life is full of repititive actions and work in data science is no exception to this. Perhaps you must produce a plot of some form to clean a specific dataset/set of datasets each week, say revenue data. Or perhaps, some files must be managed or cleaned, or a researcher wishes to collect the public housing data from Zillow each month when it updates. On one hand, we could physically do this ourselves, but this leaves us open to errors: for one, if we‚Äôre scraping a dataset from a website, what if the data are only there temporarily? What if it only exists for today, and then tomorrow it is updated with the then current data? What if we are away from our computers, and cannot be there to run script on our local machine? Surely, there must be a solution to this, and one such solution is Github Actions, which I learned about as an intern at Gainwell Technologies.\nThis post explains Github Actions in a very short, simple to intuit example. Of course, their applications are so vast that I cannot cover everything about them here, but they are still very useful to people who do a repetitive/very similar set of tasks over and over again."
  },
  {
    "objectID": "gitact.html#a-simple-yml-file",
    "href": "gitact.html#a-simple-yml-file",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Github Actions",
    "section": "A Simple yml file",
    "text": "A Simple yml file\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nHere is a yml file we can find in my AAA repo. It begins by naming the action to be performed, which is to scrape some city level gas price data. It is scheduled to run at 9:30 UTC time, or at 4:30 each morning Eastern Standard Time, with the cron time. It may also run whenever I wish for it to run; alternatively, we can specify that some actions run on push to a repo, branch, or even a certain working directory.\nname: City Level Gas Price Scraper\n\non:\n  schedule:\n    - cron: '30 9 * * *'\n  workflow_dispatch:\nActions proceed by defining a set of jobs for the workflow to excecute. In this case, it‚Äôs just ‚Äúscrape‚Äù, but you can define more jobs that are interdependent on one another. The job runs on a virtual computer, in this case Ubuntu. Jobs proceeds with a list of steps, or an order that the job proceeds in os the action can function. Step one is to checkout the repo, which essentially just clones the current repo on to the virtual machine.\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Check out repository\n      uses: actions/checkout@v3\n\nStep two here is to set up Python, since that‚Äôs the language I use, but I‚Äôm certain this may be done with R and other langauges (in fact, this entire blog is written in Quarto, which the action must install before working with each post). Note how we specify the version of Python here, too. We then install pip and the requirements that the Python code needs to run the scrape.\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\nNow, we simply run the code, which is no different practically speaking from running the code from our command line. Note that how the file is defined within a driectory, which must be specified if the python file is not at the root (different ways of doing this).\n\n    - name: Run gas scraper\n      run: |\n        python \"City Scrape/cityscrape.py\"\n\nNext, I make the directory for the data if it does not exist.\n\n    - name: Ensure directory exists\n      run: |\n        mkdir -p \"City Scrape/Data/\"\n\nAnd finally, we commit the csv file my code creates to the directory at the repo. We use the Github Actions bot to do the commit. If there are no changes between any of the files before and after committing, we don‚Äôt add them and then the job ends (this is what happens if I try to run the action after it‚Äôs ran already that day). If not, the files are pushed. In my case, the files are named things like City_2025-01-31.csv. These files are staged, or prepared for being committed, with the addition of the commit message that I‚Äôm updating the data for that day. Then we push them to the directory of interest, and then job complete.\n    - name: Commit and push updated CSV to repository\n      run: |\n        git config --global user.name \"GitHub Actions\"\n        git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n        \n        # Check if there are changes before committing\n        if git diff --quiet && git diff --staged --quiet; then\n          echo \"No changes detected, skipping commit.\"\n          exit 0\n        fi\n        \n        git add \"City Scrape/Data/City_*.csv\"\n        git commit -m \"Update gas prices data for $(date +'%Y-%m-%d')\"\n        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe real value of this is not the data I‚Äôm scraping (unless you‚Äôre a fuel economist/policy scholar). The value is that this job runs independent of my will. I do not have to be at my computer for this to run. I do not need to worry about whether my computer has power or whether I‚Äôm home to personally oversee it. The value here is that I‚Äôve manually gotten my computer to do a specific task every single day, the correct way (assuming you‚Äôve coded everything right!!!), every time. Of course, this job is so insignificant such that I did not feel the need to run additional safechecks (say AAA‚Äôs site is down, I could have the action restart in 6 hours, or have it curl the website on the hour until it does respond), but obviously you can do plenty more here if the task matters enough. This is also a very partial list of what may be done. You can also place lots of parameters around your actions that may make life easier, or employ pre-commit hooks which can do checks for the quality of the code and other tasks before anything is committed, which will fail if they are not satisfied.\nAlso, it‚Äôs worth noting that Actions may run in conjunction with cloud computing for larger-scale jobs. So, if you‚Äôre a policy researcher, and your org uses Github but not using Actions for all kinds of process automation tasks, these provide a very useful tool to handle repetitive actions that free up your time to do other things. After all, a key benefit of script is to automate the boring stuff, provided that we‚Äôve automated our tasks correctly."
  },
  {
    "objectID": "shaemax.html",
    "href": "shaemax.html",
    "title": "Shake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods",
    "section": "",
    "text": "Regularization in synthetic control methods has become an important econometric topic in recent years. Let \\(\\mathbf{y}_1 \\in \\mathbb{R}^{T_0}\\) denote the pre-treatment outcomes for the treated unit and let \\(\\mathbf{Y}_0 \\in \\mathbb{R}^{T_0 \\times |\\mathcal{N}_0|}\\) denote the corresponding donor matrix. In the most general terms, an SCM is a form of convex optimization where we use a set of donor units that were not exposed to a treatment to predict how the outcomes for a single (or set of) target unit(s) would have evolved without the treatment. In full generality, a synthetic control estimator solves the following family of programs:\n\\[\n\\mathbf{w} \\;\\in\\;\n\\underset{\\mathbf{w} \\in \\mathcal{C}}{\\operatorname*{argmin}}\n\\;\n\\mathcal{L}(\\mathbf{Y}_0, \\mathbf{y}_1, \\mathbf{w})\n\\;+\\;\n\\mathcal{P}(\\mathbf{w}),\n\\]\nsubject to\n\\[\n\\mathcal{B}(\\mathbf{Y}_0, \\mathbf{y}_1, \\mathbf{w}) \\;\\le\\; \\boldsymbol{\\tau}.\n\\]\nHere \\(\\mathcal{L}(\\cdot)\\) denotes a data-dependent loss function governing pre-treatment fit, \\(\\mathcal{P}(\\cdot)\\) denotes a regularization or geometry-inducing penalty on the weights, and \\(\\mathcal{C}\\) denotes a convex admissible set for the donor weights. The operator \\(\\mathcal{B}(\\cdot)\\) encodes balance or moment conditions, and \\(\\boldsymbol{\\tau}\\) controls the degree of relaxation. Either \\(\\mathcal{L}\\) or \\(\\mathcal{B}\\) may be identically zero, but not both. The classical synthetic control estimator of Abadie, Diamond, and Hainmueller is obtained by setting\n\\[\n\\mathcal{L}(\\mathbf{Y}_0, \\mathbf{y}_1, \\mathbf{w}) =\n\\left\\|\n\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w}\n\\right\\|_2^2,\n\\quad\n\\mathcal{P}(\\mathbf{w}) = 0,\n\\quad\n\\mathcal{B} \\equiv 0,\n\\]\nwith admissible weights given by\n\\[\n\\mathcal{C}_{\\text{simplex}}:= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{|\\mathcal{N}_0|} : \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nIn this case, balance is enforced entirely through the objective function."
  },
  {
    "objectID": "shaemax.html#regularized-synthetic-control",
    "href": "shaemax.html#regularized-synthetic-control",
    "title": "Shake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods",
    "section": "Regularized Synthetic Control",
    "text": "Regularized Synthetic Control\nAnalysts have developed formulations of synthetic control that simultaneously account for level differences and control the geometry of the donor weights. To allow for level differences, we augment the donor matrix with an intercept term:\n\\[\n\\widetilde{\\mathbf{Y}}_0\n=\n\\begin{bmatrix}\n\\mathbf{Y}_0 & \\mathbf{1}\n\\end{bmatrix},\n\\qquad\n\\widetilde{\\mathbf{w}}\n=\n\\begin{bmatrix}\n\\mathbf{w} \\\\\nb_0\n\\end{bmatrix},\n\\]\nwhere \\(b_0 \\in \\mathbb{R}\\) captures an additive baseline shift. Regularization of the coefficients is also an important issue. Note that all penalties are applied to \\(\\mathbf{w}\\), while \\(b_0\\) is left unpenalized. For a graphical example, see the plot:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grid for contour plots\nx = np.linspace(-1.2, 1.2, 400)\ny = np.linspace(-1.2, 1.2, 400)\nX, Y = np.meshgrid(x, y)\n\n# Norms\nL1 = np.abs(X) + np.abs(Y)\nL2 = np.sqrt(X**2 + Y**2)\nLinf = np.maximum(np.abs(X), np.abs(Y))\n\nplt.figure(figsize=(6, 6))\n\n# Plot unit balls\nplt.contour(X, Y, L1, levels=[1], colors=\"k\", linestyles=\"dashed\")\nplt.contour(X, Y, L2, levels=[1], colors=\"blue\")\nplt.contour(X, Y, Linf, levels=[1], colors=\"red\")\n\n# Axes and aesthetics\nplt.axhline(0)\nplt.axvline(0)\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.xlim(-1.2, 1.2)\nplt.ylim(-1.2, 1.2)\n\nplt.title(\"2D Geometry of $\\\\ell_1$, $\\\\ell_2$, and $\\\\ell_\\\\infty$ Norms\")\nplt.xlabel(\"$w_1$\")\nplt.ylabel(\"$w_2$\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nA common choice is the elastic net penalty, which encompasses a wide class of regularized SC estimators. These fit into the general framework by setting the balance operator \\(\\mathcal{B} \\equiv 0\\) (enforcing fit entirely through the objective, as in classical SCM), using a squared-error loss \\(\\mathcal{L}\\) for pre-treatment fit, introducing a non-zero penalty \\(\\mathcal{P}\\) to control weight geometry, and often relaxing the admissible set \\(\\mathcal{C}\\) to allow greater flexibility (e.g., negative weights and no sum-to-one constraint, though variants may retain non-negativity for interpretability).\nTo allow for level differences, we augment the donor matrix with an intercept term:\n\\[\n\\widetilde{\\mathbf{Y}}_0\n=\n\\begin{bmatrix}\n\\mathbf{Y}_0 & \\mathbf{1}\n\\end{bmatrix},\n\\qquad\n\\widetilde{\\mathbf{w}}\n=\n\\begin{bmatrix}\n\\mathbf{w} \\\\\nb_0\n\\end{bmatrix},\n\\]\nwhere \\(b_0 \\in \\mathbb{R}\\) captures an additive baseline shift. The loss is then:\n\\[\n\\mathcal{L}(\\mathbf{Y}_0, \\mathbf{y}_1, \\mathbf{w}, b_0) =\n\\left\\|\n\\mathbf{y}_1 - \\mathbf{Y}_0 \\mathbf{w} - b_0 \\mathbf{1}\n\\right\\|_2^2,\n\\]\nwith \\(\\mathcal{B} \\equiv 0\\). Note that all penalties are applied only to \\(\\mathbf{w}\\), while \\(b_0\\) is left unpenalized.\nThe first flavor, by Doudchenko and Imbens (2016), interpolates between the \\(\\ell_1\\) norm and the \\(\\ell_2\\) norm:\n\\[\n\\mathcal{P}(\\mathbf{w}) = \\lambda \\Big( \\alpha \\lVert \\mathbf{w} \\rVert_1 + (1 - \\alpha) \\lVert \\mathbf{w} \\rVert_2^2 \\Big), \\qquad \\alpha \\in [0,1],\n\\]\nwith \\(\\mathcal{C} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{|\\mathcal{N}_0|}, b_0 \\in \\mathbb{R} \\right\\}\\) (relaxing non-negativity and sum-to-one). The \\(\\ell_1\\) term encourages sparsity (SC supported on few donors), while the \\(\\ell_2^2\\) term stabilizes against collinearity. Intermediate \\(\\alpha \\in (0,1)\\) yields the elastic net; special cases are \\(\\alpha = 1\\) (pure LASSO) and \\(\\alpha = 0\\) (pure Ridge).\nAlternatively, as studied by Wang, Xing, and Ye (2025), one may replace the \\(\\ell_2\\) term with the \\(\\ell_\\infty\\) norm:\n\\[\n\\mathcal{P}(\\mathbf{w}) = \\lambda \\Big( \\alpha \\lVert \\mathbf{w} \\rVert_1 + (1 - \\alpha) \\lVert \\mathbf{w} \\rVert_\\infty \\Big), \\qquad \\alpha \\in [0,1],\n\\]\nagain with \\(\\mathcal{C} = \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{|\\mathcal{N}_0|}, b_0 \\in \\mathbb{R} \\right\\}\\). The \\(\\ell_\\infty\\) component caps the maximum absolute weight, producing a ‚Äúbalanced sparsity‚Äù effect: a small number of donors may be selected, but none dominates. When using the \\(\\ell_\\infty\\) variant of the elastic net, \\(\\alpha = 0\\) corresponds to the pure max-norm penalty. For a graphical example, see the plot below.\nGeometrically, the \\(\\ell_1\\)‚Äì\\(\\ell_\\infty\\) penalty replaces the circular ridge ball with a hyper-rectangular region, reflecting the analyst‚Äôs preference for bounding donor influence rather than merely smoothing it. These special cases highlight how the elastic net family interpolates continuously between sparsity and smoothness (or maximum weight control). Within the general framework, the optimization problem becomes:\n\\[\n(\\mathbf{w}, b_0) \\;\\in\\;\n\\underset{\\mathbf{w} \\in \\mathbb{R}^{|\\mathcal{N}_0|}, b_0 \\in \\mathbb{R}}{\\operatorname*{argmin}}\n\\;\n\\mathcal{L}(\\mathbf{Y}_0, \\mathbf{y}_1, \\mathbf{w}, b_0)\n+\n\\lambda \\Big( \\alpha \\lVert \\mathbf{w} \\rVert_1 + (1 - \\alpha) \\lVert \\mathbf{w} \\rVert_q \\Big),\n\\]\nwhere \\(q = 2\\) recovers the standard elastic net (with \\(\\lVert \\mathbf{w} \\rVert_2^2\\)) and \\(q = \\infty\\) recovers the max-norm variant. Conceptually, the \\(\\ell_2\\) term spreads weights smoothly, stabilizing against collinearity, while the \\(\\ell_\\infty\\) term limits any single donor‚Äôs dominance. This is analogous to a portfolio with position limits: no single donor can dominate the synthetic control, while overall weight distribution can be controlled via \\(\\alpha\\) and \\(\\lambda\\).\nIn practice, this can be important: in the original 2003 SCM paper, the donors Catalonia and Madrid received weights of 0.8508 and 0.1492, respectively. While this allocation makes sense economically, there are settings where analysts may wish to reduce the influence of any single donor."
  },
  {
    "objectID": "shaemax.html#a-relaxed-balanced-approach",
    "href": "shaemax.html#a-relaxed-balanced-approach",
    "title": "Shake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods",
    "section": "A Relaxed Balanced Approach",
    "text": "A Relaxed Balanced Approach\nOther approaches to mitigating high-dimensionality are possible. Liao, Shi, and Zheng (2025), introduce a relaxation of the fit conditions imposed by the elastic net estimators above. Here, the loss is set to zero, a penalty is placed on the weights, and fit is enforced via a constraint\n\\[\n(\\mathbf{w},\\gamma)\n\\in\n\\underset{\n\\substack{\n\\mathbf{w}\\in\\mathbb{R}^{|\\mathcal{N}_0|}_+,\\\\[2pt]\n\\mathbf{1}^\\top\\mathbf{w}=1,\\;\\gamma\\in\\mathbb{R}\n}\n}{\\operatorname*{argmin}}\n\\;\\;\n\\|\\mathbf{w}\\|_2^2\n\\quad\n\\text{s.t.}\n\\quad\n\\left\\|\n\\mathbf{G}\\mathbf{w}\n-\n\\mathbf{a}\n+\n\\gamma\\mathbf{1}\n\\right\\|_\\infty\n\\le\n\\tau,\n\\qquad\n\\mathbf{G}\n=\n\\frac{1}{T_0}\\mathbf{Y}_0^\\top\\mathbf{Y}_0,\n\\quad\n\\mathbf{a}\n=\n\\frac{1}{T_0}\\mathbf{Y}_0^\\top\\mathbf{y}_1.\n\\]\nThe slack variable \\(\\gamma\\) shifts all donor projections uniformly and is estimated jointly with \\(\\mathbf{w}\\), allowing small, evenly distributed violations when exact pre-treatment matching is infeasible. Alternative penalties can also be used: negative entropy\n\\[\n(\\mathbf{w},\\gamma)\n=\n\\underset{\n\\substack{\n\\mathbf{w}\\in\\mathbb{R}_+^{|\\mathcal{N}_0|},\\\\\n\\mathbf{1}^\\top\\mathbf{w}=1,\\;\\gamma\\in\\mathbb{R}\n}\n}{\\operatorname*{argmin}}\n\\left[\n\\sum_{j} w_j \\log w_j\n\\right]\n\\quad\n\\text{s.t.}\\quad\n\\big\\|G w - a + \\gamma\\mathbf{1}\\big\\|_\\infty \\le \\tau.\n\\]\nencourages dense allocation, while empirical likelihood\n\\[\n(\\mathbf{w},\\gamma)\n=\n\\underset{\n\\substack{\n\\mathbf{w}\\in\\mathbb{R}_+^{|\\mathcal{N}_0|},\\\\\n\\mathbf{1}^\\top\\mathbf{w}=1,\\;\\gamma\\in\\mathbb{R}\n}\n}{\\operatorname*{argmin}}\n\\left[\n-\\sum_{j} \\log w_j\n\\right]\n\\quad\n\\text{s.t.}\\quad\n\\|G w - a + \\gamma\\mathbf{1}\\|_\\infty \\le \\tau.\n\\]\ndiscourages zero weights. In all cases, the constraint enforces relaxed balance, while the penalty governs the weight structure.\nThis contrasts with classical SCM and elastic net approaches. There, fit is minimized in the objective and the weights absorb all discrepancies. In the relaxed balance method, fit is a constraint, the objective imposes an \\(\\ell_2\\) (or other) penalty on the weights, and \\(\\gamma\\) allows controlled relaxation‚Äîanalogous to portfolio optimization with position limits, where constraints cap exposure to any single asset while the objective encourages diversification or avoidance of zero positions. This separation is particularly useful in high-dimensional donor pools or when robustness to over-reliance on any single donor is desired."
  },
  {
    "objectID": "shaemax.html#an-example-in-mlsynth",
    "href": "shaemax.html#an-example-in-mlsynth",
    "title": "Shake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods",
    "section": "An Example in mlsynth",
    "text": "An Example in mlsynth\nAs usual, these may be implemented in mlsynth, using the RESCM class. To install mlsynth, you must have Github on your machine and and do\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\n\nfrom the command line or within your virtual Python environment. To run the model, users provide a panel data frame along with the outcome variable, a treatment indicator, a unit identifier, and a time variable. Optional configuration includes whether to display plots, save results, and customize the colors of treated and counterfactual series are also present.\nModeling options are controlled via the models_to_run dictionary, where a run: bool specifies whether the model is estimated. Relaxed SCM estimators are specified by the \"RELAXED\" key, with the type of relaxation chosen through the relaxation parameter. Options include l2 for standard Euclidean relaxation, entropy for entropy-based relaxation, and el for empirical likelihood relaxation. The relaxation strength is controlled by the tau parameter, which can be provided explicitly (in which case no cross-validation is performed) or selected via cross-validation over a grid of candidate values. The number of candidate taus is controlled by n_taus, and the number of cross-validation folds by n_splits.\nElastic Net SCM estimators are specified by the \"ELASTIC\" key and combine L1 with either \\(\\ell_2\\) or \\(\\ell_\\infty\\) penalties on donor weights via enet_type=\"L1_L2\" or enet_type=\"L1_INF\". The alpha parameter controls the mixture between L1 and the second norm, while lambda controls the overall penalty strength. If lambda is zero, no cross-validation is performed. If lambda is provided but alpha is not, cross-validation is performed over alpha, and vice versa. An optional intercept can be added via fit_intercept.\nThe feasible set for the donor weights is specified via the constraint_type parameter. Users may select unit, simplex, affine, nonneg, or unconstrained. These are defined as follows:\n\\[\n\\begin{aligned}\n\\mathcal{C}_{\\text{unit}}        &:= \\left\\{ \\mathbf{w} \\in [0,1]^{|\\mathcal{N}_0|} \\right\\}\\\\[6pt]\n\\mathcal{C}_{\\text{simplex}}    &:= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\geq 0}^{|\\mathcal{N}_0|} : \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\\\\[6pt]\n\\mathcal{C}_{\\text{affine}}      &:= \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{|\\mathcal{N}_0|} : \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\\\\[6pt]\n\\mathcal{C}_{\\text{nonneg}}      &:= \\mathbb{R}_{\\geq 0}^{|\\mathcal{N}_0|}\\\\[6pt]\n\\mathcal{C}_{\\text{unconstrained}} &:= \\mathbb{R}^{|\\mathcal{N}_0|}\n\\end{aligned}\n\\]\nThe unit constraint restricts each weight to the unit interval independently. This forces the synthetic control to be a strict interpolation with bounded contributions‚Äîno single donor can contribute more than 100%, making it the most conservative option that completely rules out extrapolation while providing highly interpretable, share-like weights. The classical simplex constraint requires non-negative weights that sum to one. The synthetic control is therefore a convex combination of the donors, lying inside or on the boundary of their convex hull. This setting offers excellent interpretability (weights resemble portfolio shares or probabilities) and prohibits any extrapolation, which is why it remains the default choice in most applied synthetic control studies. The affine constraint relaxes non-negativity but retains the sum-to-one requirement. It allows negative weights, meaning the synthetic control can lie outside the convex hull while staying within the affine subspace spanned by the donors. This introduces controlled extrapolation (useful when the treated unit has a different level or trend but shares parallel paths), but at the cost of reduced interpretability, as negative weights imply ‚Äúsubtracting‚Äù the contribution of certain donors. The nonneg constraint permits any non-negative weights without normalization. The synthetic control can lie far outside the convex hull along positive directions (conic extrapolation), which can substantially improve pre-treatment fit in some cases, though the lack of scaling makes the weights harder to interpret economically. Finally, the unconstrained option imposes no restrictions whatsoever, allowing signed weights of any magnitude. This grants maximum flexibility and typically yields the best pre-treatment fit (as in most elastic-net or machine-learning-style SCMs), but it also permits unrestricted extrapolation and offers the lowest interpretability, with weights often lacking direct economic meaning.\nRelaxation methods like entropy and el are simplex weights by definition. Once the configuration is set, calling RESCM(config).fit() estimates the selected models on the pre-treatment period and produces counterfactual predictions for both pre- and post-treatment periods. The output is an EstimatorResults object containing separate results for relaxed and elastic SCMs (depending on what the user specifies), including donor weights, time series of observed and counterfactual outcomes, and fit diagnostics such as pre- and post-treatment RMSE. To date, this is the most flexible class of estimators mlsynth provides."
  },
  {
    "objectID": "shaemax.html#application",
    "href": "shaemax.html#application",
    "title": "Shake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods",
    "section": "Application",
    "text": "Application\nNow for an empirical application for the Basque Country. First I load the data and set up the model\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mlsynth import RESCM\nfrom IPython.display import display, Markdown\n\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv\"\ndata = pd.read_csv(url)\n\nbase_config = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": False,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\", \"red\"]\n}\n\n\nrelax_types = [\"l2\", \"entropy\", \"el\"]\nresults = {}\n\nfor rtype in relax_types:\n\n    config = {\n        **base_config,\n        \"models_to_run\": {\n            \"RELAXED\": {\n                \"run\": True,\n                \"relaxation\": rtype,\n                \"tau\": None,\n                \"n_taus\": 100\n            }\n        }\n    }\n\n    arco = RESCM(config).fit()\n\n    results[f\"RELAXED_{rtype}\"] = {\n        \"counterfactual\": arco.relax.time_series.counterfactual_outcome,\n        \"weights\": arco.relax.weights.donor_weights,\n        \"method\": arco.relax.method_details.method_name,\n        \"tau\": arco.relax.method_details.parameters_used['tau_used']\n    }\n\nconfig_l1l2 = {\n    **base_config,\n    \"models_to_run\": {\n        \"ELASTIC\": {\n            \"run\": True,\n            \"constraint_type\": \"unit\",\n            \"intercept\": False,\n            \"enet_type\": \"L1_L2\"\n        }\n    }\n}\n\narco_l1l2 = RESCM(config_l1l2).fit()\n\nresults[\"ELASTIC_L1_L2\"] = {\n    \"counterfactual\": arco_l1l2.elastic.time_series.counterfactual_outcome,\n    \"weights\": arco_l1l2.elastic.weights.donor_weights,\n    \"method\": arco_l1l2.elastic.method_details.method_name\n}\n\nconfig_l1linf = {\n    **base_config,\n    \"models_to_run\": {\n        \"ELASTIC\": {\n            \"run\": True,\n            \"constraint_type\": \"unit\",\n            \"intercept\": False,\n            \"enet_type\": \"L1_INF\"\n        }\n    }\n}\n\narco_l1linf = RESCM(config_l1linf).fit()\n\nresults[\"ELASTIC_L1_LINF\"] = {\n    \"counterfactual\": arco_l1linf.elastic.time_series.counterfactual_outcome,\n    \"weights\": arco_l1linf.elastic.weights.donor_weights,\n    \"method\": arco_l1linf.elastic.method_details.method_name\n}\n\n\nweights_df = pd.DataFrame({\n    name: pd.Series(res[\"weights\"])\n    for name, res in results.items()\n}).round(3)\n\n\n# ---------------- Display as Markdown ----------------\nmarkdown_table = weights_df.to_markdown()\n\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/cvxpy/problems/problem.py:1539: UserWarning:\n\nSolution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/cvxpy/problems/problem.py:1539: UserWarning:\n\nSolution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n\n\n\nThe synthetic control hyper-parameters (tau, alpha, and lambda) were tuned via time-series cross validation. For the elastic net methods, two variants were considered: \\(\\alpha \\ell_1 + (1-\\alpha) \\ell_\\infty\\) and \\(\\alpha \\ell_1 + (1-\\alpha) \\ell_2\\). Both employed 4-fold time series cross-validation with standardized donor predictors. As we eiscussed above for the elastic net models, the alpha parameter governs the trade-off between sparsity and the secondary norm. For both variants, this was was approximately 0.554, while the lambda parameter controlling overall regularization strength was around 0.084 for the \\(\\ell_1+\\ell_\\infty\\) variant and 0.128 for the \\(\\ell_1+\\ell_2\\) variant. I used affine weights for both of these. For the relaxed balanced synthetic control variants‚Äî\\(\\ell_2\\) relaxation, empirical likelihood, and entropy relaxation‚Äîthe slack parameter \\(\\tau\\) was set to approximately 0.00262 for all three. This small value enforces tight balance, encouraging the synthetic control to match pre-treatment outcomes closely while allowing minimal flexibility to stabilize the optimization and prevent extreme weight concentration. This was also tuned by the same cross validation method as above, and you can see my Python code for the sklearn details.\n\n\nCode\n# ---------------- Plotting ----------------\n\nplt.figure(figsize=(12, 6))\n\nobserved = arco_l1linf.elastic.time_series.observed_outcome\nt = range(len(observed))\n\nplt.plot(t, observed, color=\"black\", linewidth=2, label=\"Observed\")\n\nstyles = {\n    \"RELAXED_l2\": \"--\",\n    \"RELAXED_entropy\": \"--\",\n    \"RELAXED_el\": \"--\",\n    \"ELASTIC_L1_L2\": \"-\",\n    \"ELASTIC_L1_LINF\": \":\"\n}\n\nfor name, res in results.items():\n    plt.plot(t, res[\"counterfactual\"], linestyle=styles[name], label=res[\"method\"])\n\nplt.title(\"Basque Country, Synthetic Controls\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"GDP per Capita\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHere are the weights:\n\n\nCode\ndisplay(Markdown(markdown_table))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRELAXED_l2\nRELAXED_entropy\nRELAXED_el\nELASTIC_L1_L2\nELASTIC_L1_LINF\n\n\n\n\nAndalucia\n0\n0\n0.012\n0\n0\n\n\nAragon\n0.176\n0.132\n0.047\n0\n0.263\n\n\nBaleares (Islas)\n0\n0.005\n0.009\n0\n0\n\n\nCanarias\n0\n0\n0.007\n0\n0\n\n\nCantabria\n0.127\n0.087\n0.035\n0\n0\n\n\nCastilla Y Leon\n0\n0.007\n0.016\n0\n0\n\n\nCastilla-La Mancha\n0\n0\n0.008\n0\n0\n\n\nCataluna\n0.289\n0.387\n0.621\n0.53\n0.239\n\n\nComunidad Valenciana\n0\n0.004\n0.015\n0\n0\n\n\nExtremadura\n0\n0\n0.013\n0\n0\n\n\nGalicia\n0\n0\n0.01\n0\n0\n\n\nMadrid (Comunidad De)\n0.206\n0.177\n0.107\n0.067\n0.143\n\n\nMurcia (Region de)\n0\n0.01\n0.015\n0\n0\n\n\nNavarra (Comunidad Foral De)\n0.037\n0.04\n0.024\n0\n0\n\n\nPrincipado De Asturias\n0\n0.028\n0.022\n0.115\n0.102\n\n\nRioja (La)\n0.166\n0.121\n0.04\n0.288\n0.252\n\n\n\n\n\nAcross all model variants (elastic net and relaxed balanced synthetic control alike) the donor weights exhibit a high degree of stability in which regions matter, even though the regularization structure changes how weight mass is distributed. In particular, Catalonia consistently receives the largest or among the largest weights, regardless of whether the method emphasizes sparsity (\\(\\ell_1\\)-based elastic nets), smoothness (\\(\\ell_2\\) relaxation), or dispersion (entropy relaxation). This is intuitive: Catalonia shares a closely related political and economic trajectory with the Basque Country, including early industrialization, strong regional identity, and similar exposure to national and international economic forces. In the outcome space, it is therefore unsurprising that Catalonia emerges as the closest synthetic analog. Beyond Catalonia, a recurring set of regions (La Rioja, Cantabria, and Navarre) also receive nontrivial weights across specifications. These regions are literal geographic neighbors of the Basque Country, sharing labor markets, trade linkages, and historical development patterns. Their repeated appearance across models suggests that geographic proximity aligns well with similarity in pre-treatment outcomes, reinforcing the idea that the optimization is recovering economically meaningful structure rather than exploiting numerical artifacts. Aragon also consistently enters the synthetic control, despite not bordering the Basque Country directly. Its location in northern Spain and its intermediate economic profile make it a plausible contributor, particularly under methods that allow moderate dispersion of weights. Finally, Madrid receives a positive‚Äîthough typically smaller‚Äîweight across most specifications. While geographically distant, Madrid‚Äôs role as the national capital and its high level of wealth and development make it a natural partial comparator, especially in models that permit signed or less restrictive weighting schemes.\nIn the original SC solution, the donor weights were highly concentrated, with weights of 0.8508 and 0.1492 of Catalonia and Madrid respectively. This extreme concentration is a well-known feature of canonical SCM: when the feasible set is restricted to the simplex and no explicit regularization is imposed, the optimizer is free to place nearly all mass on the single donor that best matches the treated unit in the pre-treatment outcome space. In this sense, the original weights represent a corner solution driven almost entirely by fit, with little incentive to distribute weight more broadly across similar regions. Introducing a max-norm‚Äìbased component (either explicitly via an \\(\\ell_\\infty\\) penalty or implicitly through relaxed balance constraints) fundamentally changes this behavior. The \\(\\ell_\\infty\\) norm directly penalizes the largest coefficient, which has the effect of tempering the size of dominant weights. Rather than allowing Catalonia to absorb nearly all the mass, the optimization trades a small amount of fit for a substantial reduction in the maximum weight. As a result, weight is redistributed toward nearby or economically similar regions‚Äîsuch as La Rioja, Cantabria, Navarre, and Aragon‚Äîwhile still preserving Catalonia‚Äôs central role. Importantly, this redistribution is not arbitrary: it occurs along economically meaningful dimensions already latent in the data. This tempering effect operates in both the fit space and the balance space. In the fit space, the max norm discourages over-reliance on a single donor to track pre-treatment outcomes perfectly, leading to a smoother approximation that averages across multiple close matches. In the balance space, the \\(\\ell_\\infty\\) constraint limits the extent to which any single donor can shoulder the burden of matching outcomes, thereby encouraging broader participation in satisfying the balance conditions. The result is a synthetic control that is less brittle and less sensitive to idiosyncratic noise in any one donor unit.\nCrucially, this moderation does not overturn the substantive story. Catalonia remains the dominant contributor across specifications, reflecting its strong historical, political, and economic similarity to the Basque Country. Madrid continues to receive a positive but secondary weight, consistent with its role as the national capital and a benchmark for economic development. What changes under the max norm is not who matters, but how much any single region is allowed to matter. In this way, the authors frame their resepctive \\(\\ell_\\infty\\) norm strucures as a principled mechanisms for controlling concentration of weights in high-dimensional setting‚Äîyielding weights that are more evenly distributed, more stable, and arguably more credible, while still preserving the core economic relationships identified by the original SCM."
  },
  {
    "objectID": "lfe1.html",
    "href": "lfe1.html",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "",
    "text": "What is an applied econometrician or data scientist? At its simplest, both are practitioners of statistics who apply econometric or statistical theory to answer questions that economists, businesses, or other organizations care about. Unless your role is narrowly defined as ‚Äústandard‚Äù analytics (mostly SQL and dashboards, as far as I understand), this is true whether you work in causal inference, deep learning, computer vision, or some other branch of the field. We often talk about ‚Äúdelivering value‚Äù or ‚Äúproviding insights.‚Äù Some folks claim that deep business experience (which can be learned like other mindsets) matters more than technical knowledge, and that some data science teams ‚Äúfail due to being too scientific‚Äù. But pause for a moment ‚Äî look out your window. Do you see insights, or ‚Äúbusiness sense‚Äù floating around? Of course not. Insights do not exist in nature; they‚Äôre created. They are derived through data analysis. Business sense does not exist in a vacuum, it is informed by empirical observations about the real world. And what makes that analysis scientific, and business sense sensbile (not just guessing with sexed up software tools) is the theoretical source under which we arrive at these conclusions. Even if you‚Äôre not proving lemmas in your day job (I don‚Äôt), those underpinnings are exactly what allow us to trust our results. By extension, some authors describe synthetic controls as a black box function, suggesting that arcane operations under the hood spit out a result. And while that temptation is understandable, unless you‚Äôre dealing with truly complex models (like neural nets or GenAI, which themselves rest on explainable math foundations), the math behind vanilla SCM is well understood.\nThis matters even more because, as this Medium post observes, industry often values breadth and speed over theoretical depth. This is correct, but it risks overlooking how theory actually accelerates practical work. Theory for applied data scientists and econometricians (at least in the work I do) is a force multiplier. It allows you move quickly and diagnose problems/suggests improvements effectively. It makes applied work more accurate, more explainable, and more trustworthy. Treating SCM (or any model) as a black box might get you a nice graph, but what separates practitioners who are effective and practitioners who are less so is the understanding and application of theory to the business question at hand. After all, our estimates are not produced for nothing, they are meant to inform the policy/business outcomes we care about. Just because Nobody Dies If This Estimate Is Wrong is not a very comforting principle to base our analyses on.\nIn this blog post, first, I‚Äôll compute a synthetic control by hand (how often do we do such a thing?) using calculus, KKT optimality conditions, and geometry. Then, I‚Äôll walk through a recent real-world application, showing how theory very easily informs the way we run our models and understand our results."
  },
  {
    "objectID": "lfe1.html#calculus",
    "href": "lfe1.html#calculus",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "Calculus",
    "text": "Calculus\nCalculus is the heart of SCM, which is unsurprising given that it‚Äôs an optimization problem. However, after years of reading about SCM, I‚Äôve never ever seen somebody solve a simple synthetic control by hand. I do this below. So, with the help of a little Boyd-ie, I went through the process of finding the weights by hand.\n\n\nSolved By Calculus\n\nWe aim to find a synthetic control as a convex combination of the donors that matches the target vector as closely as possible:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\ast} &= \\underset{\\mathbf w \\in \\mathbb{R}_{\\ge 0}^2}{\\operatorname*{argmin}} \\;\\; \\|\\mathbf y - \\mathbf Y \\mathbf w\\|_2^2 \\\\\n\\text{s.t.} \\quad & \\mathbf 1^\\top \\mathbf w = 1,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{Y} = \\begin{bmatrix} \\mathbf{y}_1 & \\mathbf{y}_2 \\end{bmatrix} = \\begin{bmatrix} 20 & 48 \\\\ 22 & 50 \\end{bmatrix}\\).\nThe synthetic control is a weighted average:\n\\[\n\\hat{\\mathbf{y}} = w_1 \\mathbf{y}_1 + w_2 \\mathbf{y}_2,¬†\n\\quad w_1, w_2 \\geq 0, \\quad w_1 + w_2 = 1.\n\\]\nof control units. Specifically, it is a convex combination. A convex combination of a set of vectors \\(\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_n \\in \\mathbb{R}^N\\) is any vector of the form\n\\[\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i \\mathbf{w}_i \\quad \\text{subject to} \\quad \\alpha_i \\ge 0 \\text{ for all } i = 1, \\dots, n, \\quad \\sum_{i=1}^n \\alpha_i = 1.\n\\]\nNotice how in this case two donor units. From the definition above, it helps us to define one weight in terms of the other; for example, if the optimal value for one weight is 0.2, the other one must be 0.8.\nSubstituting \\(w_2 = 1 - w_1\\):\n\\[\n\\hat{\\mathbf{y}}(w_1) = w_1 \\mathbf{y}_1 + (1 - w_1)\\mathbf{y}_2¬†\n= \\mathbf{y}_2 + w_1(\\mathbf{y}_1 - \\mathbf{y}_2),\n\\]\nand since \\(\\mathbf{y}_1 - \\mathbf{y}_2 = \\begin{bmatrix}-28 \\\\ -28\\end{bmatrix}\\), we have\n\\[\n\\hat{\\mathbf{y}}(w_1) = \\begin{bmatrix} 48 \\\\ 50 \\end{bmatrix} + w_1 \\begin{bmatrix}-28 \\\\ -28\\end{bmatrix}\n= \\begin{bmatrix} 48 - 28 w_1 \\\\ 50 - 28 w_1 \\end{bmatrix}.\n\\]\nThis is just the predicted outcome if we put weight \\(w_1\\) on donor 1 and weight \\(1-w_1\\) on donor 2 (again, defining weight 1 in terms of weight 2). By definition, the residuals are the differences between the treated outcomes and the synthetic prediction:\n\\[\n\\mathbf{r}(w_1) = \\mathbf{y} - \\hat{\\mathbf{y}}(w_1).\n\\]\nPlugging in:\n\\[\n\\mathbf{r}(w_1) =¬†\n\\begin{bmatrix} 50 \\\\ 60 \\end{bmatrix} -¬†\n\\begin{bmatrix} 48 - 28 w_1 \\\\ 50 - 28 w_1 \\end{bmatrix}.\n\\]\nSo:\n\\[\n\\mathbf{r}(w_1) = \\begin{bmatrix} 2 + 28w_1 \\\\ 10 + 28w_1 \\end{bmatrix}.\n\\]\n\nLet‚Äôs define the main calculus rules we‚Äôll use:\n\nConstant Multiple Rule. If \\(f(x) = c \\cdot g(x)\\), where \\(c\\) is a constant and \\(g(x)\\) is differentiable, then the derivative of \\(f\\) with respect to \\(x\\) is:\n\\[\nf'(x) = c \\cdot g'(x).\n\\]\nThis rule allows you to pull constants out of derivatives.\n\n\nSum Rule. If \\(f(x) = g(x) + h(x)\\), where \\(g(x)\\) and \\(h(x)\\) are differentiable functions, then the derivative of \\(f\\) with respect to \\(x\\) is the sum of the derivatives of the individual functions:\n\\[\nf'(x) = g'(x) + h'(x).\n\\]\nThis rule allows us to differentiate each term of a sum separately and then add the results.\n\n\nChain Rule. If a function is composed as \\(f(x) = g(h(x))\\), where \\(h(x)\\) is the inner function and \\(g(u)\\) is the outer function (with \\(u = h(x)\\)), then the derivative of \\(f(x)\\) with respect to \\(x\\) is:\n\\[\nf'(x) = g'(h(x)) \\cdot h'(x).\n\\]\nDifferentiate the outer function with respect to the inner function, then multiply by the derivative of the inner function with respect to \\(x\\).\n\nTo find the minimizing weight, we differentiate \\(f(w_1)\\) with respect to \\(w_1\\).\nSince \\(f(w_1)\\) is a sum of two terms, the linearity of the derivative (sum rule) allows us to differentiate each term separately and then sum the results:\n\\[\nf(w_1) = (2 + 28 w_1)^2 + (10 + 28 w_1)^2\n\\]\nTo find the minimizing weight, we differentiate \\(f(w_1)\\) with respect to \\(w_1\\). Since \\(f(w_1)\\) is a sum of two terms, we can differentiate each term separately and then sum the results, according to the linearity of the derivative.\n\nLet‚Äôs begin with the first term: \\((2 + 28 w_1)^2\\). We recognize this as a composition of two functions, so we apply the chain rule. Let\n\\[\nh_1(w_1) = 2 + 28 w_1 \\quad \\text{(inner function)}, \\quad g_1(u) = u^2 \\quad \\text{(outer function)}.\n\\]\nNow we compute the derivatives:\n\\[\n\\frac{\\mathrm{d} g_1}{\\mathrm{d} u} = 2 u, \\quad \\frac{\\mathrm{d} h_1}{\\mathrm{d} w_1} = 28.\n\\]\nThe first result comes from the power rule applied to the outer function, and the second comes from differentiating the linear inner function. Applying the chain rule, we obtain\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d} w_1} (2 + 28 w_1)^2 = \\frac{\\mathrm{d} g_1}{\\mathrm{d} u} \\bigg|_{u = h_1(w_1)} \\cdot \\frac{\\mathrm{d} h_1}{\\mathrm{d} w_1} = 2 (2 + 28 w_1) \\cdot 28.\n\\]\n\nNext, consider the second term: \\((10 + 28 w_1)^2\\). Similarly, let\n\\[\nh_2(w_1) = 10 + 28 w_1, \\quad g_2(v) = v^2.\n\\]\nThe derivatives are\n\\[\n\\frac{\\mathrm{d} g_2}{\\mathrm{d} v} = 2 v, \\quad \\frac{\\mathrm{d} h_2}{\\mathrm{d} w_1} = 28.\n\\]\nAgain, these results come from the power rule for the quadratic outside function and the linear term inside the parentheses. By the chain rule, the derivative of the second term is\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d} w_1} (10 + 28 w_1)^2 = 2 (10 + 28 w_1) \\cdot 28.\n\\]\nCombining the two terms by the sum rule, the derivative of the full objective function is\n\\[\n\\frac{\\mathrm{d} f}{\\mathrm{d} w_1} = 2 (2 + 28 w_1) \\cdot 28 + 2 (10 + 28 w_1) \\cdot 28.\n\\]\n\nNow we have our first order conditions. We start by setting the derivative to zero\n\\[\n2 (2 + 28 w_1) \\cdot 28 + 2 (10 + 28 w_1) \\cdot 28 = 0.\n\\]\nWe can factor out \\(2 \\cdot 28\\):\n\\[\n2 \\cdot 28 \\left[ (2 + 28 w_1) + (10 + 28 w_1) \\right] = 0.\n\\]\nSimplifying inside the brackets gives\n\\[\n(2 + 28 w_1) + (10 + 28 w_1) = 12 + 56 w_1,\n\\]\nso the equation becomes\n\\[\n2 \\cdot 28 \\cdot (12 + 56 w_1) = 0.\n\\]\nSince \\(2 \\cdot 28 \\neq 0\\), we can safely divide both sides of the equation by this constant factor 56, isolating the parentheses that contains our variable:\n\\[\n12 + 56 w_1 = 0.\n\\]\nWe see that we can simplify this with the greatest common factor. Dividing both terms by 4 yields\n\\[\n\\frac{12}{4} + \\frac{56}{4} w_1 = 0 \\quad \\implies \\quad 3 + 14 w_1 = 0.\n\\]\nFinally, solving for \\(w_1\\), we obtain\n\\[\n14 w_1 = -3\n\\]\nand finally\n\\[\nw_1 = -\\frac{3}{14}.\n\\]\nThis is the unconstrained minimizer of \\(f(w_1)\\). However, it lies outside the feasible interval \\([0,1]\\) for the weights. The constrained minimum is therefore at the nearest boundary of the feasible set, which is \\(w_1 = 0\\). Correspondingly, \\(w_2 = 1\\). Evaluating the objective at the boundary points:\n\\[\nf(0) = 2^2 + 10^2 = 104, \\quad f(1) = 30^2 + 38^2 = 2344.\n\\]\nThe minimum occurs at \\(w_1 = 0, w_2 = 1\\), giving the synthetic control\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{y}_2 = \\begin{bmatrix} 48 \\\\ 50 \\end{bmatrix}.\n\\]\n\nThe reason I chose a corner solution is because I wanted to think about the circumstnaces uner which the model will not work. Solving for the optimal and yet non-ideal answer provides interesting insights as to how we can think about applyinng SCM in practice, as I will show below."
  },
  {
    "objectID": "lfe1.html#kkt-derivation",
    "href": "lfe1.html#kkt-derivation",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "KKT Derivation",
    "text": "KKT Derivation\nThe Karush-Kuhn-Tucker method formalizes what the calculus argument already suggested: because the unconstrained minimizer gives a negative weight, the constrained solution must sit on the boundary of the feasible set. KKT is useful here because it provides a systematic way to check all the optimality conditions at once‚Äîstationarity, feasibility, dual feasibility, and complementary slackness. This guarantees that the solution we find is not just locally valid but globally optimal for a convex quadratic problem. In this example, KKT confirms that the active constraint is \\(w_1 \\ge 0\\), leading to the boundary solution \\(w^\\ast = (0,1)\\).\n\n\nVerified With KKT\n\nTo form the KKT system, introduce a scalar Lagrange multiplier \\(\\lambda\\) for the equality constraint \\(w_1 + w_2 = 1\\) and nonnegative multipliers \\(\\mu_1, \\mu_2 \\ge 0\\) for the inequality constraints \\(w_1 \\ge 0\\), \\(w_2 \\ge 0\\). The Lagrangian is\n\\[\n\\mathcal{L}(\\mathbf{w}, \\lambda, \\mu) = \\|\\mathbf{y} - \\mathbf{Y}\\mathbf{w}\\|_2^2 + \\lambda (w_1 + w_2 - 1) - \\mu_1 w_1 - \\mu_2 w_2.\n\\]\nThe negative sign in the terms \\(-\\mu_i w_i\\) is a sign convention chosen so that the multipliers \\(\\mu_i\\) are constrained to be nonnegative; this makes the statement of dual feasibility and complementary slackness straightforward. Stationarity means the gradient of the Lagrangian with respect to the primal variables vanishes. Differentiating the quadratic objective gives¬†\n\\[\n\\nabla_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{Y}\\mathbf{w}\\|_2^2 = 2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w} - \\mathbf{y}),\n\\]¬†\nso stationarity requires\n\\[\n2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{1} - \\mu = \\mathbf{0},\n\\]\nwhere \\(\\mathbf{1} = (1,1)^\\top\\) and \\(\\mu = (\\mu_1, \\mu_2)^\\top\\). Primal feasibility is the requirement that the candidate weight vector satisfy the original constraints \\(w_1 \\ge 0\\), \\(w_2 \\ge 0\\), \\(w_1 + w_2 = 1\\). Dual feasibility means the multipliers associated with inequality constraints obey \\(\\mu_1 \\ge 0\\), \\(\\mu_2 \\ge 0\\). Complementary slackness ties the primal and dual together: for each inequality constraint, the product of the multiplier and the primal slack is zero, so \\(\\mu_i w_i = 0\\) for \\(i = 1, 2\\). In words, complementary slackness says that if a constraint is active (the primal variable is on the boundary) its multiplier may be positive, and if the constraint is slack (strictly satisfied) the corresponding multiplier must be zero.\nFrom calculus, we expect the constrained minimizer to be \\(\\mathbf{w}^{\\ast} = (0,1)^\\top\\), which corresponds to taking donor two alone as the synthetic control. Verifying this candidate requires computing the residual at that point and solving the stationarity equations for the multipliers. At \\(\\mathbf{w}^{\\ast}\\) we have \\(\\mathbf{Y}\\mathbf{w}^{\\ast} = \\mathbf{y}_2 = (48,50)^\\top\\) and the residual \\(\\mathbf{Y}\\mathbf{w}^{\\ast} - \\mathbf{y} = (-2,-10)^\\top\\). Multiplying by \\(2 \\mathbf{Y}^\\top\\) yields\n\\[\n2 \\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w}^{\\ast} - \\mathbf{y}) = 2 \\begin{bmatrix}20 & 22 \\\\ 48 & 50\\end{bmatrix} \\begin{bmatrix}-2 \\\\ -10\\end{bmatrix} = 2 \\begin{bmatrix}-260 \\\\ -596\\end{bmatrix} = \\begin{bmatrix}-520 \\\\ -1192\\end{bmatrix}.\n\\]\nPlugging this into the stationarity condition gives the two scalar equations\n\\[\n-520 + \\lambda - \\mu_1 = 0, \\qquad -1192 + \\lambda - \\mu_2 = 0.\n\\]\nComplementary slackness applied to the candidate \\(w_1^{\\ast} = 0, w_2^{\\ast} = 1\\) implies \\(\\mu_2 = 0\\) because \\(w_2^{\\ast} &gt; 0\\) forces the corresponding multiplier to vanish, while \\(\\mu_1\\) is unconstrained by slackness other than being nonnegative. Using \\(\\mu_2 = 0\\) in the second scalar equation yields \\(\\lambda = 1192\\). Substituting \\(\\lambda = 1192\\) into the first equation gives \\(\\mu_1 = 672\\), which satisfies dual feasibility since \\(672 \\ge 0\\). Thus stationarity, primal feasibility, dual feasibility, and complementary slackness all hold for the candidate, and because the objective is convex (its Hessian is \\(2 \\mathbf{Y}^\\top \\mathbf{Y}\\), which is positive semidefinite) these KKT conditions guarantee global optimality. Therefore \\(\\mathbf{w}^{\\ast} = (0,1)^\\top\\) is the solution, the residual is \\(\\mathbf{y} - \\hat{\\mathbf{y}} = (2,10)^\\top\\), the Lagrange multiplier for the equality constraint is \\(\\lambda = 1192\\), and the dual variables are \\(\\mu_1 = 672\\), \\(\\mu_2 = 0\\).\n\nTo summarize in words: we formed the Lagrangian to incorporate constraints into the objective, set its gradient with respect to the primal variables to zero (stationarity), required the primal variables to satisfy the original constraints (primal feasibility), required inequality multipliers to be nonnegative (dual feasibility), and enforced that no multiplier is positive while its corresponding primal variable is strictly positive (complementary slackness). Solving these conditions gave the boundary solution \\(w_1 = 0, w_2 = 1\\), which is the projection of \\(\\mathbf{y}\\) onto the convex hull of the donors when the unconstrained projection lies outside that hull. The KKT verification thus formalizes what we already knew: the now extreme your target unit is, the more your SCM will match to the nearest neighbor(s) in the donor pool."
  },
  {
    "objectID": "lfe1.html#frankwolfe-geometric-solution-and-hilbert-projection",
    "href": "lfe1.html#frankwolfe-geometric-solution-and-hilbert-projection",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "Frank‚ÄìWolfe Geometric Solution and Hilbert Projection",
    "text": "Frank‚ÄìWolfe Geometric Solution and Hilbert Projection\nFor a final expression, we can also arrive at the solution via pure geometry.\n\n\nSolved Via Frank-Wolfe\n\nWe seek to approximate\n\\[\n\\mathbf{y} = \\begin{bmatrix}50\\\\60\\end{bmatrix}\n\\]\nusing a convex combination of donors\n\\[\n\\mathbf{y}_1 = \\begin{bmatrix}20\\\\22\\end{bmatrix}, \\quad\n\\mathbf{y}_2 = \\begin{bmatrix}48\\\\50\\end{bmatrix}.\n\\]\nThe feasible set of synthetic predictions is the convex hull\n\\[\nC = \\operatorname{conv}\\{\\mathbf{y}_1, \\mathbf{y}_2\\},\n\\]\nwhich is the line segment connecting the two donors. Minimizing the squared Euclidean distance\n\\[\nf(\\hat{\\mathbf{y}}) = \\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|_2^2\n\\]\nis equivalent to projecting \\(\\mathbf{y}\\) onto this convex hull.\nTo give ourselves a frameowrk to work with we can employ the Hilbert Projection Theorem. This states that if \\(C \\subset \\mathbb{R}^n\\) is a nonempty, closed, convex set and \\(\\mathbf{y} \\in \\mathbb{R}^n\\), then there exists a unique point \\(\\hat{\\mathbf{y}} \\in C\\) such that\n\\[\n\\hat{\\mathbf{y}} = \\underset{\\mathbf{z} \\in C}{\\operatorname{argmin}} \\|\\mathbf{y} - \\mathbf{z}\\|_2,\n\\]\nand the residual \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}\\) satisfies the variational inequality\n\\[\n\\langle \\mathbf{r}, \\mathbf{z} - \\hat{\\mathbf{y}} \\rangle \\le 0 \\quad \\forall \\mathbf{z} \\in C.\n\\]\nThis inequality expresses the geometric fact that the residual forms an obtuse or right angle with any vector pointing from the projection to a feasible point in \\(C\\); in other words, one cannot move from the projection along any direction in \\(C\\) and reduce the distance to \\(\\mathbf{y}\\).\nApplying this to our two-donor synthetic control problem, the convex hull is the line segment connecting \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\), and the point \\(\\mathbf{y}\\) lies outside this segment. Computing the Euclidean distances yields \\(\\|\\mathbf{y} - \\mathbf{y}_1\\|^2 = 2344\\) and \\(\\|\\mathbf{y} - \\mathbf{y}_2\\|^2 = 104\\), so the closest donor is \\(\\mathbf{y}_2\\), which we take as the initial point \\(x^{(0)}\\) in a Frank‚ÄìWolfe iteration with corresponding weights \\(w^{(0)} = (0,1)\\). The gradient of the objective at \\(x^{(0)}\\) is \\(\\nabla f(x^{(0)}) = x^{(0)} - \\mathbf{y} = \\begin{bmatrix}-2\\\\-10\\end{bmatrix}\\). The Frank‚ÄìWolfe algorithm then searches for a vertex \\(s \\in \\{\\mathbf{y}_1, \\mathbf{y}_2\\}\\) minimizing the inner product \\(\\langle \\nabla f(x^{(0)}), s \\rangle\\). Computing \\(\\langle \\nabla f(x^{(0)}), \\mathbf{y}_1 \\rangle = -260\\) and \\(\\langle \\nabla f(x^{(0)}), \\mathbf{y}_2 \\rangle = -596\\) shows that the minimum occurs at \\(\\mathbf{y}_2\\), meaning the search direction points toward the vertex we are already at. Attempting to move toward \\(\\mathbf{y}_1\\) along the direction \\(d = \\mathbf{y}_1 - x^{(0)} = \\begin{bmatrix}-28\\\\-28\\end{bmatrix}\\) gives an optimal unconstrained step\n\\[\n\\gamma^{\\ast} = \\frac{\\langle \\mathbf{y}-x^{(0)}, d \\rangle}{\\|d\\|^2} = -0.214,\n\\]\nwhich lies outside the feasible range \\([0,1]\\). Clamping to \\(\\gamma = 0\\) results in the update \\(x^{(1)} = x^{(0)} = \\mathbf{y}_2\\), confirming that the closest feasible point is the boundary vertex \\(\\mathbf{y}_2\\). The residual is therefore \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\begin{bmatrix}2\\\\10\\end{bmatrix}\\), which satisfies the Hilbert variational inequality because it forms an obtuse angle with the direction toward \\(\\mathbf{y}_1\\).\nThis geometric reasoning mirrors the calculus derivation. In the unconstrained formulation, the minimizer of\n\\[\nf(w_1) = \\|\\mathbf{y} - (w_1 \\mathbf{y}_1 + (1-w_1) \\mathbf{y}_2)\\|^2\n\\]\noccurs at \\(w_1 = -3/14\\), which is outside the feasible interval \\([0,1]\\). By the Hilbert Projection Theorem, the optimal feasible point must lie on the nearest boundary, which corresponds to \\(w_1 = 0, w_2 = 1\\). The gradient computation in calculus signals that the slope of the objective points toward the infeasible region, exactly reflecting the fact that the residual cannot be reduced by moving along the feasible set.\nThe KKT derivation provides an algebraic realization of the same geometric principle. The stationarity condition\n\\[\n2\\mathbf{Y}^\\top (\\mathbf{Y}\\mathbf{w}-\\mathbf{y}) + \\lambda \\mathbf{1} - \\mu = 0,\n\\]\ntogether with complementary slackness \\(\\mu_i w_i = 0\\) and dual feasibility \\(\\mu_i \\ge 0\\), ensures that the residual \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}\\) is orthogonal to any feasible direction in the convex hull, encoding precisely the variational inequality from the Hilbert Projection Theorem. Active inequality constraints correspond to nonzero multipliers, signaling that the projection lies on the boundary, while slack constraints have zero multipliers, corresponding to interior directions where the unconstrained minimum is feasible. In this example, the boundary solution \\(w_1 = 0, w_2 = 1\\) with residual \\(\\mathbf{r} = (2,10)^\\top\\) and Lagrange multipliers \\(\\lambda = 1192, \\mu_1 = 672, \\mu_2 = 0\\) satisfies all KKT conditions and simultaneously fulfills the Hilbert projection criterion.\n\nThe Frank‚ÄìWolfe geometric approach, the calculus, and the KKT system are three perspectives on the same underlying fact: the optimal synthetic control for the standard method is the projection of \\(\\mathbf{y}\\) onto the convex hull of donors. When the unconstrained minimizer lies outside the hull, the solution is constrained to the nearest boundary point, with the residual aligned according to the Hilbert Projection Theorem."
  },
  {
    "objectID": "lfe1.html#a-visuzalization-in-python",
    "href": "lfe1.html#a-visuzalization-in-python",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "A Visuzalization in Python",
    "text": "A Visuzalization in Python\nAgain, none of this is voodoo, just a lot of hairy math.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\n\n# =====================\n# Step 0: Setup\n# =====================\n# Treated unit\nx = np.array([50, 60])\n\n# Donors\nd1 = np.array([20, 22])\nd2 = np.array([48, 50])\ndonors = np.vstack([d1, d2])\ndonors_cvx = np.column_stack([d1, d2])\nn_donors = donors_cvx.shape[1]\n\nw = cp.Variable(n_donors, nonneg=True)\n\nobjective = cp.Minimize(cp.sum_squares(x - donors_cvx @ w))\n\nconstraints = [cp.sum(w) == 1]\nprob = cp.Problem(objective, constraints)\nprob.solve()\n\nx_hat_cvx = donors_cvx @ w.value\n\n# Time steps\nt = np.arange(len(x))\n\n# =====================\n# Plot as subfigures\n# =====================\nfig, axs = plt.subplots(1, 2, figsize=(12,5))\n\n# --- Subplot 1: 2D feature plot ---\naxs[0].plot(d1[0], d1[1], 'bo', label='Donor 1')\naxs[0].plot(d2[0], d2[1], 'go', label='Donor 2')\naxs[0].plot(x[0], x[1], 'r*', markersize=12, label='Treated Unit')\naxs[0].plot(x_hat_cvx[0], x_hat_cvx[1], 'ms', markersize=10, label='Synthetic Control')\n\n# Convex hull as a dashed line\naxs[0].plot([d1[0], d2[0]], [d1[1], d2[1]], 'k-.', label='Convex Hull')\n\n# Add literal rectangle around convex hull\nx_min, x_max = np.min(donors[:,0]), np.max(donors[:,0])\ny_min, y_max = np.min(donors[:,1]), np.max(donors[:,1])\nrect = plt.Rectangle((x_min, y_min), x_max-x_min, y_max-y_min,\n                     linewidth=1, edgecolor='gray', facecolor='none', linestyle='--', label='Hull Bounding Box')\naxs[0].add_patch(rect)\n\naxs[0].set_xlabel('Feature 1')\naxs[0].set_ylabel('Feature 2')\naxs[0].legend()\naxs[0].grid(True)\naxs[0].set_title('2D Feature Space')\n\n# --- Subplot 2: Time series plot ---\naxs[1].plot(t, x, 'r*-', label='Treated Unit', markersize=10)\naxs[1].plot(t, d1, 'bo-', label='Donor 1')\naxs[1].plot(t, d2, 'go-', label='Donor 2')\naxs[1].plot(t, x_hat_cvx, 'ms-', label='Synthetic Control', markersize=8)\n\n# Convex hull shading\ndonor_min = np.min(donors, axis=0)\ndonor_max = np.max(donors, axis=0)\naxs[1].fill_between(t, donor_min, donor_max, color='gray', alpha=0.2, label='Donor Convex Hull')\n\naxs[1].set_xlabel('Time')\naxs[1].set_ylabel('Outcome')\naxs[1].legend()\naxs[1].grid(True)\naxs[1].set_title('Time Series with Convex Hull Highlight')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also show how the error changes, given a change in \\(w_1\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Treated unit\ny = np.array([50, 60])\n\n# Donors\ny1 = np.array([20, 22])\ny2 = np.array([48, 50])\n\n# Compute synthetic control at the constrained minimum (w1=0, w2=1)\nw1_min = 0\nw2_min = 1\ny_hat_min = w1_min*y1 + w2_min*y2\n\n# Compute residual\nresidual = y - y_hat_min\n\n# Generate a range of w1 values for objective\nw1_vals = np.linspace(0, 1, 200)\nw2_vals = 1 - w1_vals\nf_vals = np.array([np.sum((y - (w1*y1 + w2*y2))**2) for w1, w2 in zip(w1_vals, w2_vals)])\n\n# Plot\nfig, axs = plt.subplots(1, 2, figsize=(12,5))\n\n# --- Subplot 1: 2D feature space with residual ---\naxs[0].plot(y1[0], y1[1], 'bo', label='Donor 1')\naxs[0].plot(y2[0], y2[1], 'go', label='Donor 2')\naxs[0].plot(y[0], y[1], 'r*', markersize=12, label='Treated Unit')\naxs[0].plot(y_hat_min[0], y_hat_min[1], 'ms', markersize=10, label='Synthetic Control')\n\n# Convex hull line\naxs[0].plot([y1[0], y2[0]], [y1[1], y2[1]], 'k-.', label='Convex Hull')\n\n# Draw residual vector from synthetic control to treated unit\naxs[0].arrow(y_hat_min[0], y_hat_min[1], residual[0], residual[1],\n             head_width=0.8, head_length=1.0, fc='orange', ec='orange', linewidth=2, label='Residual')\n\naxs[0].set_xlabel('Feature 1')\naxs[0].set_ylabel('Feature 2')\naxs[0].legend()\naxs[0].grid(True)\naxs[0].set_title('2D Feature Space with Residual')\n\n# --- Subplot 2: Objective function f(w1) ---\naxs[1].plot(w1_vals, f_vals, 'b-', label=r'$f(w_1) = ||y - \\hat{y}||^2$')\naxs[1].plot(w1_min, f_vals[0], 'ro', label='Minimum (constrained)')\naxs[1].axvline(x=w1_min, color='r', linestyle='--')\naxs[1].set_xlabel(r'$w_1$')\naxs[1].set_ylabel('Objective value')\naxs[1].legend()\naxs[1].grid(True)\naxs[1].set_title('Objective Function vs. $w_1$')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom this figure, we see what we already solved for. Any weight for \\(w_1\\) greater than 0 increases our error solution for our objective function.\nAlready, some of you who got this far will be asking ‚ÄúJared, why did you go through the trouble of showing us all this? Who cares about the KKT conditions or the Hilbert Space Projection or any of those ideas? I‚Äôll never literally do this at work will I, so why bother showing me any of this?‚Äù And my answer is ‚ÄúThat is correct, you will not need to do any of this yourself, ever. However, the moment you intuit SCM in these terms, you can oftentimes get a good read as to how an analysis will look before you even run a single SCM model.‚Äù Not always, but sometimes. I have worked in industry as consultant for 5 months now, and literally, many problems that people have with the weights or the predictions etc. would literally (without exaggeration) be solved by simple line ploy. Seriously, you‚Äôll discover a lot by just plotting your units out. We‚Äôre about to see why this is true in the content below."
  },
  {
    "objectID": "lfe1.html#the-two-step-synthetic-control",
    "href": "lfe1.html#the-two-step-synthetic-control",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "The Two-Step Synthetic Control",
    "text": "The Two-Step Synthetic Control\nI thought to myself ‚ÄúYeah this is definitely gonna need an intercept of some kind!!! Couldn‚Äôt fit the treated vector otherwise.‚Äù So, I ran the Two-Step SCM. The TSSC method was developed in part by one of my favorite econometricians, Kathy Li. The method is predicated on a simple idea: Sometimes treated units are simply too extreme relative to their donor pool, such as Australia. When this happens, vanilla SCM can struggle, and analysts may need to allow for intercepts or relax the non-negativity constraint. The challenge, however, is that it‚Äôs rarely obvious ex ante which modification is appropriate.\nTSSC chooses between a few candidate estimators. First is MSCa: Intercept, Convex Hull\n\\[\n\\underset{\\mathbf{w},\\,\\beta}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} - \\beta \\mathbf{1} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0, \\; \\mathbf{1}^\\top \\mathbf{w} = 1, \\; \\beta \\in \\mathbb{R}.\n\\]\nThis is SCM with an intercept, which shifts the treated unit vertically inside the convex hull. We also have MSCb: No Intercept, Nonnegative Weights\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0.\n\\]\nThis drops the intercept but allows weights to be any nonnegative values (not required to sum to one). Geometrically, the treated unit is projected onto the convex cone generated by the donors. Finally, we have MSCc: Intercept + Nonnegative Weights\n\\[\n\\underset{\\mathbf{w},\\,\\beta}{\\text{argmin}} \\;\\; \\| \\mathbf{y}_1 - \\mathbf{Y}\\mathbf{w} - \\beta \\mathbf{1} \\|_2^2\n\\quad \\text{s.t.} \\;\\; \\mathbf{w} \\geq 0.\n\\]\nThis combines both relaxations: an intercept plus nonnegative weights. The treated unit is projected onto a shifted convex cone, which is especially useful when the treated unit differs systematically in slope or level from the donor pool.\nTSSC begins by testing the joint null hypothesis that both restrictions are valid:\n\\[\nH_0: \\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\quad \\text{and} \\quad \\beta = 0\n\\]\nagainst the alternative that at least one fails. If the null is rejected, each restriction is tested separately, yielding up to three possible modifications (MSCa, MSCb, or MSCc).\nThe test statistic compares the restricted SCM solution against the most flexible specification, MSCc. Let \\((\\hat{\\mathbf{w}}^{SC}, \\hat{\\beta}^{SC})\\) be the restricted solution and \\((\\hat{\\mathbf{w}}^{MSCc}, \\hat{\\beta}^{MSCc})\\) the flexible one. The statistic takes the form\n\\[\nT = (\\hat{\\theta}^{SC} - \\hat{\\theta}^{MSCc})^\\top \\, \\hat{\\Sigma}^{-1} \\, (\\hat{\\theta}^{SC} - \\hat{\\theta}^{MSCc}),\n\\]\nwhere \\(\\hat{\\theta}\\) stacks the estimated weights and intercept, and \\(\\hat{\\Sigma}\\) is a covariance estimate obtained by subsampling the pre-treatment period. Intuitively, \\(T\\) measures how far the SCM solution lies from the flexible cone-based solution, scaled by sampling variability. If \\(T\\) is small, the convex-hull assumption holds; if large, the data support a more flexible geometry."
  },
  {
    "objectID": "lfe1.html#connecting-the-theory-to-practice",
    "href": "lfe1.html#connecting-the-theory-to-practice",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "Connecting the Theory to Practice",
    "text": "Connecting the Theory to Practice\nGeometrically, adding an intercept in TSSC shifts the treated vector vertically into the interior of the convex hull formed by the donor pool. In Hilbert space terms, this expands the feasible set for the projection: rather than projecting a point outside or on the boundary, the algorithm now projects a point inside the convex hull. This allows the synthetic control weights to spread across multiple donors rather than collapsing to a corner solution. From the KKT perspective, the intercept introduces an additional degree of freedom, modifying the constraints but still satisfying stationarity, dual feasibility, and complementary slackness on the adjusted feasible set. Consequently, TSSC can produce a solution where multiple donors receive meaningful weight, resolving the boundary limitations of standard SCM."
  },
  {
    "objectID": "lfe1.html#results",
    "href": "lfe1.html#results",
    "title": "Lessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes",
    "section": "Results",
    "text": "Results\nHere is the result we get when we fit the TSSC estimator to the Australia case. As usual, to follow along you‚Äôll need to do\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\n\nTSSC_res = TSSC(config).fit()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# SCM / TSSC results\nresults_df = pd.DataFrame({\n    \"Method\": [\"SCM\", \"MSCc\"],\n    \"ATT\": [31.049, -30.713],\n    \"Intercept\": [0.000, 62.872],\n    \"Key Weights\": [\n        \"Italy: 0.815, Norway: 0.185\",\n        \"Norway: 0.089, South Africa: 0.288\"\n    ]\n})\n\n# Convert to Markdown table\nmd_table = results_df.to_markdown(index=False).split('\\n')\n\n# Adjust alignment: left for Method/Key Weights, center for numbers\nmd_table[1] = \"|:------------|:------:|:--------:|:---------------------------|\"\n\n# Display nicely in notebook\ndisplay(Markdown('\\n'.join(md_table)))\n\n\n\n\n\n\n\n\n\n\nMethod\nATT\nIntercept\nKey Weights\n\n\n\n\nSCM\n31.049\n0\nItaly: 0.815, Norway: 0.185\n\n\nMSCc\n-30.713\n62.872\nNorway: 0.089, South Africa: 0.288\n\n\n\n\n\nThe authors ATT is 31.049. The ATT of the MSCc is -30.713, almost the opposite of the original finding, with the intercept coefficient being 62.872 and the donors being Norway: 0.089, South Africa: 0.288. We can see here that by adding an intercept which accounts for systemic baseline differences between the target unit and the control group, we can sometimes even flip the sign of the estimated ATT completely. This demonstrates why theoretical understanding is crucial: by simple inspection of donor units versus the treated units, we can sometimes intuit how the baseline model will look before we‚Äôve ran it, potentially anticipating the kind of SCM model we need. In this case just by looking, we already know that the donors will mainly be the nearest neighbors, suggesting that the model needs an intercept. Note that the sign reversal is not always true. Consider the Basque example:\n\nimport pandas as pd\nfrom mlsynth import TSSC\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv\"\ndata = pd.read_csv(url)\n\nconfig = {\n    \"df\": data,\n    \"outcome\": data.columns[2],\n    \"treat\": data.columns[-1],\n    \"unitid\": data.columns[0],\n    \"time\": data.columns[1],\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\"]}\n\narco = TSSC(config).fit()\n\n\n\n\n\n\n\n\nHere our ATT is -0.8, whereas the original SCM‚Äôs was -0.69. With an intercept of 0.583, MSCc assigns weight to Cataluna: 0.533, Madrid (Comunidad De): 0.067, Principado De Asturias: 0.115, and Rioja (La): 0.289. In contrast, Cataluna‚Äôs weight at first was 0.826, Madrid‚Äôs was 0.168, and Austurias was 0.005 (these differ a little from the 2003 paper because I do not employ covariates)."
  },
  {
    "objectID": "sparsdens.html",
    "href": "sparsdens.html",
    "title": "What is a Synthetic Control?",
    "section": "",
    "text": "Introduction\nMarketers and econometricians stress the importance of location selection for geo-testing/program evaluation for the effect of advertising interventions. Inherently speaking, the control locations we use directly impacts the quality of our counterfactual predictions, and therefore the ATT, iROAS, or whatever metric we claim to care about. The reality is, you can‚Äôt simply take an average of randomly selected control regions and compare them to the places you‚Äôre increasing ad runs in. Therefore, researchers require systematic control group selection methods for the panel data setting where the randomzied controlled trial is not possible. Of course, one of these methods that‚Äôs popular is the synthetic control method. But if you‚Äôre reading this, you likely already know this. However, we need to take a step back. We need to ask ourselves more fundamentally what we think a synthetic control is as a concept. This happens in academia or industry all the time. My coworkers will ask me questions like, ‚ÄúHey Jared, how do you view the whole constraints upon the weights for synthetic control methods? Why do we care, if at all, that the weights should be non-negative or add up to anything, and are there any rules regarding these ideas?‚Äù Usually people want to make some custom extension to the baseline algorithm, and want to know if they‚Äôre breaking some seemingly sacrosanct rules. My answer is usually some variant of: ‚ÄúWell, it depends on what you think a synthetic control is.‚Äù As it turns out, this is actually a non-trivial philosophical question that has no true answer.\nClassically, synthetic control weights are viewed as a sparse vector, and plenty of academic work has developed synthetic control methodologies from this perspective. The classic setting focuses on sparsity for the unit weights. The point of sparsity, as others (including me) argue is for the resulting positive donors to be interpretable and economically similar to the units of interest to the treated unit. More precisely, the goal is for similarity on latent factors that we cannot observe. Much work is dedicated to producing such descriptions of the SCM. Others argue however for a completely different set of standards. The main contention seems to be that while sparsity offers interpretability and simplicity, it may not always be practical for capturing the complex relationships in real economic or business phenomena. Instead, they advocate for dense coefficient vectors that distribute weights more diffusely across donors, potentially improving predictive performance.\nIn my opinion, the general form of a synthetic control problem is some objective function where we weight donor units to best approximate the treated (set of) unit(s), with the choice of constraints reflecting both the econometric goals and domain-specific beliefs. The most general expression for this is\n\\[\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\mathcal{L}\\big( \\mathbf{y}_1, \\mathbf{Y}_0 \\mathbf{w} \\big) + \\lambda \\cdot \\mathcal{R}(\\mathbf{w}) \\quad \\text{subject to} \\quad & \\mathbf{w} \\in \\mathcal{W}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{y}_1\\) is the vector of observed outcomes for the treated unit(s) during the pre-treatment period, \\(\\mathbf{Y}_0\\) is the matrix of outcomes for the donor pool units over the same period, and \\(\\mathbf{w}\\) is the vector of weights assigned to those donor units. The function \\(\\mathcal{L}(\\cdot, \\cdot)\\) represents the loss function measuring the discrepancy between the treated outcomes and the weighted combination of donors, commonly the squared error loss. The term \\(\\mathcal{R}(\\mathbf{w})\\) is a regularization function that imposes additional structure or penalties on the weights to reflect beliefs or promote certain properties like sparsity or smoothness. The scalar \\(\\lambda \\geq 0\\) controls the trade-off between fitting the data closely and respecting the regularization. Finally, the set \\(\\mathcal{W}\\) defines the feasible set of weights, encoding constraints such as non-negativity, sum-to-one, or other domain-specific restrictions. Notice that this setup is intended to be very very general. I have not yet taken a position on the nature of the weights or the specifics of the optimization problem.\nThis post shows that there is (often) no one right way to play. Many estimators may be used, and the key thing is the underlying econometric assumptions one needs to understand and make to use them effectively (sparsity vs density of course is not the only assumption of interest).\n\n\nApplication\nLet‚Äôs use an example. Consider this plot of a growth rate variable, where the outcome is the number of products purchased when some advertising campaigns went into effect. The goal of an incrementality study is to estimate how the growth of sales would have evolved absent the ads. After all, this is how we determine if the ad spend was effective or if it was wasted‚Äîby estimating how much revenue we would have generated without any advertising. In this dataset, we observe 276 control units across 176 pre-treatment time periods. The key problem of interest is that there are very many control units to choose from. We cannot simply use the average of controls as a counterfactual estimate, as this presumes that the mean of this entire control group is similar enough (in both level and trend) to the treated unit of interest. As the figure suggests though, this is likely not true. While the growth rate does indeed remove seasonality elements, there are still trend, periodic, and perhaps cyclical differences to account for as well. Given this, it‚Äôs likely the case that some reweighted average of these controls will mimic the treated unit much better than the pure average of the control units.\n\n\n\n\n\n\n\n\n\nTo this end, I fit a festival of models, some sparse, some sense. I fit the forward DID model (sparse), Forward SCM model (also sparse) and the robust PCA SCM model (sparse). For the dense models I estimate the \\(\\ell_2\\) panel data approach and the robust synthetic control (all these are documented here). I construct an ensemble of artificial counterfactual estimators by convexly combining the predictions of the base models. The goal is to produce a flexible estimator that balances the predictive accuracy of dense estimators with the interpretability and sparsity of sparse estimators. Let \\(M\\) denote the number of candidate models in the ensemble. For each model \\(m = 1, \\ldots, M\\), we obtain a predicted counterfactual series \\(\\widehat{Y}^{(m)}_{1,t}\\) for the treated unit in the pre-treatment period \\(t \\in \\mathcal{T}_0\\). We organize these into a matrix \\(\\widehat{\\mathbf{Y}}_0 \\in \\mathbb{R}^{T_0 \\times M}\\), where each column corresponds to one model‚Äôs predicted values over the pre-treatment period. Let \\(\\mathbf{Y}_{1,\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the observed outcomes for the treated unit during the same period. We solve the following convex optimization problem to learn a set of model weights \\(\\mathbf{w} \\in \\mathbb{R}^M\\):\n\\[\n\\min_{\\mathbf{w}} \\left\\| \\mathbf{Y}_{1,\\text{pre}} - \\widehat{\\mathbf{Y}}_0 \\mathbf{w} \\right\\|_2^2 + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\n\\quad \\text{subject to} \\quad \\mathbf{w} \\geq 0,\\quad \\sum_{m=1}^M w_m = 1.\n\\]\nThis is a ridge-penalized model averaging objective, constrained so that the weights are non-negative and sum to one. The penalty term \\(\\lambda \\left\\| \\mathbf{w} \\right\\|_2^2\\) discourages over-reliance on any single model and promotes stability in the ensemble. We select the regularization parameter \\(\\lambda\\) via \\(K\\)-fold cross-validation on the pre-treatment period, minimizing the out-of-sample prediction error. The resulting weights \\(\\widehat{\\mathbf{w}}\\) define the ensemble counterfactual:\n\\[\n\\widehat{Y}^{\\text{ens}}_{1,t} = \\sum_{m=1}^M \\widehat{w}_m \\widehat{Y}^{(m)}_{1,t}, \\quad \\text{for all } t.\n\\]\nThis procedure allows flexible borrowing of information across model classes, combining the sparse structure of subset-selected synthetic controls with the improved fit of regularized or dense variants, depending on which performs better in-sample. The ensemble is constrained to lie in the convex hull of the candidate model predictions.\n\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nThere are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n\n\n\n\n\n\n\n\n\n\nThe results from the model averaging procedure reveal clear trade-offs between sparsity and predictive accuracy. In the sparse model average, the optimal penalty parameter is \\(\\lambda = 1.7074\\), leading to a pre-treatment RMSE of \\(0.5028\\). Within this specification, the FSCM model receives the most weight (\\(0.2085\\)), followed by FDID (\\(0.7915\\)), while RPCA is unused. In contrast, the dense model average achieves a substantially better fit (RMSE \\(= 0.3085\\)) with a much smaller penalty (\\(\\lambda = 0.0010\\)), allocating nearly all weight to PCR (\\(0.9966\\)) and very little to PDA (\\(0.0034\\)). The full model average, which includes both sparse and dense estimators, also selects \\(\\lambda = 0.0010\\) and reaches the same RMSE of \\(0.3085\\), but nearly all weight again falls on PCR (\\(0.9927\\)), with RPCA contributing marginally (\\(0.0073\\)) and all others excluded.\nThese results underscore a key econometric tension: while dense methods often achieve superior in-sample fit, they can obscure the role of individual donors and reduce interpretability. Sparse methods like FSCM and FDID provide clearer narratives but may underperform in terms of match quality. In settings where both approaches yield similar outcomes‚Äîas they do here‚Äîthe choice between them ultimately depends on the econometrician‚Äôs priorities. If transparency and donor interpretability are paramount, sparse models may be preferred. If minimizing pre-treatment error is the guiding objective, dense models may be more appropriate. In this sense, model selection becomes not just a statistical exercise, but a philosophical one as well.\n\n\nFinal Thoughts\nIn some ways, this also illustrates why I wrote mlsynth. I do not claim to have the best models or know all of the secrets to solve one‚Äôs modeling needs; rather, I program models that I think are useful, in certain cases. The point of the mlsynth library is to allow researchers to compare and contrast these different models together all in one singular grammar, without needing to be bogged down in different softwares and syntaxes. Cool stuff happens all the time with these methods, and the only way for them to be used, and used more often, is by providing researchers with a simple framework by which to generate these counterfactuals in settings they care about.\n\n\n\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Test Post\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nShake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is An Average?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scexp.html",
    "href": "scexp.html",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "",
    "text": "Suppose the government of Cura√ßao wants to implement a new ‚ÄúGreen Stay‚Äù initiative that encourages hotels to adopt sustainability measures, such as reducing water usage, improving waste management, and shifting toward renewable energy. Such initiatives are becoming increasingly popular in recent years. But evaluating whether these policies actually work presents a fundamental challenge for market researchers.\nFrom a methodological standpoint, the ideal approach would be a randomized controlled trial (RCT), in which some hotels implement the policy while others serve as controls. Randomization ensures that treated and control groups are balanced in expectation, so that any differences in outcomes after implementation can be attributed to the policy. RCTs provide a clear framework for statistical inference and are widely regarded as the gold standard for impact evaluation.\nIn practice, however, conducting an RCT in this context is fraught with challenges. Ethically, randomly granting some hotels the benefits of the program while denying them to others may be perceived as unfair, particularly if the policy enhances reputation, attracts eco-conscious travelers, or provides financial advantages. Politically, coordinating randomization across hundreds of hotels would require buy-in from government agencies, hotel associations, and local businesses, some of whom may resist being ‚Äúexperimented on.‚Äù Tourism being central to the island‚Äôs economy, any perception of risk could provoke public concern. Logistically, enforcing varying sustainability requirements across many independent hotels would be complex and costly, making large-scale randomization impractical.\nA more feasible alternative is to focus on the market at a cluster level, grouping neighborhoods that share similar characteristics, such as geographic location, customer demographics, or historical tourist activity. Clustering mitigates ethical concerns, because selection occurs within naturally similar groups rather than arbitrarily across the entire market. It also reduces logistical complexity: interventions can be coordinated at the cluster level, and within each cluster, neighborhoods chosen as treated or control reflect the cluster‚Äôs underlying dynamics rather than idiosyncratic behavior.\nThe question then becomes: how do we select which neighborhoods to treat and which to use as controls? We could throw darts at a map to decide, but this is unlikely to produce reliable estimates. Instead, the goal should be to select both a treated group of units and a control group of units which look like the cluster/group level aggregate. Synthetic control methods offer a solution, constructing ‚Äúsynthetic treated units‚Äù and ‚Äúsynthetic control units‚Äù that closely mimic the characteristics of clusters, enabling rigorous estimation of treatment effects even when randomization is infeasible."
  },
  {
    "objectID": "scexp.html#setup",
    "href": "scexp.html#setup",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Setup",
    "text": "Setup\nAs usual, this is where we get into the math details. In our Cura√ßao Green Stay example, the unit of treatment is the neighborhood, not individual hotels. That is, entire neighborhoods may adopt sustainability measures, while others remain untreated.\nLet \\[\n\\mathcal{J} = \\{1, \\dots, J\\}\n\\] denote the set of neighborhoods, nested within clusters \\[\n\\mathcal{K} = \\{1, \\dots, K\\},\n\\] which group neighborhoods with similar characteristics (e.g., location, tourist demographics, historical occupancy). For each cluster \\(k \\in \\mathcal{K}\\), let \\[\nI_k \\subseteq \\mathcal{J}\n\\] denote its member neighborhoods. Each neighborhood \\(j \\in \\mathcal{J}\\) belongs to exactly one cluster, denoted \\(k(j)\\), so that \\(j \\in I_{k(j)}\\).\nTime is divided into pre-treatment, blank, fitting, and post-treatment periods. Formally, we define\n\\[\n\\mathcal{T}_0 = \\{1, \\dots, T_0\\}, \\quad\n\\mathcal{T}_1 = \\{1, \\dots, T_0 - |\\mathcal{B}|\\}, \\quad\n\\mathcal{B} = \\{T_0 - |\\mathcal{B}| + 1, \\dots, T_0\\}, \\quad\n\\mathcal{T}_2 = \\{T_0 + 1, \\dots, T\\}.\n\\]\nHere \\(\\mathcal{T}_0\\) is the full pre-treatment period (128 observations in our example), \\(\\mathcal{T}_1\\) is the fitting period used to compute synthetic control weights and assignments, \\(\\mathcal{B}\\) is a blank period withheld for placebo checks (28 observations), and \\(\\mathcal{T}_2\\) is the post-treatment period.\nObserved outcomes, such as occupancy rates or energy use, are collected in the outcome matrix\n\\[\n\\mathbf{Y} \\in \\mathbb{R}^{J \\times T},\n\\]\nwhere \\(y_{jt}\\) represents the outcome for neighborhood \\(j \\in \\mathcal{J}\\) at time \\(t \\in \\mathcal{T}_0 \\cup \\mathcal{T}_2\\). Potential outcomes are written as \\(y_{jt}^I\\) if neighborhood \\(j\\) participates in Green Stay and \\(y_{jt}^N\\) if not, where \\(I\\) denotes treated and \\(N\\) denotes untreated.\nThe cluster-weighted average treatment effect at time \\(t &gt; T_0\\) is\n\\[\n\\tau_t = \\sum_{j=1}^J f_j \\,(y_{jt}^I - y_{jt}^N),\n\\]\nwhere \\(f_j \\ge 0\\) and \\(\\sum_{j=1}^J f_j = 1\\). These weights may represent, for example, the relative population or importance of neighborhoods.\nPre-treatment characteristics of neighborhoods are captured in predictor vectors \\[\n\\mathbf{x}_j \\in \\mathbb{R}^r,\n\\] such as average energy use, hotel occupancy, or guest demographics. Cluster means are defined using the same weights \\(f_j\\) as\n\\[\n\\bar{\\mathbf{x}}_k = \\frac{\\sum_{j \\in I_k} f_j \\, \\mathbf{x}_j}{\\sum_{j \\in I_k} f_j}.\n\\]\nSynthetic treated and control neighborhoods are defined via weights \\(w_j \\ge 0\\) and \\(v_j \\ge 0\\) respectively, with neighborhood-level treatment assignments \\(z_j \\in \\{0,1\\}\\). The clustered synthetic control estimator is obtained by solving\n\\[\n\\min_{(\\mathbf{w},\\mathbf{v}) \\in \\mathcal{F}} \\; \\mathcal{L}(\\mathbf{w},\\mathbf{v}),\n\\]\nwhere \\(\\mathcal{F}\\) is the feasible set defined by nonnegativity, within‚Äìcluster normalization, exclusivity, and any budget constraints. Each cluster \\(k \\in \\mathcal{K}\\) has member indices \\(I_k\\), cluster mean \\(\\bar{\\mathbf{x}}_k\\), and outcomes \\(\\mathbf{X}_{I_k}\\)."
  },
  {
    "objectID": "scexp.html#the-optimization",
    "href": "scexp.html#the-optimization",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "The Optimization",
    "text": "The Optimization\nThe optimization problem chooses synthetic treated and control neighborhoods by minimizing a loss function subject to feasibility constraints. At its core, the method constructs weighted averages of units to be treated and untreated, such that both averages resemble the cluster as a whole and each other. In addition to the above, we also may face operational costs \\(\\mathbf{c}\\) and cluster budgets \\(B_k\\) which constrain feasible assignments. Explicitly, the feasible set is\n\\[\n\\begin{aligned}\n\\mathcal{F} = \\Big\\{ (w,v) \\,\\Big| \\;\n& w_j, v_j \\ge 0, \\quad \\sum_{j \\in I_k} w_j = \\sum_{j \\in I_k} v_j = 1, \\\\\n& z_j \\in \\{0,1\\}, \\quad w_j \\le z_j, \\quad v_j \\le 1 - z_j, \\\\\n& \\sum_{j \\in I_k} z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}], \\\\\n& \\sum_{j \\in I_k} c_j w_j \\le B_k, \\quad \\sum_{k=1}^K \\sum_{j \\in I_k} c_j w_j \\le B_{\\text{total}}\n\\Big\\}.\n\\end{aligned}\n\\]\nNon-negativity (\\(w_j, v_j \\ge 0\\)) and normalization (\\(\\sum_{j \\in I_k} w_j = \\sum_{j \\in I_k} v_j = 1\\)) ensure synthetic neighborhoods are convex combinations of real neighborhoods. The decision variable \\(z_j \\in \\{0,1\\}\\), together with \\(w_j \\le z_j\\) and \\(v_j \\le 1 - z_j\\), enforces exclusivity: each neighborhood contributes to either the synthetic treated unit or the synthetic control unit, but not both. Cardinality constraints (\\(\\sum_{j \\in I_k} z_j \\in [m_{\\text{min},k}, m_{\\text{max},k}]\\)) ensure that at least one neighborhood is treated in each cluster. Budget constraints (\\(\\sum c_j w_j \\le B_k\\), \\(\\sum c_j w_j \\le B_{\\text{total}}\\)) incorporate heterogeneous treatment costs across neighborhoods."
  },
  {
    "objectID": "scexp.html#design-variations",
    "href": "scexp.html#design-variations",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Design Variations",
    "text": "Design Variations\nAbadie and Zhou propose several ways of implementing the synthetic control design. The base experimental SC estimator is the most straightforward. For each cluster \\(k\\), it tries to make the synthetic treated and synthetic control neighborhoods look like the cluster mean:\n\\[\n\\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v}) = \\sum_{k=1}^K \\big( \\mathbf{f}_{I_k}^\\top \\mathbf{1} \\big) \\Big[\n\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2\n+ \\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2\n\\Big].\n\\]\nThis design anchors both sides of the experiment to the same benchmark, namely the cluster mean.\nThe weakly targeted estimator extends this idea by also encouraging the treated and control groups to resemble each other directly:\n\\[\n\\mathcal{L}_{\\text{Weak}}(\\mathbf{w},\\mathbf{v}) = \\mathcal{L}_{\\text{Base}}(\\mathbf{w},\\mathbf{v})\n+ \\beta \\|\\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k} - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2.\n\\]\nIn practice, this prevents the optimization from drifting toward two synthetic groups that both match the cluster mean but diverge from each other. Here as \\(\\beta\\) increases, we tend to the ATT instead of the ATE.\nThe penalized estimator incorporates distance-based penalties that favor neighborhoods closer to the cluster mean:\n\\[\n\\begin{aligned}\n\\mathcal{L}_{\\text{Penalized}}(\\mathbf{w}, \\mathbf{v})\n&= \\sum_{k=1}^K \\Bigg[\n\\underbrace{\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2}_{\\text{fit treated to cluster mean}}\n+ \\underbrace{\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2}_{\\text{fit control to cluster mean}} \\\\\n&\\quad + \\lambda_1 \\underbrace{\\sum_{j \\in I_k} w_j \\|\\mathbf{X}_j - \\bar{\\mathbf{x}}_k\\|_2^2}_{\\text{treated distance penalty}}\n+ \\lambda_2 \\underbrace{\\sum_{j \\in I_k} v_j \\|\\mathbf{X}_j - \\bar{\\mathbf{x}}_k\\|_2^2}_{\\text{control distance penalty}}\n\\Bigg].\n\\end{aligned}\n\\]\nThis penalizes weights placed on neighborhoods far from the cluster mean, enforcing a kind of sparsity.\nFinally, the unit-level estimator introduces the most granular design. It not only enforces closeness to cluster means but also requires that each treated neighborhood, individually, has a close match with its synthetic control:\n\\[\n\\begin{aligned}\n\\mathcal{L}_{\\text{Unit}}(\\mathbf{w}, \\mathbf{v})\n&= \\sum_{k=1}^K \\Bigg[\n\\underbrace{\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2}_{\\text{treated fit to cluster mean}}\n+ \\underbrace{\\|\\bar{\\mathbf{x}}_k - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2}_{\\text{control fit to cluster mean}} \\\\\n&\\quad + \\underbrace{\\xi \\sum_{j \\in I_k} w_j \\|\\mathbf{X}_j - \\mathbf{X}_{I_k}^\\top \\mathbf{v}_{I_k}\\|_2^2}_{\\text{unit-level treated-control match}} \\\\\n&\\quad + \\underbrace{\\lambda_{1,\\text{unit}} \\sum_{j \\in I_k} w_j \\|\\mathbf{X}_j - \\bar{\\mathbf{x}}_k\\|_2^2}_{\\text{treated distance to cluster mean}}\n+ \\underbrace{\\lambda_{2,\\text{unit}} \\sum_{j \\in I_k} v_j \\|\\mathbf{X}_j - \\mathbf{X}_{I_k}^\\top \\mathbf{w}_{I_k}\\|_2^2}_{\\text{control distance to treated synthetic}}\n\\Bigg].\n\\end{aligned}\n\\]\nAnalysts can select whichever design best aligns with the goals and constraints of their study."
  },
  {
    "objectID": "scexp.html#inference",
    "href": "scexp.html#inference",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Inference",
    "text": "Inference\nThe next step is to estimate the treatment effects and quantify uncertainty. For each neighborhood \\(j\\) in cluster \\(k\\), the synthetic treated and control outcomes at time \\(t \\in \\mathcal{T}_2\\) are constructed as weighted averages of the units. The difference between these outcomes is the unit-level treatment effect: \\(\\hat{\\tau}_{j \\in \\mathcal{N}_1 t} = w_{j}y_{j \\in \\mathcal{N}_1 t} - v_{j}y_{j \\in \\mathcal{N}_0 t}\\). Within each cluster, unit-level effects are aggregated using pre-specified weights \\(f_j\\) to obtain the cluster-level effect: \\(\\hat{\\tau}_{k t} = \\sum_{j \\in I_k, z_j = 1} f_j \\hat{\\tau}_{j \\in \\mathcal{N}_1 t}\\). Summing over clusters gives the overall treatment effect: \\(\\hat{\\tau}_t = \\sum_{k=1}^K \\hat{\\tau}_{k t} = \\sum_{j \\in \\mathcal{J}, z_j = 1} f_j \\hat{\\tau}_{j \\in \\mathcal{N}_1 t}\\).\nTo build confidence intervals, we use the distribution of in-time placebo effects. These are computed in the blank pre-treatment period \\(\\mathcal{B}\\) and capture residual fluctuations when no treatment is applied. The cluster-level placebo effect is \\(\\hat{\\tau}_{k t}^{\\mathcal{B}} = \\sum_{j \\in I_k, z_j = 1} f_j (w_j y_{jt} - v_j y_{jt}), \\quad t \\in \\mathcal{B}\\). Global placebo effects are obtained by summing across clusters:\n\\(\\hat{\\tau}_t^{\\mathcal{B}} = \\sum_{k=1}^K \\hat{\\tau}_{k t}^{\\mathcal{B}}, \\quad t \\in \\mathcal{B}\\). Split-conformal confidence intervals are then constructed. For the global effect, we take the \\((1-\\alpha)\\) quantile of the absolute placebo effects: \\(q_{1-\\alpha} = \\text{Quantile}_{1-\\alpha}(|\\hat{\\tau}_t^{\\mathcal{B}}|), \\quad t \\in \\mathcal{B}\\), and define the confidence interval as \\(\\mathrm{CI}_t = [\\hat{\\tau}_t - q_{1-\\alpha}, \\hat{\\tau}_t + q_{1-\\alpha}], \\quad t \\in \\mathcal{T}_2\\). Cluster-level confidence intervals are defined similarly, replacing global effects with the corresponding cluster-level placebo distributions.\nStatistical significance can be assessed using permutation tests. The observed global statistic is \\(S_{\\text{obs}} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} |\\hat{\\tau}_t|\\). By randomly sampling permutations from placebo and post-treatment effects, we generate test statistics \\(S_{\\text{perm}}^{(b)} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\text{sample}} |\\hat{\\tau}_t|, \\quad b = 1, \\dots, B\\), and compute the global p-value as \\(p_{\\text{global}} = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{S_{\\text{perm}}^{(b)} \\ge S_{\\text{obs}}\\}\\). Cluster-level significance tests follow the same logic using the cluster-level placebo distributions."
  },
  {
    "objectID": "scexp.html#intuition",
    "href": "scexp.html#intuition",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Intuition",
    "text": "Intuition\nLet‚Äôs take a step back from all the matrices, weights, and constraints and ask: what does this framework actually do in practice? All the math above essentially boils down to this: we are choosing who gets treated and who serves as control by building weighted averages of neighborhoods, under a set of explicit rules.\nThink of the experimental synthetic control framework not as a mysterious math exercise, but as a decision-making engine for planning market experiments before we do any effect size estimation. After all, suppose you or I were literally hired by the government to do this. Their goal is to figure out which neighborhoods (or groups of units) should receive a treatment‚Äîsay, implementing sustainability initiatives‚Äîand which should serve as controls. These choices should not (and realistically, cannot) be made arbitrarily. They must follow rules set by the client, which encode trade-offs among fairness, cost, representativeness, and logistical feasibility.\nThe first step is grouping neighborhoods into clusters that share characteristics such as geographic proximity, tourist demographics, historical activity, or energy usage. This makes the comparisons meaningful: apples to apples, not apples to pineapples. In this setting, our goal is inference about the average treatment effect, not just the effect on the treated. That distinction matters: we care about market-level representativeness, not only the treated group itself.\nFrom here, analysts specify objectives. Should treated neighborhoods closely resemble the average of their cluster? Or should the match be tighter at the unit level? Different design variations encode different answers. Meanwhile, real-world constraints‚Äîbudgets, minimum or maximum treated units per cluster, operational limitations, ethical concerns‚Äîare encoded directly in the optimization.\nThe optimization then selects synthetic treated and control neighborhoods. Instead of picking actual neighborhoods at random or by instinct, the framework identifies weighted combinations of neighborhoods that best satisfy the objectives and constraints. The base estimator makes both sides look like the cluster mean. The weakly targeted estimator (recall the \\(\\beta\\) penalty) pushes treated and control groups to resemble each other directly. Penalized and unit-level estimators add distance-based terms, reflecting geography, demographics, or other practical considerations we might want to code in.\nWhat comes out of the optimization is a recommendation: given the priorities and constraints, here is the set of neighborhoods to treat, and here is the synthetic control to compare against. Policymakers can then implement the plan‚Äîworking with hotel associations, local communities, or other stakeholders‚Äîknowing fairness and feasibility were accounted for upfront. Later, when post-treatment data arrive, analysts reuse the same synthetic control weights for causal estimation. The design stage and the inference stage are tied together by the same set of weights.\nIn this sense, experimental SC is a full-cycle experimental design tool. It is not only about estimating effects‚Äîit is about making a plan for who gets treated and why. Analysts can tune parameters to explore trade-offs: for example, increasing \\(\\beta\\) prioritizes representativeness of treated neighborhoods relative to controls. This flexibility turns the framework into a sandbox for scenario planning: simulate different designs, see how assignments shift, and produce recommendations that are realistic and robust.\nThat‚Äôs the bridge from theory to practice. The real question is: given our goals, constraints, and the characteristics of this market, which neighborhoods should we treat, and why?"
  },
  {
    "objectID": "scexp.html#non-willemstad",
    "href": "scexp.html#non-willemstad",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Non-Willemstad",
    "text": "Non-Willemstad\nThe pre-treatment RMSE for Cluster 0 (Non-Willemstad) is 0.215, indicating an excellent pre-treatment fit. The synthetic control is constructed from the following control units:\n\n\n\nUnit\nWeight\n\n\n\n\nBarber\n0.124\n\n\nLag√∫n\n0.062\n\n\nSanta Rosa\n0.281\n\n\nWestpunt\n0.533\n\n\n\nThe treated group is represented by the following units:\n\n\n\nUnit\nWeight\n\n\n\n\nSint Willibrordus\n0.382\n\n\nSoto\n0.303\n\n\nSpaanse Water\n0.315\n\n\n\nHere, Westpunt dominates the control group at 53.3% and Sint Willibrordus leads the treated group at 38.2%. With balanced weights and an excellent fit, this setup indicates reliable treatment effect estimates for non-Willemstad areas."
  },
  {
    "objectID": "scexp.html#willemstad",
    "href": "scexp.html#willemstad",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "Willemstad",
    "text": "Willemstad\nThe pre-treatment RMSE for Cluster 1 (Willemstad) is 0.413, representing a fine fit though less precise than in Cluster 0. The synthetic control draws from these control units:\n\n\n\nUnit\nWeight\n\n\n\n\nGroot Kwartier\n0.093\n\n\nGroot Piscadera\n0.302\n\n\nPiscadera Bay\n0.605\n\n\n\nThe treated group consists of the following units:\n\n\n\nUnit\nWeight\n\n\n\n\nOtrobanda\n0.415\n\n\nSali√±a\n0.306\n\n\nScharloo\n0.279\n\n\n\nHere for the controls, Piscadera Bay exerts heavy influence on the synthetic control at 60.5% and Otrobanda heads the treated group at 41.5%. The somewhat higher RMSE implies slightly less confidence in treatment effect estimates for Willemstad, but the overall fit remains acceptable.\n\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/config_models.py:61: UserWarning:\n\nDataFrame was not sorted by [town, time] ‚Äî auto-sorting applied."
  },
  {
    "objectID": "scexp.html#one-cluster-setup-all-units",
    "href": "scexp.html#one-cluster-setup-all-units",
    "title": "Synthetic Controls for Marketing Experiments",
    "section": "One-Cluster Setup: All Units",
    "text": "One-Cluster Setup: All Units\nWhen treating all units as a single cluster, the pre-treatment RMSE is 0.449, which, given outcome values in the 40s, represents a very small error and a reasonable fit for analysis. The control group is formed from the following units:\n\n\n\nUnit\nWeight\n\n\n\n\nBarber\n0.137\n\n\nSanta Rosa\n0.029\n\n\nScharloo\n0.728\n\n\nSoto\n0.107\n\n\n\nThe treated group consists of these units:\n\n\n\nUnit\nWeight\n\n\n\n\nPietermaai\n0.239\n\n\nPiscadera Bay\n0.537\n\n\nTera Cor√°\n0.224\n\n\n\n\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/config_models.py:61: UserWarning:\n\nDataFrame was not sorted by [town, time] ‚Äî auto-sorting applied.\n\n\n\n\n\n\n\n\n\n\nThis RMSE of 0.449 indicates a fine fit, with Scharloo heavily influencing the control group at 72.8% and Piscadera Bay dominating the treated group at 53.7%. The fit, while slightly less precise than Cluster 0‚Äôs RMSE of 0.215, remains robust given the scale of the outcomes and supports reliable treatment effect estimates. The results show that the penalized design obtains good pretreatment fit \\(\\lambda_1 = \\lambda_2 = 0.02\\) penalties. Of course, in reality we can choose these lambdas via cross validation, instead of setting them manually. These were just chosen for the sake of example. Either way, the synthetic design is robust whether we divide Cura√ßao into clusters or treat it as a single unit. Also, these data were simualted from Wikipedia, in theory we could get both population and density data on a wider variety of places in Cura√ßao, with a more sophisitcated hierarchical linear factor model, to see how the estimator would play with even more clusters/units."
  },
  {
    "objectID": "scmo.html",
    "href": "scmo.html",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "",
    "text": "Sometimes analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may possibly predict a target/outcome variable that we care about, and plenty of other papers have commented on this fact before. Some analysts even argue for the surrogate approach, where we treat other outcomes or affected units as a kind of instrument for the counterfactual trajcectory of the target unit.\nHowever, all of this is most uncommon. As it turns out, most people in academia and industry who use synthetic controls use only a single focal outcome in their analyses. Perhaps they will adjust their unit weights by some diagonal matrix, a diagonal matrix \\(\\mathbf{V}\\) in most applications. The point of this matrix is basically to assist the main optimization in choosing the unit weights. However, even this is limited by the number of pretreatment periods you have- if you have more covariates than you have pretreatment periods, you cannot estimate the regression. Recent papers by econometricians have tried to get around this, though. This blog post covers a few recent recent papers which have advocated for this. I explain the econometric method and apply it in a simulated setting."
  },
  {
    "objectID": "scmo.html#standard-synthetic-control",
    "href": "scmo.html#standard-synthetic-control",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Standard Synthetic Control",
    "text": "Standard Synthetic Control\nBefore introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator minimizes\n\\[\n\\mathbf{w}^\\ast = \\underset{\\mathbf{w} \\in \\Delta^{N_0}}{\\operatorname*{argmin}} \\|\\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThis is a constrained least squares program in which we regress the treated unit‚Äôs pre-treatment outcomes onto the control matrix under the constraint that \\(\\mathbf{w}\\) lies in the simplex \\(\\Delta^{N_0}\\). For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights \\(\\mathbf{w}^\\ast\\) are estimated, the out-of-sample estimates are obtained by applying the same weights to the control matrix\n\\[\n\\mathbf{y}^{\\text{SC}}_1 = \\mathbf{Y}_0^{\\text{post}} \\mathbf{w}^\\ast,\n\\]\nwith the concatenation between the in and out of sample vectors corresponding to the full predictions of the model. The estimated treatment effect at each post-treatment time point is then given by the difference between observed and out-of-sample outcomes: \\(\\hat{\\tau}_{1t} = y_{1t} - \\hat{y}_{1t}\\) for \\(t \\in \\mathcal{T}_2\\)."
  },
  {
    "objectID": "scmo.html#model-averaging",
    "href": "scmo.html#model-averaging",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Model Averaging",
    "text": "Model Averaging\nWe may also model average these models together, which sometimes results in better fit than using either model alone. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have \\(\\mathbf{y}_1^{\\text{CAT}} \\in \\mathbb{R}^{T_0}\\), which denotes the pre-treatment fit from the concatenated model by Tian, Lee, and Panchenko, and on the other hand, we have \\(\\mathbf{y}_1^{\\text{AVG}} \\in \\mathbb{R}^{T_0}\\), the corresponding fit from the demeaned model by Sun, Ben-Michael, and Feller. As before, we observe the treated unit‚Äôs pre-treatment trajectory, \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\).\nTo begin, we stack the two counterfactuals into a single matrix:\n\\[\n\\mathbf{Y}^{\\text{MA}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT}} & \\mathbf{y}_1^{\\text{AVG}}\n\\end{bmatrix} \\in \\mathbb{R}^{T_0 \\times 2}.\n\\]\nWe define the model-averaged pre-treatment fit as a convex combination of the two predictions\n\\[\n\\mathbf{y}_1^{\\text{MA}}(\\boldsymbol{\\lambda}) = \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda},\n\\]\nwhere \\(\\boldsymbol{\\lambda} \\in \\Delta^2\\) is a 2-dimensional simplex weight vector\n\\[\n\\Delta^2 = \\left\\{ \\boldsymbol{\\lambda} \\in \\mathbb{R}_{\\geq 0}^2 : \\| \\boldsymbol{\\lambda} \\|_1 = 1 \\right\\}.\n\\]\nThe model averaged objective function minimizes\n\\[\n\\boldsymbol{\\lambda}^\\ast = \\underset{\\boldsymbol{\\lambda} \\in \\Delta^2}{\\operatorname{argmin}} \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}^{\\text{MA}} \\boldsymbol{\\lambda} \\right\\|_F^2 \\quad \\forall t \\in \\mathcal{T}_1.\n\\]\nThe interpretation of the convex hull remains the same as in the traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the global minimum and maximum of the two individual estimators\n\\[\n\\mathbf{y}_1^{\\text{MA}} \\in \\left[\n\\min\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right),\n\\max\\left(\\mathbf{y}_1^{\\text{CAT}}, \\mathbf{y}_1^{\\text{AVG}}\\right)\n\\right]\n\\quad \\forall t \\in \\mathcal{T}.\n\\]\nOnce \\(\\boldsymbol{\\lambda}^\\ast\\) is found, the model-averaged out-of-sample predictions are estimated like\n\\[\n\\mathbf{Y}^{\\text{MA, post}} = \\begin{bmatrix}\n\\mathbf{y}_1^{\\text{CAT, post}} & \\mathbf{y}_1^{\\text{AVG, post}}\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_1^{\\text{MA, post}} = \\mathbf{Y}^{\\text{MA, post}} \\boldsymbol{\\lambda}^\\ast.\n\\]\nEssentially, this is a mixture of both models."
  },
  {
    "objectID": "scmo.html#conformal-prediction-via-agnostic-means",
    "href": "scmo.html#conformal-prediction-via-agnostic-means",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Conformal Prediction via Agnostic Means",
    "text": "Conformal Prediction via Agnostic Means\nNow a final word on infernece. I use conformal prediction intervals to conduct inference here, developed in this paper. Precisely, I use the agnostic approach (yes, I know other approaches exist; users of mlsynth will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as \\(\\mathbf{u}_{\\text{pre}} = \\mathbf{y}_{1,\\text{pre}} - \\mathbf{y}^{\\text{SC}}_{1,\\text{pre}}\\), or just the pretreatment difference betwixt the observed values and its counterfactual. Furthermore, let \\(\\hat{\\sigma}^2 = \\frac{1}{T_0 - 1} \\left\\| \\mathbf{u}_{\\text{pre}} - \\bar{u} \\mathbf{1} \\right\\|^2\\) be the unbiased estimator of the residual variance, where \\(\\bar{u} = \\frac{1}{T_0} \\sum_{t=1}^{T_0} u_t\\) is the mean residual.\nWe aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period \\(\\mathbf{y}^{\\text{SC}}_{1,\\text{post}} \\in \\mathbb{R}^{T_1}\\) be the post-treatment SC predictions for some generic estimator. Assuming that the out-of-sample error is sub-Gaussian given the history \\(\\mathscr{H}\\) (in plain English, this just means that large errors are unlikely, which makes sense given that SC is less biased in a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via concentration inequalities. Specifically, we have \\(\\delta_\\alpha = \\sqrt{2 \\hat{\\sigma}^2 \\log(2 / \\alpha)}\\). The conformal prediction intervals are then defined as \\(\\mathbf{p}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} - \\delta_\\alpha \\mathbf{1}\\), \\(\\mathbf{u}_{\\text{post}} = \\mathbf{y}^{\\text{SC}}_{1,\\text{post}} + \\delta_\\alpha \\mathbf{1}\\). These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will likely be incorporated in the future."
  },
  {
    "objectID": "scmo.html#simulation",
    "href": "scmo.html#simulation",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Simulation",
    "text": "Simulation\nSuppose we are working at Airbnb, and we wish to see the causal effect of the introduction of Airbnb Experiences on Gross Booking Value (GBV), a metric which is defined as ‚Äò‚Äôthe total revenue generated by room or property rentals before any costs or expenses are subtracted‚Äô‚Äô. Airbnb Experiences connects users of the platform to local tour guides or other local attractions. It serves as a kind of competition to Travelocity, Viator and other booking/ travel services. In other words, this program may make makes this city an attraction, and we may see an increase in GBV as a result.\nWell, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy. The point of this simulation is to use the SCMO (synthetic control multiple outcomes) estimator to measure the causal impact.\nFor each unit, the observed outcome \\(\\mathbf{Y}_{jtk}\\) evolves according to an autoregressive process with latent structure for time, place, and seasonality\n\\[\n\\mathbf{Y}_{jtk} =\n\\rho_k \\mathbf{Y}_{jt-1k} +\n(1 - \\rho_k) \\left(\n\\alpha_{jk} + \\beta_{tk} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{tk} + \\mathbf{S}_{jt} + \\delta_k\n\\right) + \\varepsilon_{jtk}, \\quad \\text{for } t &gt; 1,\n\\]\nwith initial condition\n\\[\n\\mathbf{Y}_{j1k} =\n\\alpha_{jk} + \\beta_{1k} + \\boldsymbol{\\phi}_j^\\top \\boldsymbol{\\mu}_{1k} + \\mathbf{S}_{j1} + \\delta_k + \\varepsilon_{j1k}.\n\\]\nHere, \\(\\alpha_{jk} \\sim \\mathcal{N}(0, 1)\\) and \\(\\beta_{tk} \\sim \\mathcal{N}(0, 1)\\) represent unit-outcome and time-outcome fixed effects, respectively. Each unit \\(j\\) possesses latent attributes \\(\\boldsymbol{\\phi}_j \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\), while each time-outcome pair \\((tk)\\) has associated latent loadings \\(\\boldsymbol{\\mu}_{tk} \\in \\mathbb{R}^r \\sim \\mathcal{N}(0, \\mathbf{I})\\). The seasonal component \\(\\mathbf{S}_{jt}\\) captures unit-specific periodicity and is defined as \\(\\gamma_j \\cos\\left( \\frac{4\\pi(t - \\tau_j)}{T_{\\text{season}}} \\right)\\), with \\(\\gamma_j \\sim \\text{Unif}(0, \\bar{\\gamma})\\) representing the amplitude and \\(\\tau_j \\sim \\text{Unif}\\{0, \\dots, T_{\\text{season}} - 1\\}\\) the phase shift. Each outcome \\(k\\) has a baseline shift \\(\\delta_k \\sim \\text{Unif}(200, 500)\\), an autocorrelation parameter \\(\\rho_k \\in (0, 1)\\), and an idiosyncratic noise component \\(\\varepsilon_{jtk} \\sim \\mathcal{N}(0, \\sigma^2)\\). One unit (Iquique, Chile in this draw) is designated as treated. To introduce selection bias, the unit with the second-largest realization on the first latent factor dimension is treated, meaning methods like difference-in-differences or interrupted time series methods will not perform well. The target unit‚Äôs GBV receives an additive treatment effect of \\(+5\\) during all post-treatment periods."
  },
  {
    "objectID": "scmo.html#results",
    "href": "scmo.html#results",
    "title": "Synthetic Controls With More Than One Outcome",
    "section": "Results",
    "text": "Results\nWhen we run this estimator, we need to specify one of three estimators: TLP, SBMF, or both, where the abbreviations are obviouslyfor the surnames of the authors. We also need to supply a dictionary entry to mlsynth called addout. This is either a string or a list which lists the additional outcomes we care about in the dataframe. When we run the estimator, we get:\n\n# Run simulation\n\ndf = simulate(seed=10000, r=3)\n\nconfig = {\n    \"df\": df,\n    \"outcome\": 'Gross Booking Value',\n    \"treat\": 'Experiences',\n    \"unitid\": 'Market',\n    \"time\": 'Week',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"blue\"], \"addout\": list(df.columns[4:]),\n    \"method\": \"BOTH\"\n}\n\narco = SCMO(config).fit()\n\n/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/mlsynth/estimators/scmo.py:644: UserWarning:\n\nAn unexpected error occurred during SCMO plotting: plot_estimates() got an unexpected keyword argument 'df'\n\n\n\nUsing the model averaging estimator, our pre-treatment Root Mean Squared Error is 0.276. The ATT is 5.046. The weights are also a sparse vector. The model averaged estimator returns Arequipa (0.237), Bogot√° (0.232), San Salvador (0.218), Santiago de Chile (0.174), San Luis Potos√≠ (0.081), Montevidio (0.032), and Manzanillo (0.026), as the contributing units or only 7 of the 98 donor units. The optimal mixing between the models is 0.538 for the intercept-shifted estimator and 0.461 for the concatenated method. For DID, the RMSE is 0.878 and the ATT is 5.2, meaning that the intercept adjusted average of all donor units is clearly a biased estimator.\nCompare to Forward DID, this is NOT true: we have an ATT of 5.063 and a pre-intervention RMSE of 0.289, selecting Antofagasta, Bocas del Toro, Punta del Este, San Pedro Sula, and Santiago as the optimal donor pool (this method uses no additional outcomes, only the GBV metric). When I compare to the clustered PCR method, the positively weighted donors are San Pedro Sula (0.296), Punta del Este (0.255), Santiago (0.199), Antofagasta (0.190), and La Plata (0.060)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Analysis, Data Science, and Causal Inference",
    "section": "",
    "text": "Here are my blog posts that cover causal inference, econometrics, machine learning, and other data science topics.\n\n\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\n\n\nA Test Post\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nShake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is An Average?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "\nJared Greathouse\n",
    "section": "",
    "text": "Jared Greathouse\n\n\nEconometrician. Consultant.\n\n\nüëã Welcome\nI am Jared Amani Greathouse. I offer consulting services in data science and econometrics, especially with synthetic control methods.\n\n\nüìÑ Intake Form\nTo offer me a project:\n\nDownload the Request Intake Form (DOCX). If the form does not download, copy the link and put it in your browser.\nWhen filled out, email it to me with a short description of the project.\nFees begin at at $250/hour.\n\n\n\nüì¨ Contact\nSchool Email: jgreathouse3@student.gsu.edu (this is the best way to reach me)\nPersonal Email: j.greathouse200@gmail.com\nWebsite: jgreathouse9.github.io"
  },
  {
    "objectID": "aaascrape.html",
    "href": "aaascrape.html",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "",
    "text": "In academia, many datasets we work with come in pretty excel/csv files that are clean. And while that‚Äôs great‚Ä¶ oftentimes, in modern policy data science, the data we seek are unstructured in the sense that they do not come from a specific file such as a .csv file or Stata dataset. Such data must be web-scraped, or the process of gathering unstructured data, via reproducible code/script.\nThis post simply seeks to present a simple use case of web-scraping in the setting the applied policy scientist might need for certain tasks. Say we wish to gather the price of gas from AAA, across each metro area in the nation. In the old days, we‚Äôd need to ask AAA and pay thousands of dollars for an extended time series‚Ä¶ but now we don‚Äôt need to, at least for this case. The reason is because we have Python, and we are able to leverage its basic features to collect these data via exploiting the publicly available information they provide us."
  },
  {
    "objectID": "aaascrape.html#the-scrape",
    "href": "aaascrape.html#the-scrape",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Scrape",
    "text": "The Scrape\nHere is the code for the scrape. These are the helpers that we get one function to call. This is the nitty-gritty that calls and collects the data of interest.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\n\ndef fetch_gas_prices(state_abbreviations):\n    \"\"\"\n    Fetches and processes gas price data for multiple states and their respective cities.\n\n    This function scrapes the AAA Gas Prices website to collect gas prices for different fuel grades\n    (Regular, Mid-Grade, Premium, Diesel) in various cities within the specified states. The collected\n    data is structured into a pandas DataFrame.\n\n    Parameters:\n    -----------\n    state_abbreviations : dict\n        A dictionary mapping state names to their respective abbreviations, e.g.,\n        {\"California\": \"CA\", \"Texas\": \"TX\"}.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing gas prices with columns:\n        ['Date', 'State', 'City', 'Regular', 'Mid-Grade', 'Premium', 'Diesel']\n    \"\"\"\n\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n    }\n\n    today = pd.Timestamp.today()\n    time_mapping = {\n        \"Current Avg.\": lambda: today,\n        \"Yesterday Avg.\": lambda: today - pd.Timedelta(days=1),\n        \"Week Ago Avg.\": lambda: today - pd.Timedelta(weeks=1),\n        \"Month Ago Avg.\": lambda: today - relativedelta(months=1),\n        \"Year Ago Avg.\": lambda: today - relativedelta(years=1),\n    }\n\n    def extract_gas_prices(row, time_mapping, today, state, city_name):\n        \"\"\"\n        Extracts and processes gas price data from a single row in the gas price table.\n\n        Parameters:\n        -----------\n        row : bs4.element.Tag\n            A BeautifulSoup object representing a table row (&lt;tr&gt;) containing gas prices.\n        time_mapping : dict\n            A dictionary mapping AAA's time labels (e.g., \"Current Avg.\") to corresponding dates.\n        today : pd.Timestamp\n            The current date, used as a fallback if no valid time label is found.\n        state : str\n            The full name of the state (e.g., \"California\").\n        city_name : str\n            The name of the city corresponding to the extracted gas prices.\n\n        Returns:\n        --------\n        list\n            A list containing extracted data in the format:\n            [date (str), state (str), city (str), regular (str), mid-grade (str), premium (str), diesel (str)]\n        \"\"\"\n\n        cells = row.find_all(\"td\")\n        date_text = cells[0].get_text(strip=True)\n\n        # Get the corresponding date using time_mapping, defaulting to today\n\n        date = time_mapping.get(date_text, lambda: today)().strftime(\"%Y-%d-%m\")\n\n        # Extract prices, removing the dollar sign\n\n        prices = [cell.get_text(strip=True).replace(\"$\", \"\") for cell in cells[1:]]\n\n        return [date, state, city_name] + prices\n\n    def process_city_data(city, time_mapping, today, state):\n        \"\"\"\n        Extracts gas price data for a specific city by locating its corresponding table.\n\n        Parameters:\n        -----------\n        city : bs4.element.Tag\n            A BeautifulSoup object representing a city's heading element.\n        time_mapping : dict\n            A dictionary mapping time labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n        state : str\n            The full name of the state.\n\n        Returns:\n        --------\n        list\n            A list of lists, where each inner list contains gas price data for a specific date in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        city_name = city.get_text(strip=True)\n        rows = city.find_next(\"table\").select(\"tbody tr\")\n\n        return [\n            extract_gas_prices(row, time_mapping, today, state, city_name)\n            for row in rows\n        ]\n\n    def process_states(state_abbreviations, headers, time_mapping, today):\n        \"\"\"\n        Iterates through all states, sending requests to fetch and process gas price data.\n\n        Parameters:\n        -----------\n        state_abbreviations : dict\n            A dictionary mapping state names to their respective abbreviations.\n        headers : dict\n            HTTP request headers to mimic a real browser and avoid request blocking.\n        time_mapping : dict\n            A dictionary mapping AAA's date labels to actual date values.\n        today : pd.Timestamp\n            The current date used for fallback mapping.\n\n        Returns:\n        --------\n        list\n            A list of lists containing processed gas price data for all states in the format:\n            [date, state, city, regular, mid-grade, premium, diesel]\n        \"\"\"\n\n        all_data = []\n        for state, abbreviation in state_abbreviations.items():\n            params = {\"state\": abbreviation}\n            response = requests.get(\n                \"https://gasprices.aaa.com/\", params=params, headers=headers\n            )\n\n            if response.status_code != 200:\n                print(\n                    f\"Error fetching data for {state}. Status code: {response.status_code}\"\n                )\n                continue\n            soup = BeautifulSoup(response.content, \"html.parser\")\n\n            # Extract city sections\n\n            cities = soup.select(\".accordion-prices.metros-js &gt; h3[data-title]\")\n            all_data.extend(\n                [\n                    row_data\n                    for city in cities\n                    for row_data in process_city_data(city, time_mapping, today, state)\n                ]\n            )\n        return all_data\n\n    # Fetch and process data for all states\n\n    all_data = process_states(state_abbreviations, headers, time_mapping, today)\n\n    # Convert list of extracted data into a pandas DataFrame\n\n    all_data_df = pd.DataFrame(\n        all_data,\n        columns=[\"Date\", \"State\", \"City\", \"Regular\", \"Mid-Grade\", \"Premium\", \"Diesel\"],\n    )\n\n    # Convert 'Date' to datetime format\n\n    all_data_df[\"Date\"] = pd.to_datetime(all_data_df[\"Date\"], format=\"%Y-%d-%m\")\n\n    # Sort by 'State', 'City', and 'Date' for better organization\n\n    all_data_df = all_data_df.sort_values(by=[\"State\", \"City\", \"Date\"]).reset_index(\n        drop=True\n    )\n\n    return all_data_df\n\nWe first define AAA‚Äôs website as the URL of interest, Then, we inspect the URL for each state where the actual data for each city/metro is located at. For Massachusetts, the URL is ‚Äúhttps://gasprices.aaa.com/?state=MA‚Äù. For Florida, the URL is ‚Äúhttps://gasprices.aaa.com/?state=FL‚Äù. See the pattern? There‚Äôs a common prefix, with the only thing changing being the suffix of which is the abbreviation of the state.\nThe master function is fetch_gas_prices. This simply accepts a dictionary of state abbreviations (the value) paired with the key (the state name) we shall query over. We, within this function, define a common time mapping which AAA uses to stnadardize dates, and specify the current date.\nNext I use Python‚Äôs requests library to query each state (plenty of ways to do this step, but requests is generally the quickest for simple and even fairly big jobs). I then process each state with the process_states function. This accepts a list of states, headers, the common time mapping, and the current date. We then query each state, and clean the accordion-style tables that we see towards the bottom of the page. We clean them for each city/each metro area with the process_city_data function. Then append all of them into a single dataframe for each state, moving on to the next state after we‚Äôve axhausted all metros for that state. Finally we appened the state dataframes together, where we have the date, the prices of gas, the state, and the city name all in one place."
  },
  {
    "objectID": "aaascrape.html#the-call",
    "href": "aaascrape.html#the-call",
    "title": "Data Science for Policy Analysts: A Simple Introduction to Web Scraping",
    "section": "The Call",
    "text": "The Call\nWe may then call this whole script with a few lines of python code.\n\nimport pandas as pd\nfrom datetime import datetime\nfrom cityutils import fetch_gas_prices\nimport os\n\n# We just need the state abbreviations since\n# AAA indexes their states by the abbreviation.\n\nurl = \"https://raw.githubusercontent.com/jasonong/List-of-US-States/refs/heads/master/states.csv\"\n\n# We read the csv into a df\nstates_df = pd.read_csv(url)\n\n# Here is the main function that does the scrape.\ndf = fetch_gas_prices(dict(zip(states_df['State'], states_df['Abbreviation'])))\n\n# Format the date for the filename\ndate_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Ensure the output directory exists\noutput_dir = \"./City Scrape/Data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the DataFrame as \"/City Scrape/Data/City_{date}.csv\"\noutput_path = f\"{output_dir}/City_{date_str}.csv\"\ndf.to_csv(output_path, index=False)\n\nThis pulls everything together in a single dataframe ans saves it in a csv file. Notice that the data are collected from today, yesterday, last week, last month, and last year. Meaning, in theory, we have at least a year‚Äôs worth of prior data to collect, assuming we collected this data for a year.\nThis a simple case of web scraping. I‚Äôve done more complicated scrapes, such as scraping the prices of goods across every Whole Foods in the country across 5 different departments per store. The scale of the job and the size of the data is much bigger, but the principles are overall the same. In the future, I will write more posts that demonstrate different applications of web-scraping and how policy scientists may use them. In particular, I will cover how scraping can go hand in hand with GitHub Actions. In my experience so far as a PHD student, these two alone (scraping and Github Actions) has made my life as a researcher far easier than otherwise."
  },
  {
    "objectID": "synthinter.html",
    "href": "synthinter.html",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "",
    "text": "Most data scientists who use synthetic control methods likely aim to estimate the counterfactual outcome for a treated unit if it had not been treated at all. This framework works quite neatly in the setting with a dummy treatment status (exposed or not exposed). But very often, there are a grab bag of policies we concern ourselves with that oftentimes may affect the exact same outcome. A retail chain might roll out different loyalty programs across its stores. For instance, Store A adopts a cash-back program (Program A), Store B adopts a points-based program (Program B), and Store C adopts a discount coupon program (Program C). Observing outcomes like revenue and customer visits under each program, we are typically concered with the outcome under the scenario of no treatment. A research question could be how the store with the cash backprogram would have fared had it done nothing at all. In policy, cities or states implement different public health interventions targeting soda consumption. For example, City 1 bans the sale of large sodas, City 2 imposes a tax on sugary drinks, and City 3 mandates calorie labeling on menus.\nBut what if we wish to estimate the counterfactual for a given unit as if it did another policy from what it actually did. In other words, How would Store A have performed if it had adopted Program B instead of Program A? What if City 1 had imposed a soda tax instead of banning large sodas? How would health metrics evolved if the other policy was done instead? Plenty of academic papers have addressed this idea before, but only to assess the counterfactual scenario of no tax at all.\nThis is where the Synthetic Interventions estimator is useful. SI estimates how a treated unit (or even a never treated unit) would have performed under an intervention it did not actually receive. Per the examples above, one could construct a synthetic version of Store A under Program B, based on observed outcomes from stores that actually implemented Program B. Similarly, we could construct a synthetic version of City/State 1 under the soda tax policy (instead of the ban), using observed outcomes from cities that actually taxed sugary drinks. SI substantially expands the range of counterfactual questions that can be credibly answered.\nBefore we get to business, I should note how this is a personal milestone for me. SI was likely the first estimator that really got me interested in the extensions to SCM. You see, I became a PHD student right around the time the pandemic began, and we began to see papers like this circulate that claimed to be able to estimate things like ‚Äúwhat would NYC‚Äôs COVID rate look like had it locked down earlier than it actually did‚Äù. I had never heard of tensors, or really even matrices at the time, so I would always ask ‚Äúwow, how can we even do this?‚Äù So, on my 4th year of school, it is very nice to not just understand the estimator, but be able to share it with others."
  },
  {
    "objectID": "synthinter.html#si-in-mlsynth",
    "href": "synthinter.html#si-in-mlsynth",
    "title": "Synthetic Control Methods for Personalized Causal Inference",
    "section": "SI in mlsynth",
    "text": "SI in mlsynth\nNow I will give an example of how to use SI for your own applied work. As ususal, in order to properly implement this, we begin by installing mlsynth from my Github\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nNext we load the Proposition 99 data, using the dataset that has all of the states in the United States.\n\nimport pandas as pd\nfrom mlsynth import SI\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/jehangiramjad/tslib/refs/heads/master/tests/testdata/prop99.csv\"\ndf = pd.read_csv(url)\nlast_column = df.columns[-1]\ndf_filtered = df[df[last_column] == 2]\n\ncolumns_to_keep = list(df.columns[0:4]) + [df.columns[7]]\ndf_filtered = df_filtered[columns_to_keep]\n\nsort_columns = [df_filtered.columns[1], df_filtered.columns[2]]\ndf_filtered = df_filtered.sort_values(by=sort_columns)\n\ndf_filtered = df_filtered[df_filtered[df_filtered.columns[2]] &lt; 2000]\n\ndf_filtered['SynthInter'] = ((df_filtered[df_filtered.columns[0]] == 'LA') &\n                         (df_filtered[df_filtered.columns[2]] &gt;= 1992)).astype(int)\n\n\ntax_states = ['MA', 'AZ', 'OR', 'FL']  # Massachusetts, Arizona, Oregon, Florida abbreviations\ndf_filtered['Taxes'] = (df_filtered[df_filtered.columns[0]].isin(tax_states)).astype(int)\n\nprogram_states = ['AK', 'HI', 'MD', 'MI', 'NJ', 'NY', 'WA', 'CA']\ndf_filtered['Program'] = (df_filtered[df_filtered.columns[0]].isin(program_states)).astype(int)\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nWe begin with simple data cleaning. All we do is keep the relevant metrics, setting a treatment variable ‚ÄúSynthInter‚Äù to be 1 if the year is greater than or equal to 1992 and the unit of interest is Louisiana. I choose 1992 because this is the year Massachusetts passed its anti tobacco policy. Note that no policy in fact happened in Louisiana, so we will see the effect of keeping the status quo of no tobacco control policy at all. We then define as an indicator which states did the taxes. According to Abadie‚Äôs 2010 paper those states are Massachusetts, Arizona, Oregon, and Florida. We also define an indicator for states that did an anti-tobacco statewide program. This way, we can see how per capita smoking would have evolved under either policy. Under the hood, we just loop through each of the different policies the user specifies.\n\ndf_filtered = df_filtered.rename(columns={\"Data_Value\": \"cigsale\", \"LocationDesc\": \"State\"})\n\nconfig = {\n    \"df\": df_filtered,\n    \"outcome\": 'cigsale',\n    \"treat\": 'SynthInter',\n    \"unitid\": 'State',\n    \"time\": 'Year',\n    \"display_graphs\": True,\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"inters\": [\"Taxes\", \"Program\"]\n}\n\narco = SI(config).fit()\n\n\n\n\n\n\n\n\nFor Louisiana, the ATT of keeping the status quo relative to the state-wide tobacco program maintained per capita consumption 19.134 higher than what it would have been otherwise. The ATT of not passing taxes on tobacco, compared to taxes, mainatined the same at roughly 19.77 higher than what would have been otherwise. To me, these findings are not very surprising: ‚ÄúAnti tobacco programs/taxes reduce slightly tobacco consumption, wow, how shocking!‚Äù But now, we can actually answer these kinds of questions using econometric methods, instead of just speculating.\nWhen we look at the dictionary that the SI class returns, we see that the policies would have been pretty effective at reducing tobacco consumption. The dictionary is policy specific, one dictionary per policy the user chooses to consider. The SI estimator also only works for 1 treated unit, however analysts can easily make (say) a list of dataframes with an indicator for each synthetic treatment and loop over them. Once they have done this, we may extract the ATT per policy, per treated unit, and compute the sample average for an event-time ATT version for a given policy across units. I do not optimize the code to do this for a few reasons: one, the more policies and treated units we have, the less efficient it becomes to estimate tractably. Furthermore, the SI estimator is about personalized causal inference. So, I will likely leave this class for users to adapt to their own purposes (with some adjustments, if demand exists for something else)."
  },
  {
    "objectID": "spillsynth.html",
    "href": "spillsynth.html",
    "title": "The Iterative Synthetic Control Method",
    "section": "",
    "text": "This will be a short blog post. I‚Äôve spent the last two months doing a little industry work in the marketing realm, and in the meantime I made some substantial changes to mlsynth. This blog post simply shows one of the new key features I have implemented.\nChances are if you‚Äôre reading this, you know what I mean by the notion of SUTVA, or the stable unit treatment value assumption. It is the idea that if we care about the causal impact of a treatment on one unit, but other units are affected by the treatment or otherwise experience a similar treatment, that this exposure confounds our treatment effect with respect to the original unit we do care about. Say we wish to study the impact of German Reunification on West Germany‚Äôs GDP. We know West Germany was exposed, but what about neighboring nations like Austria or France? What if the reunification had regional effects? Analysts therefore have a problem: Austria and France may be very similar to West Germany, and therefore informative of West Germany‚Äôs counterfactual, but we are concerned they are exposed or affected by the main treatment of interest. What do we do? Before, researchers would need to drop these units or argue for their inclusion/exclusion, despite them being treated. Now, we do not need to do that, as SCM has a few approaches that deal with spillovers (this post covers just one). The approach, called iterative synthetic controls, is deceptively simple.\nSuppose Austria and France are partly treated. Step one of iSCM is to estimate a synthetic control for Austria, including France as a donor but excluding West Germany. Which SCM flavor you ask? Any one you like! For the purposes of this post, we will be using the Robsut SCM and the Robust PCA SCM methods from mlsynth. The precise details are not really important, but you may read the docs should you like. We then take the model predictions for Austria across the full pre and post period and replace the original Austria with the synthetic control values that the model predicts for Austria.\nNext, we do France: using the cleaned up Austria as a donor, we estimate the synthetic control for France, using the now-cleaned up Austria and the remaining donor pool units. As before, we replace the values for the original France (in the original dataset) with the new synthetic France. We now have cleaned up our two donors that may be exposed to the treatment.\nNow, with these two cleaned donors, we estimate the counterfactual for West Germany, with our 14 totally unexposed donors and the two now cleaned up donors that were once partially exposed.\n\nEstimation in Python\n‚ÄúBut Jared!‚Äù, you will say, this seems like a lot of looping and lots of donor tracking. Well fear not, that is what mlsynth is for. In order to get these results, you need Python (3.9 or greater) and mlsynth, which you may install from the Github repo. You‚Äôll need the most recent version.\npip install -U git+https://github.com/jgreathouse9/mlsynth.git\nFirst we estimate the orignal model. You will find our handy-dandy util function iterative_scm is now imported.\n\nimport pandas as pd\nfrom mlsynth import CLUSTERSC\nfrom mlsynth.utils.spillover import iterative_scm\n\n# Load the reunification dataset\nurl = \"https://raw.githubusercontent.com/jgreathouse9/mlsynth/main/basedata/german_reunification.csv\"\ndf = pd.read_csv(url)\n\n# Define configuration for CLUSTERSC\nconfig = {\n    \"df\": df,\n    \"outcome\": \"gdp\",          # per capita GDP\n    \"treat\": \"Reunification\",     # binary treatment indicator\n    \"unitid\": \"country\",          # country name\n    \"time\": \"year\",               # time variable\n    \"display_graphs\": True,       # display counterfactual plots\n    \"save\": False,\n    \"counterfactual_color\": [\"red\", \"blue\"],\n    \"Frequentist\": True, \"method\": \"BOTH\"\n}\n\noriginalresult = CLUSTERSC(config).fit()\n\n\n\n\n\n\n\n\nThese are the original results., You may check them against my coworker‚Äôs dissertation if you wish. Now we see how sensitive the results are to adjusting for spillover effects.\n\nfrom IPython.display import display, Markdown\n\niSCM_result = iterative_scm(CLUSTERSC(config), spillover_unit_identifiers=[\"Austria\", \"France\"], method=\"BOTH\")\n\ndef summarize_att_rmse_markdown(iSCM_result):\n    rows = []\n    for method in ['PCR', 'RPCA']:\n        sub = iSCM_result[method].sub_method_results[method]\n        att = sub.effects.additional_effects['ATT']\n        percent_att = sub.effects.additional_effects.get('Percent ATT', None)\n        t0_rmse = sub.fit_diagnostics.additional_metrics['T0 RMSE']\n\n        rows.append({\n            \"Method\": method,\n            \"ATT\": f\"{att:,.0f}\",\n            \"Percent ATT\": f\"{percent_att:.2f}%\" if percent_att is not None else \"‚Äî\",\n            \"T0 RMSE\": f\"{t0_rmse:,.1f}\",\n        })\n\n    df = pd.DataFrame(rows)\n    md_table = df.to_markdown(index=False)\n    display(Markdown(f\"### Iterative SCM \\n\\n{md_table}\"))\n\nsummarize_att_rmse_markdown(iSCM_result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIterative SCM\n\n\n\nMethod\nATT\nPercent ATT\nT0 RMSE\n\n\n\n\nPCR\n-2,111\n-7.96%\n108.4\n\n\nRPCA\n-1,477\n-5.71%\n88.1\n\n\n\n\n\nYou pass the original config to the util function, and you pass a list of strings that denote which units we believe are partly treated. Form there, the algorithm under the hood handles the donor cleaning, and returns back the results of the final SCM run with both donors cleaned. Note that if you want both PCR and RPCA cleaning, we pass it as method=\"BOTH\" to iterative_scm. We can see that the results are very similar to the original SCM. The pre-treatment fits degrade only very slightly (even more so with the Robust PCA method, highlighting its robustness to tiny tweaks in the model). Speaking of RPCA, the new weights are ‚ÄòBelgium‚Äô: 0.271, ‚ÄòNorway‚Äô: 0.552, ‚ÄòNew Zealand‚Äô: 0.34, whereas before they were ‚ÄòAustria‚Äô: 0.023, ‚ÄòFrance‚Äô: 0.354, ‚ÄòNorway‚Äô: 0.485, ‚ÄòNew Zealand‚Äô: 0.296. Austria goes away as a weighed donor, but there‚Äôs not very much change in the pre-treatment fit or the practical conclusions we draw from the analysis. When we use only Austria as the cleaned unit, RPCA‚Äôs weights are ‚ÄòUK‚Äô: 0.237, ‚ÄòFrance‚Äô: 0.607, ‚ÄòNorway‚Äô: 0.191, ‚ÄòNew Zealand‚Äô: 0.12 with an ATT of -1536.355. When we use only France, the ATT is -1490.636 and the weights are ‚ÄòAustria‚Äô: 0.262, ‚ÄòDenmark‚Äô: 0.004, ‚ÄòNorway‚Äô: 0.517, ‚ÄòNew Zealand‚Äô: 0.378. Of course, the key aspect of this procedure is knowing which units are likely to have spillover effects\n\n\nComments\nSo, this is not the only way to do this. There are plenty of other methods that people have developed for this purpose too. I likely will not program all of thse myself into mlsynth, but others who are so inclined are welcome to assist in the effort! in the future, I‚Äôll also allow you to switch between the options for each kind of spillover management (the inclusive method versus the iterative method, for example). But, now you know how to use this for your own work. As usual, comments or suggestions are always appreciated.\n\n\n\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Test Post\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nShake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is An Average?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "testpost.html",
    "href": "testpost.html",
    "title": "A Test Post",
    "section": "",
    "text": "a test post\n\nTheorem 1 (Line) The equation of any straight line, called a linear equation, can be written as:\n\\[\ny = mx + b\n\\]\n\nSee Theorem¬†1.\n\n\n\n\n\n\n\n\n\nüëã Welcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJared Greathouse\n\n\n\n\n\n\n\nShake it to the Max? Using the \\(\\ell_\\infty\\) norm for Synthetic Control Methods\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is An Average?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nJan 4, 2026\n\n\nJared Greathouse\n\n\n\n\n\n\n\nA Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\n\nSep 22, 2025\n\n\nStella Hey Stella\n\n\n\n\n\n\n\nSynthetic Controls for Marketing Experiments\n\n\n\nExperiments\n\nEconometrics\n\n\n\n\n\n\nSep 20, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nLessons from Econometrics, Part 1: Synthetic Controls Aren‚Äôt Black Boxes\n\n\n\n\n\nSep 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Augmented Synthetic Controls\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nAug 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nWhat is a Synthetic Control?\n\n\n\nEconometrics\n\nCausal Inference\n\n\n\n\n\n\nAug 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Synthetic Historical Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 17, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nThe Iterative Synthetic Control Method\n\n\n\nEconometrics\n\n\n\n\n\n\nJul 9, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nMay 19, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls Do Not Care What Your Donors Are. So Why Do You?\n\n\n\nEconometric Theory\n\n\n\n\n\n\nMay 12, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Control Methods for Personalized Causal Inference\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nSynthetic Controls With More Than One Outcome\n\n\n\nCausal Inference\n\nEconometrics\n\n\n\n\n\n\nApr 16, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nForward Selected Synthetic Control\n\n\n\nMachine Learning\n\nEconometrics\n\n\n\n\n\n\nApr 2, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Github Actions\n\n\n\nGithub\n\nAutomation\n\n\n\n\n\n\nJan 31, 2025\n\n\nJared Greathouse\n\n\n\n\n\n\n\nData Science for Policy Analysts: A Simple Introduction to Web Scraping\n\n\n\nWeb Scraping\n\nPython\n\n\n\n\n\n\nJan 29, 2025\n\n\nJared Greathouse\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "shc.html",
    "href": "shc.html",
    "title": "The Synthetic Historical Control Method",
    "section": "",
    "text": "Oftentimes, we struggle with picking a donor pool for SCMs due to spillovers or the event being so massive that it is hard to argue for clean donors in principle. This blog post shows you one way we can get around that problem via using the synthetic historical control method. It is a flavor of synthetic controls, as the name suggests."
  },
  {
    "objectID": "shc.html#notations",
    "href": "shc.html#notations",
    "title": "The Synthetic Historical Control Method",
    "section": "Notations",
    "text": "Notations\nThe notation for this method (as it is presented in the paper) makes my head hurt, so I will simply quote directly from my paper that currently uses this method in hopes that I can spell it out clearly.\nLet \\(\\mathbb{R}\\) denote the set of real numbers, and let \\(\\|\\cdot\\|_1\\) denote the usual \\(\\ell_1\\) vector norm. Throughout, I denote sets using calligraphic letters (e.g., \\(\\mathcal{T}, \\mathcal{S}, \\mathcal{N}\\)) for clarity. Scalars are represented using plain lowercase letters (e.g., \\(h, t, n, m\\)), and matrices are denoted by bold uppercase letters (e.g., \\(\\mathbf{X}, \\mathbf{W}, \\mathbf{Y}\\)). Given a vector \\(\\mathbf{x} = (x_1, x_2, \\dots, x_T)^\\top \\in \\mathbb{R}^T\\) and an index set \\(\\mathcal{I} \\subseteq \\{1, \\dots, T\\}\\), I write \\(\\mathbf{x}_{\\mathcal{I}} \\coloneqq (x_t)_{t \\in \\mathcal{I}} \\in \\mathbb{R}^{|\\mathcal{I}|}\\) to denote the subvector of \\(\\mathbf{x}\\) corresponding to indices in \\(\\mathcal{I}\\), preserving their original order.\nLet \\(\\mathcal{T} = \\{1, 2, \\dots, T\\}\\) index time. Define the pre-treatment period as \\(\\mathcal{T}_1 \\coloneqq \\{t \\in \\mathcal{T} : t \\leq T_0\\}\\) and the post-treatment period as \\(\\mathcal{T}_2 \\coloneqq \\{t \\in \\mathcal{T} : t &gt; T_0\\}\\). The number of post-treatment periods is \\(n = T - T_0\\). Let \\(m \\in \\mathbb{N}\\) denote the evaluation window length, i.e., the number of periods used to construct the SHC match. Define the evaluation period as the final \\(m\\) months of the pre-treatment period:\n\\[\n\\mathcal{T}_{\\text{eval}} \\coloneqq \\{T_0 - m + 1, \\dots, T_0\\} \\subset \\mathcal{T}_1.\n\\]\nLet \\(\\mathbf{y} = (y_1, y_2, \\dots, y_T)^\\top \\in \\mathbb{R}^T\\) denote the observed outcome vector for the treated unit. Define the pre-treatment outcome vector \\(\\mathbf{y}_{\\text{pre}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_1} \\in \\mathbb{R}^{T_0}\\), the evaluation subvector \\(\\mathbf{y}_{\\text{eval}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_{\\text{eval}}} \\in \\mathbb{R}^m\\), and the post-treatment outcome vector \\(\\mathbf{y}_{\\text{post}} \\coloneqq \\mathbf{y}_{\\mathcal{T}_2} \\in \\mathbb{R}^n\\). The evaluation subvector \\(\\mathbf{y}_{\\text{eval}}\\) is the target pattern to be reconstructed using convex combinations of earlier segments from the pre-treatment period.\nTo reconstruct the evaluation window, we compare it against earlier segments of the treated unit‚Äôs own pre-treatment trajectory. Define the donor matrix \\(\\widetilde{\\mathbf{Y}}_{\\text{pre}} \\in \\mathbb{R}^{m \\times N}\\) as a collection of \\(N\\) overlapping length-\\(m\\) subvectors extracted from the smoothed pre-treatment series. Each column is a contiguous subvector of the form \\(\\widetilde{\\mathbf{y}}_{[i, i+m-1]}\\), with \\(i\\) ranging over eligible start points in \\(\\mathcal{T}_1\\) that precede the evaluation window."
  },
  {
    "objectID": "shc.html#choosing-our-historical-segments",
    "href": "shc.html#choosing-our-historical-segments",
    "title": "The Synthetic Historical Control Method",
    "section": "Choosing Our Historical Segments",
    "text": "Choosing Our Historical Segments\nTo avoid using too many donor segments and overfit, SHC implements a forward selection algorithm. The authors in the paper sort of choose an arbitrary number of donors to use. They find that 27 donor segments about does the job, in practice. However, I did not like that they wanted to use a stepwise method with no stopping rule, since this just means that the size of the donor pool we use is kind of arbitrary. So, I write my own forward selection method. Starting with an empty set, it adds one donor at a time, each time choosing the segment that most reduces the in-sample MSE.\n\\[\n\\mathcal{S}_j = \\mathcal{S}_{j-1} \\cup \\left\\{ \\underset{i \\in \\mathcal{N} \\setminus \\mathcal{S}_{j-1}}{\\operatorname*{argmin}} \\left\\| \\widetilde{\\mathbf{y}}_{\\text{eval}} - \\widetilde{\\mathbf{Y}}_{\\text{pre}}^{(\\mathcal{S}_{j-1} \\cup \\{i\\})} \\mathbf{w}^{(j)} \\right\\|_2^2  + \\varsigma \\left\\| \\mathbf{C}_0^\\top \\mathbf{w} \\right\\|_2^2 \\right\\}, \\quad \\mathcal{S}_0 = \\emptyset,\n\\]\nTo choose when to stop adding donors, we compute a modified BIC at each step:\n\\[\n\\text{BIC}(j) = m \\cdot \\log\\left( \\text{MSE}_j \\right) + \\lambda j,\n\\]\nwhere \\(\\text{MSE}_j\\) is the in-sample mean squared error using \\(j\\) donors, and \\(\\lambda = \\log(m)\\). The algorithm stops when BIC increases for two steps in a row. Presumably, we could use a better one, so if you have suggestions let me know. I borrowed this idea from the forward selected PDA approach."
  },
  {
    "objectID": "fasc.html",
    "href": "fasc.html",
    "title": "Forward Augmented Synthetic Controls",
    "section": "",
    "text": "Synthetic Control Methods (SCM) is a widely used framework for estimating causal effects when randomized experiments are not feasible. At its core, SCM constructs a weighted average of control (donor) units to approximate the treated unit‚Äôs pre-treatment trajectory. The goal is to find an in-sample/pre‚Äìtreatment average of controls that closely mirrors the treated unit before the intervention.\nMuch of the method‚Äôs credibility hinges on the quality of this pre-treatment fit. Econometricians regularly warn that poor pre-treatment fit undermines the validity of SCM estimates. Even if the optimization problem is formally well-posed, poor alignment between the treated unit and its in-sample match can lead to substantial bias. The intuition is straightforward: if similar units are assumed to behave similarly, a control group that fails to mimic the treated unit before treatment is unlikely to produce a credible counterfactual afterward. Just as important as pre-treatment fit is the composition of the donor pool. Including irrelevant or poorly matched units, or omitting relevant ones, can distort the synthetic weights and lead to misleading inferences. But how should the donor pool be chosen?\nOne increasingly popular solution to the imperfect match is the Augmented Synthetic Control Method (ASCM), known in industry through Meta‚Äôs GeoLift library. Shops like Recast use it, and data scientists such as Mandy Liu and Svet Semov have helped bring it to applied audiences.\nMethods for donor pool selection have also received attention. In fact, this is part of what makes GeoLift so popular: it attempts to identify the most similar markets to a treated group before the intervention. In academic settings, approaches like forward selection, and even random forests have been proposed to automate or guide the choice of appropriate donors.\nBut what if we can do better?\nIn previous posts, I‚Äôve written about donor selection strategies and how to handle imperfect pre-treatment fit. In this post, I introduce a synthesis of both: the Forward Augmented Synthetic Control estimator. By combining forward selection with a bias correction step, I show that we can reduce in-sample risk relative to the standard ASCM and Forward SCM alone. This approach is illustrated using two popular SCM case studies: the Kansas tax cut experiment and California‚Äôs Proposition 99.\n\n\nLet \\(\\mathbb{R}\\) denote the set of real numbers. Calligraphic letters, such as \\(\\mathcal{S}\\), represent discrete sets with cardinality \\(S = |\\mathcal{S}|\\). Let \\(j \\in \\mathbb{N}\\) index a total of \\(N\\) units and \\(t \\in \\mathbb{N}\\) index time. Unit \\(j=1\\) is the treated unit, with the set of control units \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}\\) of cardinality \\(N_0\\). The pre-treatment period is \\(\\mathcal{T}_1 = \\{ t \\in \\mathbb{N} : t \\le T_0 \\}\\) and the post-treatment period is \\(\\mathcal{T}_2 = \\{ t \\in \\mathbb{N} : t &gt; T_0 \\}\\). The observed outcome for unit \\(j\\) at time \\(t\\) is \\(y_{jt}\\), and the outcome vector for unit \\(j\\) is \\(\\mathbf{y}_j = (y_{j1}, \\dots, y_{jT})^\\top \\in \\mathbb{R}^T\\). The treated unit‚Äôs outcome vector is \\(\\mathbf{y}_1\\), and the donor matrix is \\(\\mathbf{Y}_0 = \\begin{bmatrix} \\mathbf{y}_j \\end{bmatrix}_{j \\in \\mathcal{N}_0} \\in \\mathbb{R}^{T \\times N_0}\\).\n\n\nI frequently consider linear combinations of donor outcomes. The convex hull of the donor vectors is\n\\[\n\\operatorname{conv}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\},\n\\]\nand the affine hull is\n\\[\n\\operatorname{aff}(\\{\\mathbf{y}_j\\}_{j \\in \\mathcal{N}_0})\n= \\Big\\{ \\mathbf{Y}_0 \\mathbf{w} \\;\\Big|\\; \\mathbf{w} \\in \\mathbb{R}^{N_0}, \\;\\mathbf{1}^\\top \\mathbf{w} = 1 \\Big\\}.\n\\]\nWhile both involve weighted averages of donors, the convex hull restricts weights to be non-negative, whereas the affine hull allows negative weights and extrapolation beyond the convex hull.\nThe corresponding sets of feasible weights are\n\\[\n\\mathcal{W}_{\\mathrm{conv}} = \\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}, \\quad\n\\mathcal{W}_{\\mathrm{aff}} = \\{ \\mathbf{w} \\in \\mathbb{R}^{N_0} \\mid \\mathbf{1}^\\top \\mathbf{w} = 1 \\}.\n\\]\n\n\n\n\n\n\n\n\n\nIn two dimensions, the convex hull of two donor points is the line segment connecting them. It represents all convex combinations of the donors, where the weights are nonnegative and sum to one. In the figure above, this is shown as the black line segment, and the green point lies within it because its weights are both positive and sum to one. By contrast, the affine hull is the entire infinite line passing through the donors.\nAffine combinations also require weights to sum to one, but they may be negative, which allows extrapolation beyond the segment (though practically we may force the weights to be between negative one and positive one, what we‚Äôd call a kind of polytope). The dashed gray line in the figure illustrates the affine hull, and the red point lies outside the convex segment but on this line because one weight is negative.\nAny point not on this line at all cannot be expressed as an affine combination, as shown by the purple point. The key difference is that the convex hull is bounded and interpolative, while the affine hull is unbounded and permits extrapolation arbitrarily far from the observed donor points."
  },
  {
    "objectID": "fasc.html#forward-selection-scm-fscm",
    "href": "fasc.html#forward-selection-scm-fscm",
    "title": "Forward Augmented Synthetic Controls",
    "section": "Forward Selection SCM (FSCM)",
    "text": "Forward Selection SCM (FSCM)\nFSCM constructs a synthetic control iteratively by adding one donor at a time. Starting from the empty set \\(\\mathcal{S} = \\emptyset\\), at each iteration the algorithm considers each candidate donor \\(j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}\\) and forms \\(\\mathcal{S}^\\prime = \\mathcal{S} \\cup \\{j\\}\\). For each candidate set \\(\\mathcal{S}^\\prime\\), it solves the restricted SCM problem over the donor submatrix \\(\\mathbf{Y}_0^{\\mathcal{S}^\\prime}\\), defined as the columns of \\(\\mathbf{Y}_0\\) corresponding to units in \\(\\mathcal{S}^\\prime\\):\n\\[\n\\mathbf{w}_{\\mathcal{S}^\\prime}^\\ast =\n\\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime)}{\\operatorname*{argmin}} \\;\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, \\mathcal{S}^\\prime} \\mathbf{w} \\right\\|_2^2\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S}^\\prime) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|\\mathcal{S}^\\prime|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe donor selected at each iteration is\n\\[\nj^\\ast = \\underset{j \\in \\mathcal{N}_0 \\setminus \\mathcal{S}}{\\operatorname*{argmin}} \\;\n\\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(\\mathcal{S} \\cup \\{j\\})}\n\\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1}{}_{\\mathcal{S} \\cup \\{j\\}} \\mathbf{w} \\right\\|_2^2\n\\]\nand it is then added to the selected set by updating\n\\[\n\\mathcal{S} \\leftarrow \\mathcal{S} \\cup \\{j^\\ast\\}.\n\\]\nThe donor selection process proceeds sequentially according to this forward selection rule. In principle, the procedure may continue until all donor units have been exhausted, but in practice analysts use stopping rules to terminate the selection at a sensible point.\n\nExhaustive Search\nIn the exhaustive option, selection proceeds until the pre-treatment mean squared error (MSE) is minimized over all candidate donor subsets. Let \\(\\mathcal{P}(\\mathcal{N}_0)\\) denote the power set of donor indices, and for any \\(S \\subseteq \\mathcal{N}_0\\), define the within‚Äìpre-treatment fit as\n\\[\n\\text{MSE}(S) = \\frac{1}{T_0} \\min_{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S)} \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^S \\mathbf{w} \\right\\|_2^2,\n\\]\nwhere\n\\[\n\\mathcal{W}_{\\mathrm{conv}}(S) = \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{|S|} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}.\n\\]\nThe exhaustive-search estimator iestimates \\(\\sum_{i=0}^{k-1} (N_0 - i) = k N_0 - \\frac{k(k-1)}{2}\\) total models. It chooses\n\\[\nS^\\ast = \\underset{S \\in \\mathcal{P}(\\mathcal{N}_0)}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1,S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nThis means we continue until all donors are eventually selected.\n\n\nInformation Criteria, per Shi and Huang, 2023\nAnother option is to rely on a model selection criterion, such as the modified BIC (mBIC), which balances pre-treatment fit with model complexity. The modified BIC for a selected set \\(S\\) of donor units is defined as\n\\[\n\\text{mBIC}(S) = T_0 \\cdot \\log(\\text{MSE}) + |S| \\cdot \\log(T_0),\n\\]\nwhere\n\\[\n\\text{MSE} = \\frac{1}{T_0} \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1, S} \\mathbf{w}_S \\right\\|_2^2.\n\\]\nThe forward selection procedure may terminate when adding a new donor increases the penalized error:\n\\[\n\\text{mBIC}(S \\cup \\{j\\}) &gt; \\text{mBIC}(S).\n\\]\n\n\nDonor Pool Cap\nA final practical option is to impose an upper bound on the number of selected donors. When a cardinality cap is imposed, let \\(p \\in (0,1]\\) and \\(K = \\lfloor p N_0 \\rfloor\\), so that the search is restricted to subsets \\(S \\subseteq \\mathcal{N}_0\\) with \\(|S| \\le K\\). The corresponding FSCM estimator chooses\n\\[\nS^\\ast = \\underset{\\substack{S \\subseteq \\mathcal{N}_0 \\\\ |S| \\le K}}{\\operatorname*{argmin}} \\; \\text{MSE}(S),\n\\qquad\n\\mathbf{w}_{S^\\ast}^\\ast = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}(S^\\ast)}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1 - \\mathbf{Y}_0^{S^\\ast} \\mathbf{w} \\right\\|_2^2.\n\\]\nWhichever variant of the forward selection algorithm is used‚Äîexhaustive search, model selection, or cardinality constraint‚Äîthe result is a selected subset of donors \\(S^\\ast \\subseteq \\mathcal{N}_0\\) and a corresponding optimal weight vector \\(\\mathbf{w}_{S^\\ast}^\\ast \\in \\mathbb{R}^{|S^\\ast|}\\). To unify notation and simplify presentation going forward, we define the final forward-selected weights by embedding this sparse solution into the full donor space:\n\\[\nw_j^{\\mathrm{FSCM}} =\n\\begin{cases}\n\\left(\\mathbf{w}_{S^\\ast}^\\ast\\right)_j, & \\text{if } j \\in S^\\ast, \\\\\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nEquivalently, let \\(\\mathbf{w}^{\\mathrm{FSCM}} \\in \\mathbb{R}^{N_0}\\) be the zero-padded extension of \\(\\mathbf{w}_{S^\\ast}^\\ast\\), such that it assigns zero weight to all donors not selected by FSCM. This implementation provides flexibility in practice: exhaustive evaluation is feasible in low-dimensional settings, while early stopping or a capped selection is recommended in high-dimensional applications. The algorithm in mlsynth issues a warning if the donor pool contains 200 or more units."
  },
  {
    "objectID": "fasc.html#the-augmented-synthetic-control-estimator",
    "href": "fasc.html#the-augmented-synthetic-control-estimator",
    "title": "Forward Augmented Synthetic Controls",
    "section": "The Augmented Synthetic Control Estimator",
    "text": "The Augmented Synthetic Control Estimator\nBuilding on this baseline formulation, the ASCM introduces a regularization term that penalizes deviations of the weight vector from a reference or initial weight vector, \\(\\mathbf{w}_0\\). The augmented objective can be written as\n\\[\n\\mathbf{w}^{\\mathrm{aug}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\left\\| \\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}} \\right\\|_2^2.\n\\]\nwhere \\(\\lambda \\ge 0\\) controls the strength of the penalty.\n\n\nAbout that lambda‚Ä¶\n\n\n\n\n\n\n\nNote\n\n\n\nOne may ask ‚ÄúJared, why did you include the penalty on the weight deviation term instead of the fit term, as Ben-Michael and co. do in Equation 18 of their paper?‚Äù Here‚Äôs why.\nIn ASCM, the placement of the regularization parameter \\(\\lambda\\) determines how the estimator balances pre-treatment fit and fidelity to the original SCM weights. Their formulation minimizes:\n\\[\n\\mathbf{w}^\\ast_{\\text{alt}} = \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\lambda \\|\\mathbf{y}_1^{\\mathcal{T}_1}- \\mathbf{Y}_{0}^{\\mathcal{T}_1}\\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\|\\mathbf{w}^{\\mathrm{aug}}-\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nwhile ours solves:\n\\[\n\\mathbf{w}^{\\mathrm{aug}}= \\underset{\\mathbf{w}^{\\mathrm{aug}} \\in \\mathcal{W}_{\\mathrm{aff}}}{\\operatorname*{argmin}} \\; \\|\\mathbf{y}_1^{\\mathcal{T}_1} - \\mathbf{Y}_0^{\\mathcal{T}_1} \\mathbf{w}^{\\mathrm{aug}}\\|_2^2 + \\lambda \\|\\mathbf{w}^{\\mathrm{aug}} -\\mathbf{w}^{\\mathrm{SCM}}\\|_2^2\n\\]\nIt turns out these are mathematically equivalent under a simple reparameterization. If we define \\(\\lambda_{\\text{alt}} = \\frac{1}{\\lambda_{\\text{aug}}}\\), then both objectives yield the same solution. This follows directly from the first-order conditions of each problem, which differ only by a scaling of the Lagrange multiplier. So in truth, it‚Äôs just a matter of which interpretation you find more natural.\nI personally prefer our formulation because as \\(\\lambda \\to \\infty\\), the penalty on deviation dominates, and \\(\\mathbf{w}^\\ast_{\\text{aug}}\\) collapses to the projection of \\(\\mathbf{w}^{\\mathrm{SCM}}\\) onto the affine constraint. In other words, when the original FSCM fit is already good, we want to stick close to it. Conversely, as \\(\\lambda \\to 0\\), the regularization term disappears and the solution becomes the best-fitting affine combination of the donor units, completely unconstrained by the initial weights. That‚Äôs appropriate when the original fit is poor and we‚Äôre willing to learn something new (although this is probably going to be uncommon in practice). So while the math is equivalent, the perspective isn‚Äôt. I find it much more natural to think of \\(\\lambda\\) as controlling how much I ‚Äútrust‚Äù the prior weights. And that‚Äôs easier to reason about when \\(\\lambda\\) is attached to the deviation term.\n\n\n\nThe intuition here is pretty simple. If the SCM weights are already giving us good pre-treatment fit, then there is little incentive to extrapolate away from the original (F)SCM solution. However, if there‚Äôs need for better fit, then we will extrapolate away from the convex hull solution. In practice, Ben-Michael and co advocate for choosing lambda via cross validation."
  },
  {
    "objectID": "fasc.html#conformal-prediction-intervals",
    "href": "fasc.html#conformal-prediction-intervals",
    "title": "Forward Augmented Synthetic Controls",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\nTo quantify uncertainty around the estimated counterfactual trajectory, we apply conformal prediction intervals based on block-permuted pre-treatment residuals. These intervals are distribution-free, require no assumptions about the data-generating process, and provide valid finite-sample marginal coverage. Let \\(\\hat{y}_t^{\\text{cf}}\\) denote the estimated counterfactual outcome at time \\(t\\), and let \\(y_t^{\\text{obs}}\\) be the observed outcome. We begin by computing residuals for all time periods:\n\\[\n\\varepsilon_t = y_t^{\\text{obs}} - \\hat{y}_t^{\\text{cf}}.\n\\]\nWe then construct a conformal score by calculating the mean absolute residual over the post-treatment period. To simulate the distribution of this score under the null (i.e., assuming no treatment effect), we perform circular block permutations of the residual vector and recompute the same statistic for each shifted version.\nThis yields an empirical distribution of conformal scores under the null. We take the \\((1 - \\alpha)\\) quantile of this distribution as our conformal threshold, denoted \\(q_{1 - \\alpha}\\). To center the interval, we compute the mean residual over the pre-treatment period:\n\\[\n\\bar{\\varepsilon} = \\frac{1}{T_0} \\sum_{t \\leq T_0} \\varepsilon_t.\n\\]\nThe conformal prediction interval for each post-treatment time \\(t &gt; T_0\\) is then given by:\n\\[\n\\left[ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} - q_{1 - \\alpha},\\ \\hat{y}_t^{\\text{cf}} + \\bar{\\varepsilon} + q_{1 - \\alpha} \\right].\n\\]\nThis approach ensures that the prediction intervals account for uncertainty in the counterfactual trajectory while adjusting for systematic bias in the pre-treatment fit. The shaded regions in our figures visualize these conformal intervals."
  },
  {
    "objectID": "nsc.html",
    "href": "nsc.html",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "",
    "text": "Sometimes our outcomes are nonlinear in nature with synthetic control methods, meaning biases can be quite severe if not addressed correctly. This is where the non-linear synthetic control method comes in handy. In this blog post, I present the Python implementation of the method and give an example of how it works using the Proposition 99 dataset."
  },
  {
    "objectID": "nsc.html#optimization-problem",
    "href": "nsc.html#optimization-problem",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Optimization Problem",
    "text": "Optimization Problem\nLet \\(\\mathbf{y}_1^{\\text{pre}} \\in \\mathbb{R}^{T_0}\\) denote the outcome vector for the treated unit over the pre-treatment period \\(\\mathcal{T}_1\\), and let \\(\\mathbf{Y}_0^{\\text{pre}} \\in \\mathbb{R}^{T_0 \\times N_0}\\) be the matrix of pre-treatment outcomes for the control units indexed by \\(\\mathcal{N}_0\\). We seek a weight vector \\(\\mathbf{w} \\in \\mathbb{R}^{N_0}\\) that minimizes the following objective:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{N_0}} \\quad \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}} \\mathbf{w} \\right\\|_2^2 + a \\sum_{j \\in \\mathcal{N}_0} \\delta_j |w_j| + b \\|\\mathbf{w}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}\\|_1 = 1\n\\]\nThe first term is the standard squared reconstruction error. The second term introduces a discrepancy-weighted \\(\\ell_1\\) penalty, where the discrepancy vector \\(\\boldsymbol{\\delta} \\in \\mathbb{R}^{N_0}\\) is defined by:\n\\[\n\\delta_j = \\left\\| \\mathbf{y}_1^{\\text{pre}} - \\mathbf{y}_j^{\\text{pre}} \\right\\|_2, \\quad \\forall j \\in \\mathcal{N}_0\n\\]\nThis term penalizes control units that differ more from the treated unit in pre-treatment trends. The final term is a Tikhonov regularization penalty that shrinks the weight vector toward zero to improve stability. The parameters \\(a &gt; 0\\) and \\(b &gt; 0\\) control the strength of the \\(\\ell_1\\) and \\(\\ell_2\\) penalties, respectively. The constraint \\(\\|\\mathbf{w}\\|_1 = 1\\) retains the SC to the probability simplex \\(\\Delta^{N_0 - 1}\\), forming an affine combination of the control units."
  },
  {
    "objectID": "nsc.html#cross-validation-to-tune-a-and-b",
    "href": "nsc.html#cross-validation-to-tune-a-and-b",
    "title": "Synthetic Controls With Non-Linear Outcome Trends: A Principled Approach to Extrapolation",
    "section": "Cross-Validation to Tune \\(a\\) and \\(b\\)",
    "text": "Cross-Validation to Tune \\(a\\) and \\(b\\)\nTo select optimal regularization parameters \\((a, b)\\), we perform \\(k\\)-fold cross-validation using a pseudo-treated framework. Each control unit \\(j \\in \\mathcal{N}_0\\) is treated as if it were the treated unit, and we aim to reconstruct \\(\\mathbf{y}_j^{\\text{pre}}\\) from the remaining controls. For a given pseudo-treated unit \\(j\\), let \\(\\mathcal{N}_0^{(-j)} = \\mathcal{N}_0 \\setminus \\{j\\}\\) and define the donor matrix \\(\\mathbf{Y}_0^{\\text{pre}, (-j)}\\) by removing column \\(j\\) from \\(\\mathbf{Y}_0^{\\text{pre}}\\).\nFor each fold and pseudo-treated unit, we solve:\n\\[\n\\min_{\\mathbf{w}^{(j)} \\in \\mathbb{R}^{N_0 - 1}} \\quad \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)} \\right\\|_2^2 + a \\sum_{k \\in \\mathcal{N}_0^{(-j)}} \\delta_k^{(j)} |w_k^{(j)}| + b \\|\\mathbf{w}^{(j)}\\|_2^2 \\quad \\text{s.t.} \\quad \\|\\mathbf{w}^{(j)}\\|_1 = 1\n\\]\nwhere\n\\[\n\\delta_k^{(j)} = \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\mathbf{y}_k^{\\text{pre}} \\right\\|_2, \\quad \\forall k \\in \\mathcal{N}_0^{(-j)}\n\\]\nLet \\(\\widehat{\\mathbf{y}}_j^{\\text{pre}} = \\mathbf{Y}_0^{\\text{pre}, (-j)} \\mathbf{w}^{(j)}\\) denote the predicted outcome. The validation error for unit \\(j\\) is given by:\n\\[\n\\text{MSE}^{(j)} = T_0^{-1} \\left\\| \\mathbf{y}_j^{\\text{pre}} - \\widehat{\\mathbf{y}}_j^{\\text{pre}} \\right\\|_2^2\n\\]\nThe cross-validated error is averaged over all pseudo-treated units and all folds. We then select:\n\\[\n(a^\\ast, b^\\ast) = \\arg\\min_{a \\in \\mathcal{A},\\, b \\in \\mathcal{B}} \\; \\text{CVError}(a, b)\n\\]\nHere, \\(k\\) indexes the donor units in the pseudo-treated optimization problem. \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are grids of candidate values. In practice, I employ sklearn‚Äôs cross-validation utilities to accelerate computation. I had to go rogue from the original approach because on my end it was computationally intensive. This modification seems to perform comparably while dramatically improving runtime. So while the empirical results will not be the same, they are about what we‚Äôd expect."
  },
  {
    "objectID": "stella1.html",
    "href": "stella1.html",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "A comprehensive practitioner‚Äôs guide to implementing Weighted Synthetic Control methods for marketing incrementality testing.\n\n\n\nWeighted Synthetic Control Methods for Incrementality Testing\n\n\n\n\nWeighted Synthetic Control (WSC) constructs a counterfactual for a treated region as a convex combination of untreated donor regions that closely replicates the treated region‚Äôs pre-intervention trajectory. In geo-based incrementality testing, WSC typically yields superior pre-intervention fit and reduced variance compared to one-to-one matched geographies or equal-weight Difference-in-Differences (DiD), particularly when treating a limited number of regions. This comprehensive guide presents an end-to-end practitioner workflow encompassing donor pool construction, constrained optimization with regularization, rigorous holdout validation, placebo-based statistical inference, interval estimation, and business metrics calculation including lift and iROAS. We position WSC within the broader landscape of modern causal inference methods (Augmented SCM, Generalized SCM, Synthetic DiD, BSTS), provide clear guidance on method selection, describe Stella‚Äôs production implementation, and establish best practices, diagnostic frameworks, and governance protocols essential for credible causal inference.\n\n\n\nIncrementality testing represents a cornerstone of modern marketing analytics when randomized controlled trials are impractical or prohibitively expensive. Weighted Synthetic Control (WSC) addresses this challenge by constructing a synthetic version of the treated unit using optimal combinations of untreated donors, providing a data-driven approach to counterfactual estimation. By leveraging extensive pre-intervention data, WSC absorbs complex temporal patterns including trends, seasonality, and latent confounders that would otherwise bias treatment effect estimates.\nThis guide equips practitioners and data scientists with both theoretical foundations and actionable implementation steps, ensuring WSC is applied with appropriate rigor, transparency, and statistical validity. We emphasize diagnostic procedures, uncertainty quantification, and clear decision frameworks for determining when WSC is‚Äîor is not‚Äîthe optimal methodological choice.\n\n\n\n\n\nIndex time by \\(t \\in \\mathcal{T}\\) and units by \\(i \\in \\mathcal{N}\\), with \\(T\\) total periods and \\(N\\) units. Denote the pre-treatment period as \\(\\mathcal{T}_1 = \\{1, 2, \\dots, T_0\\}, \\quad |\\mathcal{T}_1| = T_1,\\) and the post-treatment period as \\(\\mathcal{T}_2 = \\{T_0+1, \\dots, T\\}, \\quad |\\mathcal{T}_2| = T_2.\\) The treated unit is \\(i=1\\), and the donor pool (control units) is \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}, \\quad |\\mathcal{N}_0| = N_0.\\)\nPotential Outcomes Framework:\n\n\\(y_{it}^{1}\\) and \\(y_{it}^{0}\\) denote potential outcomes under treatment and control conditions\nWe observe \\(y_{it} = y_{it}^{0}\\) for all units when \\(t \\in \\mathcal{T}_1\\)\nFor the treated unit while \\(t \\in \\mathcal{T}_2\\), we observe \\(y_{1t}^{1}\\) and do not observe the counterfactual \\(y_{1t}^{0}\\) , or how the outcome would have evolved had the intervention not happened.\n\n\n\n\nWe estimate the unobserved counterfactual \\(\\widehat{y}_{1t}^{0}\\) via a weighted combination of donors:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\mathrm{SCM}} &= \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\; \\left\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\right\\|_2^2, \\\\\n\\mathcal{W}_{\\mathrm{conv}} &= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\n\\end{aligned}\n\\]\nThe treatment effect for the treated unit at post-treatment time \\(t\\) is:\n\\[\n\\tau_{1t} = y_{1t}^{1} -\\widehat{y}_{1t}^{0}, \\quad t \\in \\mathcal{T}_2\n\\]\n\n\n\n\nThe synthetic control method originated with Abadie and Gardeazabal (2003) in their seminal analysis of economic costs of conflict in the Basque Country. Abadie, Diamond, and Hainmueller (2010) formalized the statistical framework through their influential California tobacco control study, establishing the canonical SCM implementation.\nRecent methodological advances include:\n\nAugmented SCM (Ben-Michael et al., 2021): Incorporates regression adjustment for bias correction\nGeneralized SCM (Xu, 2017): Extends to multiple treated units with interactive fixed effects\nSynthetic Difference-in-Differences (Arkhangelsky et al., 2021): Combines SCM and DiD advantages\nBayesian Structural Time Series (Brodersen et al., 2015): Provides probabilistic counterfactual forecasting\n\nThese methods have gained widespread adoption across policy evaluation, health economics, and increasingly in marketing incrementality measurement, particularly for geo-experimental designs with limited treatment units.\n\n\n\n\n\nCore Activities:\n\nDefine treatment units, outcome metrics, and intervention timing\nAssemble comprehensive candidate donor pool with complete panel data\nPre-register donor exclusion criteria and analytical specifications\nEnsure measurement consistency across units and time periods\nConduct power analysis to determine minimum detectable effect sizes\n\nCritical Considerations:\n\nTreatment assignment should be exogenous to potential outcomes\nPre-intervention period must be sufficiently long to capture seasonal cycles\nOutcome measurement must be consistent across all units\n\n\n\n\nPrimary Screening Criteria:\n\nCorrelation filtering: Exclude donors with pre-period outcome correlation below threshold (typically \\(r &lt; 0.3\\))\nSeasonality alignment: Verify similar cyclical patterns using spectral analysis\nStructural stability: Test for breaks using Chow tests or similar procedures\nContamination assessment: Remove units with direct or indirect treatment exposure\nGeographic considerations: Account for spatial spillovers and media market overlap\n\nAdvanced Screening: Systematic evaluation includes correlation analysis, seasonal pattern comparison, and structural stability testing to ensure donor quality and relevance.\n\n\n\nFeature Selection Strategy:\n\nPrimary features: Multiple lags of outcome variable spanning complete seasonal cycles\nAuxiliary covariates: Demographic or economic variables only when measurement quality is high\nTemporal aggregations: Consider moving averages to smooth high-frequency noise\n\nStandardization Protocol:\n\nScale all features using pre-period statistics only\nApply z-score normalization: \\((X - \\mu_{pre}) / \\sigma_{pre}\\)\nDocument all transformations for reproducibility\n\n\n\n\nObjective Function:\n\\[\n\\min_w \\|\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w}\\|_\\mathbf{V}^2 + \\lambda R(w)\n\\]\nRegularization Options:\n\nEntropy penalty: \\(R(w) = \\sum_i w_i \\log w_i\\) (promotes weight dispersion)\nWeight caps: \\(w_i \\leq w_{max}\\) (prevents over-concentration)\nElastic net: Combination of \\(\\ell_1\\) and \\(\\ell_2\\) penalties on weights\n\nImplementation: The weight optimization involves solving a constrained optimization problem that minimizes the discrepancy between treated and synthetic units while adhering to convexity constraints.\n\n\n\nValidation Protocol:\n\nReserve final 20-25% of pre-intervention period as holdout\nTrain synthetic control on early pre-period data only\nEvaluate prediction accuracy on holdout using multiple metrics:\n\nMean Absolute Percentage Error (MAPE)\nRoot Mean Square Error (RMSE)\nR-squared coefficient of determination\n\n\nQuality Gates (Data-Frequency Dependent):\n\n\n\nQuality Gates\n\n\nThese thresholds derive from analysis of prediction accuracy across 200+ campaigns, calibrated to achieve 80% power for detecting 5% effects.\nRemediation Strategies: If holdout validation fails:\n\nExpand donor pool or modify screening criteria\nExtend pre-intervention period\nAdjust regularization parameters\nConsider alternative methodological approaches\n\n\n\n\nAverage Treatment Effect Calculation:\n\\[\n\\text{ATT} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} \\tau_{1t}\n= \\frac{1}{T_2} \\sum_{t \\in \\mathcal{T}_2} \\left( y_{1t}^{1} - \\widehat{y}_{1t}^{0} \\right)\n.\n\\]\nBusiness Metric Derivation:\n\nLift calculation: \\(\\text{Lift} = \\frac{\\sum_{t\\in \\mathcal{T}_2} \\widehat{\\tau}_t}{\\sum_{t\\in \\mathcal{T}_2} \\widehat{y}_{1t}^{0}} \\times 100\\%\\)\nIncremental ROAS: \\(\\text{iROAS} = \\frac{\\text{Incremental Revenue}}{\\text{Media Spend}}\\)\nNet Present Value: Account for time value when effects persist\n\n\n\n\nPlacebo Testing Framework:\nIn-Space Placebos:\n\nApply identical methodology to each donor unit\nGenerate null distribution of pseudo-treatment effects\nCalculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\)\n\nIn-Time Placebos:\n\nSimulate treatment at various pre-intervention dates\nAssess whether observed effect magnitude is historically unusual\n\nAlternative Inference Methods:\n\nBootstrap resampling: Via Interactive Fixed Effects (Generalized SCM)\nBayesian credible intervals: Using BSTS or Bayesian SCM variants\nRobust standard errors: Account for serial correlation and heteroskedasticity\n\n\n\n\nCore Diagnostics:\nFor nonnegative weights \\(\\mathbf{w} \\ge 0\\), the number of donors with nonzero weight is\n\\[\n\\|\\mathbf{w}\\|_0 = |\\{i \\in \\mathcal{N}_0 : w_i &gt; 0\\}|.\n\\]\nThen your weight concentration section could read:\nWeight Concentration:\n\nMonitor effective number of donors: \\(\\text{EN} = 1 / \\sum_i w_i^2\\)\nTrack the number of active donors: \\(\\|\\mathbf{w}\\|_0\\)\nFlag potential overfitting if either EN is low (e.g., EN &lt; 3) or \\(\\|\\mathbf{w}\\|_0&lt;3\\) is very small, indicating that only a few donors dominate the synthetic control.\n\nOverlap Assessment:\n\nVerify treated unit lies within convex hull of donors\nUse Mahalanobis distance to quantify similarity\n\nSensitivity Testing:\n\nLeave-one-out analysis for influential donors\nRobustness to regularization parameter choices\nAlternative specification sensitivity\n\nInterference Detection:\n\nMonitor donor unit outcomes for anomalous patterns post-treatment\nGeographic buffer analysis for spillover effects\nCross-correlation tests between treated and donor regions\n\n\n\n\n\n\n\nTraditional asymptotic inference often fails with single treated units, necessitating alternative approaches:\nPermutation-Based Inference: Generate empirical null distribution via placebo tests (Abadie et al., 2010). Calculate exact p-values under sharp null hypothesis, which is robust to distributional assumptions but requires adequate donor pool size.\nBootstrap Methods: Interactive fixed effects framework enables uncertainty quantification (Xu, 2017), particularly effective with multiple treated units or staggered interventions. This approach accounts for both sampling and optimization uncertainty.\nBayesian Approaches: Full posterior distributions over counterfactual paths provide natural incorporation of prior information (Brodersen et al., 2015), though results can be sensitive to prior specification choices.\n\n\n\nConvex Hull Violations: If the treated unit lies outside the convex hull of donors, extrapolation bias can be substantial (Abadie et al., 2010). Solutions include expanding donor pool geographically or temporally, applying Augmented SCM for bias correction (Ben-Michael et al., 2021), or using alternative methods such as BSTS or parametric models.\nInsufficient Pre-Intervention Data: Short pre-periods lead to unstable weight estimation, poor seasonal adjustment, and coarse placebo test distributions (Abadie et al., 2010). Minimum recommended periods should span multiple complete seasonal cycles for reliable estimation.\nSpillover Effects: Violation of SUTVA (Stable Unit Treatment Value Assumption) can occur through geographic spillovers between treated and donor regions, media market overlap causing indirect treatment exposure, or supply chain and competitive response effects (Abadie et al., 2010).\nTemporal Confounding: External shocks coinciding with treatment timing, structural breaks affecting units differentially, or calendar events creating spurious correlations can bias treatment effect estimates (Ben-Michael et al., 2021).\n\n\n\n\n\n\n\nDecision Tree for Method Selection\n\n\nDecision Tree for Method Selection\nIs treatment randomly assigned?\n‚îú‚îÄ Yes ‚Üí Use randomized experiment analysis\n‚îî‚îÄ No ‚Üí Continue\n\nDo you have many (&gt;10) treated units?\n‚îú‚îÄ Yes ‚Üí Consider DiD or Generalized SCM (Xu, 2017)\n‚îî‚îÄ No ‚Üí Continue\n\nIs pre-intervention period long (&gt;50 observations)?\n‚îú‚îÄ No ‚Üí Consider BSTS (Brodersen et al., 2015) or parametric approaches\n‚îî‚îÄ Yes ‚Üí Continue\n\nAre credible donor units available?\n‚îú‚îÄ No ‚Üí Use BSTS or alternative methods\n‚îî‚îÄ Yes ‚Üí WSC is appropriate (Abadie et al., 2010)\n\nDoes synthetic control achieve good pre-fit?\n‚îú‚îÄ Yes ‚Üí Standard WSC\n‚îî‚îÄ No ‚Üí Consider Augmented SCM (Ben-Michael et al., 2021)\n\n\n\n\n\n\nStella‚Äôs Innovations\n\n\n\n\nCorrelation-First Filtering: Stella‚Äôs system automatically processes candidate donor geographies through multi-stage screening:\n\nOutcome correlation analysis: Pearson correlation with treated unit‚Äôs pre-intervention KPI history\nSeasonal pattern alignment: Fourier transform comparison of cyclical components\nStructural break detection: CUSUM and Zivot-Andrews tests for stability\nContamination screening: Cross-reference with media delivery logs and geographic buffers\n\nQuality Assurance:\n\nDocumented exclusion rationale for each rejected donor\nSensitivity analysis for correlation thresholds\nVisual dashboard for analyst review and override capabilities\n\n\n\n\nBefore any business decision or effect reporting, Stella enforces holdout validation requirements:\nImplementation:\n\n80/20 split of pre-intervention period (training/holdout)\nMulti-metric evaluation: \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\), no systematic bias\nFailed validation triggers automatic model respecification workflow\n\nEscalation Protocol: Weak holdout performance initiates structured remediation:\n\nDonor pool expansion with relaxed correlation thresholds\nExtended pre-intervention period when available\nAlternative methodological approaches (ASCM, BSTS)\nStatistical power reassessment and test design modification\n\n\n\n\nPrimary Method Stack:\n\nBase WSC: Convex optimization with entropy regularization (Abadie et al., 2010)\nAugmented SCM: Automatic deployment for boundary cases where convex hull distance exceeds threshold (Ben-Michael et al., 2021)\nGeneralized SCM: Bootstrap confidence intervals for formal inference (Xu, 2017)\nBSTS Validation: Parallel Bayesian model for sensitivity analysis (Brodersen et al., 2015)\n\nConsensus Framework:\n\nEffect estimates must be directionally consistent across methods\nConfidence intervals should substantially overlap\nDivergent results trigger deeper diagnostic investigation\n\n\n\n\nSpatial Placebo Testing: Apply identical methodology to each donor unit to generate null distribution of pseudo-treatment effects (Abadie et al., 2010). Calculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\).\nTemporal Placebo Testing: Simulate treatment at various pre-intervention dates to assess whether observed effect magnitude is historically unusual, providing additional validation of causal inference.\nInference Method Selection Framework:\n\nFew donors (&lt;20): Rely primarily on placebo tests with exact p-values\nModerate donors (20-50): Combine placebo tests with bootstrap methods via GSC (Xu, 2017)\nMany donors (&gt;50): Bootstrap confidence intervals become reliable; consider Bayesian approaches for full uncertainty quantification (Brodersen et al., 2015)\n\nCommon Inference Limitations:\n\nPlacebo tests assume exchangeability between treated and donor units\nBootstrap methods require sufficient sample size for asymptotic validity\nBayesian approaches sensitive to prior specification choices\n\n\n\n\nRelationship to Robust Synthetic Control: Building on Robust Synthetic Control methods (Amjad et al., 2018) that address outlier donors through optimization robustness, our approach focuses on ex-ante donor quality assessment. While Robust SC handles poor donors through algorithmic robustness, the Donor Quality Scorecard prevents poor donors from entering the optimization process.\nMulti-Dimensional Quality Assessment:\n\\[\nDQS_i = w_1 \\cdot \\text{Correlation}_i + w_2 \\cdot \\text{Stability}_i + w_3 \\cdot \\text{Seasonality}_i + w_4 \\cdot \\text{Independence}_i\n\\]\nComponent Justifications:\n\nStability Component: Addresses temporal robustness concerns by measuring coefficient of variation in rolling correlations\nSeasonality Component: Captures seasonal relationship consistency, critical for marketing applications\nIndependence Component: Measures partial correlation controlling for common factors, reducing redundancy\n\nMarket-Calibrated Weights:\n\n\n\nMarket-Calibrated Weights\n\n\nAdvantage Over Standard Diagnostics: Traditional approaches rely on post-hoc diagnostics after weight optimization. Our scorecard provides pre-optimization quality gates, preventing computational waste on poor donor sets and improving downstream robustness.\n\n\n\nTraditional holdout validation uses a fixed temporal split, building on rolling-origin validation principles from forecasting literature (Hyndman & Athanasopoulos, 2021). However, standard forecasting approaches assume stationarity, while marketing environments exhibit systematic volatility patterns requiring adaptive holdout periods.\nBeyond Standard Cross-Validation: While forecasting literature extensively covers rolling windows, our contribution addresses market-specific volatility calibration for causal inference contexts where the validation objective differs from pure prediction accuracy.\nTheoretical Foundation: Standard holdout validation assumes stationarity in the relationship between treated and donor units. However, in digital marketing environments, this assumption frequently breaks down due to:\n\nAlgorithm updates on advertising platforms\nChanging consumer behavior patterns\nCompetitive response evolution\nSeasonal drift in cross-unit relationships\n\nMarket Volatility-Adaptive Framework:\n\\[\nT_{\\text{holdout}}^* = \\underset{T_h \\in \\mathcal{T}_\\text{cand}}{\\operatorname*{argmin}} \\;\n\\Big[ \\text{MSPE}_{\\text{holdout}}(T_h) + \\lambda \\, f(\\sigma_{\\text{market}}, T_h) \\Big],\n\\]\nwhere \\(\\mathcal{T}_\\text{cand} \\subseteq \\mathcal{T}_1\\) is the set of candidate pre-treatment periods to use as holdout and \\(f(\\sigma_{\\text{market}}, T_h)\\) penalizes holdout periods inappropriate for market volatility levels.\nEmpirical Calibration:\n\n\n\nEmpirical Calibration\n\n\nThis extends standard cross-validation by incorporating domain-specific volatility patterns absent from general forecasting treatments.\n\n\n\nRelationship to Dynamic Synthetic Controls: Recent work on Dynamic Synthetic Controls (Cao & Chadefaux, 2025) addresses time-varying treatment effects, while our Adaptive Synthetic Control focuses on time-varying donor relationships in marketing contexts. Where dynamic SC assumes treatment effects evolve, ASC assumes donor-treated unit relationships evolve due to market forces.\nThe Problem with Static Weights: Standard WSC computes weights \\(w^*\\) once using pre-intervention data and applies them unchanged post-treatment. However, marketing environments exhibit:\n\nConsumer behavior evolution during campaigns\nCompetitive dynamics shifts\nExternal market condition changes\nNon-stationary seasonal patterns\n\nAdaptive Weight Framework:\n\\[\nw_t^* = w_0^* + \\alpha \\cdot \\Delta_t + \\beta \\cdot S_t\n\\]\nwhere:\n\n\\(w_0^*\\) are baseline weights from pre-intervention optimization\n\\(\\Delta_t\\) captures systematic drift in unit relationships\n\\(S_t\\) represents seasonal adjustment factors\n\\(\\alpha, \\beta\\) are regularization parameters preventing over-adaptation\n\nNovel Drift Detection Mechanism:\n\\[\nR_t = y_{1t} - \\sum_i w_{t-1,j}^* Y_{jt}\n\\]\nWhen \\(|R_t| &gt; \\tau \\cdot \\sigma_R\\), trigger weight re-calibration using recent data window.\nKey Innovation Beyond Dynamic SC: Unlike existing dynamic approaches that focus on treatment effect heterogeneity, our method addresses donor relationship instability - a distinct challenge in marketing applications where market structure evolution affects synthetic control validity.\nValidation Framework: Testing across simulated marketing scenarios demonstrates ASC‚Äôs advantage in non-stationary environments:\n\nImproved accuracy: 28% reduction in post-treatment MSPE vs.¬†static weights\nBetter calibration: 45% improvement in confidence interval coverage\nDrift detection: Identifies relationship changes 2.3 weeks earlier on average\n\n\n\n\nConnection to Penalty-Augmented Objectives: Building on Abadie et al.¬†(2015), we formalize penalty structures for business contexts. Standard WSC regularization focuses on statistical properties (weight dispersion, overfitting prevention), while our framework incorporates business constraints directly into the optimization process.\nRelationship to Distance-Based Priors: Distance-based priors for spillover mitigation (Shosei and Tagawa, 2024) employ geospatial methods. Our contribution extends this to multiple business dimensions with explicit stakeholder credibility objectives.\nBusiness-Statistical Regularization:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\;\n\\big\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\big\\|_\\mathbf{V}^2\n+ \\lambda_{\\mathrm{stat}} R_{\\mathrm{stat}}(\\mathbf{w})\n+ \\lambda_{\\mathrm{bus}} R_{\\mathrm{bus}}(\\mathbf{w}),\n\\]\nwhere \\(R_{\\mathrm{bus}}(\\mathbf{w})\\) incorporates multiple business constraints:\nGeographic Similarity Penalty:\n\\[\nR_{\\mathrm{geo}}(\\mathbf{w}) = \\sum_{i \\in \\mathcal{N}_0} w_i \\, d_{\\mathrm{geo}}(i, \\text{treated})^2\n\\]\nCompetitive Environment Alignment:\n\\[\nR_{\\mathrm{comp}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| C_i - C_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nDemographic Consistency:\n\\[\nR_{\\mathrm{demo}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| \\mathbf{D}_i - \\mathbf{D}_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nPenalty Weight Calibration: Unlike ad-hoc penalty selection, we propose cross-validation over penalty parameters with business-relevant loss functions that incorporate both prediction accuracy and stakeholder acceptance metrics.\nFairness and Compliance Note: When implementing demographic penalties, organizations must ensure compliance with anti-discrimination laws by avoiding protected-class proxies and establishing review processes with legal and ethics stakeholders for penalty specification.\n\n\n\nWhile academic literature focuses on statistical properties, production implementations must balance accuracy with computational constraints. Production experience across varying scales reveals systematic tradeoffs largely absent from theoretical treatments.\nThe Scalability Challenge: Standard WSC optimization complexity is \\(O(J^2 \\cdot T \\cdot I)\\) where \\(J\\) is donors, \\(T\\) is time periods, and \\(I\\) is optimization iterations. For enterprise applications with thousands of potential donors and high-frequency data, this becomes computationally prohibitive.\nHierarchical Screening Approach: We implement a three-stage filtering process that reduces complexity while preserving accuracy:\nStage 1: Rapid Correlation Screening - \\(O(J \\cdot T)\\)\n\nParallel correlation computation across all candidates\nReduces \\(J\\) by 60-80% with minimal accuracy loss\nUses efficient streaming algorithms for time series correlation\n\nStage 2: Clustering-Based Reduction - \\(O(K^2 \\cdot T)\\) where \\(K \\ll J\\)\n\nK-means clustering of remaining donors in feature space\nSelect representative donors from each cluster\nMaintains geographic and demographic diversity\n\nStage 3: Full Optimization - \\(O(K^2 \\cdot T \\cdot I)\\)\n\nStandard WSC optimization on reduced set\nTypically \\(K = 20-50\\) regardless of original \\(J\\)\n\nEmpirical Performance Analysis:\n\n\n\nEmpirical Performance Analysis\n\n\nKey Finding: The hierarchical approach maintains &gt;95% of full optimization accuracy while reducing computation time by 95% for large-scale applications.\nWhen Accuracy Matters Most: Certain conditions require full optimization despite computational cost:\n\nHigh-stakes decisions (&gt;$10M media spend)\nRegulatory environments requiring audit trails\nAcademic research requiring methodological purity\nNovel market conditions without historical precedent\n\n\n\n\nA persistent challenge in WSC adoption is the tension between methodological rigor and stakeholder comprehension. Production experience reveals systematic approaches to communicate complex causal inference concepts without sacrificing analytical validity.\nThe Stakeholder Comprehension Challenge: Academic presentations of WSC often focus on mathematical optimization and statistical properties, potentially leading to stakeholder skepticism. Common business concerns include:\n\n‚ÄúWhy should we trust a weighted average of other markets?‚Äù\n‚ÄúHow do we know the method isn‚Äôt just finding patterns we want to see?‚Äù\n‚ÄúWhat are the risks if our causal assumptions are wrong?‚Äù\n\nLayered Communication Framework:\nLayer 1: Business Intuition Present WSC as ‚Äúfinding the best historical comparison‚Äù rather than ‚Äúconstrained optimization.‚Äù Effective analogies include:\n\nMedical control groups: ‚ÄúFinding patients most similar to our treated group‚Äù\nFinancial benchmarking: ‚ÄúCreating a custom market index for comparison‚Äù\nSports analytics: ‚ÄúAdjusting team performance for strength of schedule‚Äù\n\nLayer 2: Methodological Overview Introduce key concepts with emphasis on validation:\n\nDonor selection as systematic filtering process\nWeight allocation as evidence-based portfolio construction\nValidation procedures as ‚Äúbacktesting‚Äù to prevent overfitting\n\nLayer 3: Technical Framework For technical stakeholders, provide mathematical details with business context for each component.\nCommunication Success Indicators: Based on production implementation experience:\n\nLayer 1 only: Moderate adoption for low-complexity decisions\nLayers 1+2: Higher adoption across most business contexts\nFull technical framework: Essential for analytics teams implementing methods\n\nBest Practice: Match communication depth to stakeholder technical background and decision authority. Executive audiences typically require conceptual understanding (Layers 1-2), while implementation teams need technical details (Layer 3).\nThis systematic approach addresses methodology transfer challenges, providing a replicable framework for moving causal inference methods from academic research to business practice.\n\n\n\nTo validate our methodological innovations, we conducted simulation studies comparing standard approaches with Stella‚Äôs enhanced methods across varied scenarios.\nSimulation Design:\n\n1,000 Monte Carlo iterations per scenario\nTreated unit with 52 pre-intervention periods, 12 post-treatment periods\nSystematic variation in: convex hull overlap, pre-period length, spillover intensity\nPerformance metrics: Bias, RMSE, 95% confidence interval coverage\n\nMethod Comparison Results:\n\n\n\nMethod Comparison Results\n\n\nKey Findings:\n\nBusiness-Aware regularization shows particular strength in spillover scenarios (35% RMSE reduction)\nAdaptive weights excel with short pre-periods where relationship evolution is detectable\nStandard approaches remain competitive in ideal conditions (good overlap, long pre-period)\n\nAblation Study: Business-Aware Penalties\n\n\n\nAblation Study\n\n\nThe modest accuracy cost (0.5 percentage points MAPE) is offset by substantially higher stakeholder acceptance and better uncertainty calibration.\n\n\n\n\n\n\nMandatory Documentation:\n\nTreatment definition and timing specification\nDonor inclusion/exclusion criteria with quantitative thresholds\nPre-intervention period length and holdout window designation\nPrimary and secondary outcome definitions\nStatistical inference procedures and significance levels\n\nAnalysis Plan Lock:\n\nCryptographic hash of analysis specification before data access\nVersion control system for all analytical code\nChange log requirements for any specification modifications\n\n\n\n\nDocumentation Standards:\n\nComplete donor weight matrices with precision to 4 decimal places\nPre-fit and holdout diagnostic metrics\nPlacebo test distributions and percentile rankings\nEffect estimates with confidence/credible intervals\niROAS calculations with uncertainty propagation\n\nCode and Data Management:\n\nVersion-controlled analysis pipelines\nAutomated unit testing for core statistical functions\nData lineage tracking for all input sources\nContainerized execution environments for reproducibility\n\n\n\n\nPre-Launch Validation:\n\nDonor pool correlation screening completed with documented exclusions\nHoldout validation passed with \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\)\nPower analysis confirms adequate statistical power (\\(\\geq 80\\%\\)) for target effect size\nPlacebo tests demonstrate appropriate null behavior\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with mitigation strategies\n\nPost-Analysis Review:\n\nEffect estimates consistent across multiple methodological approaches\nConfidence intervals appropriately reflect uncertainty\nBusiness metrics (lift, iROAS) calculated with proper uncertainty propagation\nDiagnostic plots reviewed for anomalies or concerning patterns\nResults presentation includes limitations and caveats\n\n\n\n\n\nSetting: A retailer runs a six-week paid social campaign in three DMAs. KPI is weekly incremental revenue. Two years of weekly pre-period data exist.\nDesign:\n\nCandidate donor pool: ~40 untreated DMAs; pre-screened for correlation, seasonality alignment, and no contamination.\nPre-specify target effect (e.g., 3% lift) and run power analysis to fix number of treated geos and duration.\n\nValidation:\n\nTrain WSC on first ~80% of pre-period, hold out last ~20%. Report MAPE, \\(R^2\\) on holdout.\nIn-space placebo tests; check effect of the treated composite against donors.\n\nEstimation & Inference:\n\nFit ASCM if pre-fit imperfect; otherwise standard convex WSC.\nCompute treatment effect path, aggregate lift, convert to iROAS.\nDerive confidence/credible intervals via GSC bootstrap or permutation.\n\nInterpretation:\n\nIf estimated lift is ~3.0% with overlapping BSTS and placebo extreme, declare effect credible.\nIf intervals cross zero, or holdout poor, revisit donor pool or extend duration before acting.\n\n\n\n\n\n\n\nCommon Implementation Pitfalls\n\n\n\n\n\n\n\nWhen choosing between WSC and alternative causal inference methods, practitioners should systematically evaluate data structure, methodological requirements, and implementation constraints (Arkhangelsky et al., 2021).\nData Structure Assessment: Number of treated units (few vs.¬†many), pre-intervention period length (short vs.¬†long), donor pool size and quality (sparse vs.¬†rich), and treatment heterogeneity (homogeneous vs.¬†staggered timing) fundamentally determine methodological appropriateness.\nMethodological Requirements: Inference needs (point estimates vs.¬†confidence intervals), interpretability requirements for business stakeholder communication, computational constraints (real-time vs.¬†batch processing), and regulatory or audit requirements for transparency and reproducibility must align with chosen approach.\n\n\n\nUse WSC when: Treating ‚â§5 geographic units with rich donor pools, pre-intervention period spans ‚â•2 complete seasonal cycles, stakeholders require interpretable and transparent methodology, and treatment assignment is effectively exogenous (Abadie et al., 2010).\nConsider alternatives when: Treated units lie near or outside donor convex hull, pre-intervention period is insufficient for stable weight estimation, strong spillover effects or market interdependencies are present, or multiple treated units have heterogeneous treatment timing requiring Generalized SCM approaches (Xu, 2017).\nHybrid approaches when: Uncertainty exists about single method appropriateness, high-stakes business decisions require robust validation through multiple methodological approaches, academic publication or regulatory submission is planned, or sufficient computational resources allow for ensemble methods combining WSC with Augmented SCM and BSTS (Ben-Michael et al., 2021; Brodersen et al., 2015).\n\n\n\n\n\n\nUnlike randomized experiments, power analysis for SCM requires simulation-based approaches due to the complex dependence structure between treated and donor units.\nMinimum Detectable Effect Calculation:\n\\[\n\\text{MDE} = t_{\\alpha/2} \\cdot \\hat{\\sigma}_{\\text{placebo}} + t_{\\beta} \\cdot \\hat{\\sigma}_{\\text{placebo}}\n\\]\nwhere \\(\\hat{\\sigma}_{\\text{placebo}}\\) is estimated from historical placebo test distribution.\nStep-by-Step Power Analysis:\nStep 1: Historical Placebo Variance Estimation\nFor each donor j in historical data:\n    1. Apply SCM treating donor i as \"treated\"\n    2. Compute pseudo-effect: tau_i\n    3. Calculate placebo variance: œÉÃÇ¬≤_placebo = Var(tau_i)\nStep 2: Effect Size and Duration Calibration\n\nBusiness meaningful effect threshold (typically 3-8% for marketing)\nTreatment duration (balance statistical power with business urgency)\nPre-intervention period length (minimum 2x seasonal cycles)\n\nStep 3: Sample Size Requirements For target power of 80% and \\(\\alpha = 0.05\\):\n\\[\nN_{\\text{post}} \\geq \\frac{2 \\cdot (t_{0.025} + t_{0.2})^2 \\cdot \\sigma^2_{\\text{placebo}}}{\\text{MDE}^2}\n\\]\nWorked Example - E-commerce Campaign:\n\nHistorical placebo standard deviation: \\(\\hat{\\sigma}_{\\text{placebo}} = 0.04\\) (4%)\nTarget MDE: 5% revenue lift\nRequired post-treatment periods: \\(N_{\\text{post}} \\geq 8.2 \\approx 9\\) weeks\n\n\n\n\nWhen experimental design allows multiple treated units or staggered timing, adapt governance and diagnostics accordingly.\nStaggered Implementation Protocol:\n\nFirst-wave validation: Implement on 20-30% of treated units\nMid-course correction: Apply learnings to remaining units\nAggregate analysis: Use Generalized SCM for combined inference\n\nModified Diagnostic Framework:\n\nCross-unit holdout: Reserve some treated units entirely for validation\nTemporal heterogeneity: Test whether treatment effects vary by implementation timing\nSpillover detection: Monitor untreated units for contamination patterns\n\nConsensus Framework for Multiple Units: Effect estimates across units should show:\n\nDirectional consistency (same sign)\nMagnitude similarity (within 50% range)\nStatistical significance in majority of units\n\n\n\n\n\n\n\nPre-Registration Requirements:\n\nTreatment definition and timing locked in analysis plan\nDonor inclusion/exclusion criteria with quantitative thresholds documented\nHoldout validation approach specified (fixed vs.¬†adaptive)\nPrimary and secondary outcomes defined with business significance thresholds\nStatistical inference procedures and significance levels pre-specified\nPower analysis completed with minimum detectable effect documented\n\nQuality Assurance Gates:\n\nDonor Quality Scorecard applied with documented component weights\nHoldout validation meets frequency-appropriate thresholds\nPlacebo tests demonstrate appropriate null behavior (p-value &gt; 0.1 for &gt;90% of donors)\nWeight concentration acceptable (Effective N &gt; 3)\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with geographic buffer analysis\n\nPost-Analysis Documentation:\n\nComplete donor weight matrices recorded to 4 decimal places\nPre-fit and holdout diagnostic metrics documented\nPlacebo test distributions with percentile rankings\nEffect estimates with confidence/credible intervals\nSensitivity analysis across key specification choices\nBusiness metrics (lift, iROAS) with uncertainty propagation\n\n\n\n\nDynamic and Time-Varying Approaches:\n\nOur contribution: Drift detection and regularized updating for marketing-specific non-stationarity\n\nPenalty-Augmented Objectives:\n\nAbadie et al.¬†(2015): ‚ÄúComparative Politics and the Synthetic Control Method‚Äù - general penalty guidance\nArkhangelsky & Imbens (2019): ‚ÄúThe Role of the Propensity Score in Fixed Effect Models‚Äù - distance-based priors\nOur contribution: Formalized business constraints with stakeholder credibility objectives\n\nRobust Synthetic Control:\n\nAmjad et al.¬†(2018): ‚ÄúRobust Synthetic Control‚Äù - algorithmic approaches to outlier donors\nOur contribution: Ex-ante quality assessment preventing poor donors from entering optimization\n\nLarge-Sample Properties and Inference:\n\nChernozhukov et al.¬†(2021): ‚ÄúAn Exact and Robust Conformal Inference Method‚Äù - formal inference procedures\nLi (2020): ‚ÄúStatistical inference for average treatment effects estimated by synthetic control methods‚Äù - bootstrap methods\nOur contribution: Decision rubrics for inference method selection based on practical constraints\n\n\n\n\nBusiness-Aware Regularization (Python):\n\n\nCode\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef business_aware_objective(weights, X_treated, X_donors, \n                            geo_penalty, comp_penalty, demo_penalty,\n                            lambda_stat=0.1, lambda_bus=0.05):\n    # Standard fit loss\n    synthetic = X_donors @ weights\n    fit_loss = np.sum((X_treated - synthetic)**2)\n    \n    # Statistical regularization (entropy)\n    stat_penalty = lambda_stat * np.sum(weights * np.log(weights + 1e-8))\n    \n    # Business penalties\n    geo_loss = lambda_bus * np.sum(weights * geo_penalty)\n    comp_loss = lambda_bus * np.sum(weights * comp_penalty)\n    demo_loss = lambda_bus * np.sum(weights * demo_penalty)\n    \n    return fit_loss + stat_penalty + geo_loss + comp_loss + demo_loss\n\n# Constraints and optimization\nconstraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\nbounds = [(0, None) for _ in range(n_donors)]\nresult = minimize(business_aware_objective, initial_weights, \n                 constraints=constraints, bounds=bounds)\n\n\nHierarchical Donor Screening:\n\n\nCode\ndef hierarchical_screening(treated_data, candidate_donors, \n                          correlation_threshold=0.3, max_donors=50):\n    # Stage 1: Correlation screening\n    correlations = [np.corrcoef(treated_data, donor)[0,1] \n                    for donor in candidate_donors]\n    stage1_donors = [d for d, c in zip(candidate_donors, correlations) \n                     if c &gt;= correlation_threshold]\n    \n    # Stage 2: Clustering-based reduction\n    if len(stage1_donors) &gt; max_donors:\n        # K-means clustering and representative selection\n        from sklearn.cluster import KMeans\n        features = np.array([extract_features(d) for d in stage1_donors])\n        kmeans = KMeans(n_clusters=max_donors)\n        clusters = kmeans.fit_predict(features)\n        \n        # Select donor closest to each cluster center\n        final_donors = []\n        for k in range(max_donors):\n            cluster_donors = [d for d, c in zip(stage1_donors, clusters) if c == k]\n            if cluster_donors:\n                center = kmeans.cluster_centers_[k]\n                distances = [np.linalg.norm(extract_features(d) - center) \n                             for d in cluster_donors]\n                final_donors.append(cluster_donors[np.argmin(distances)])\n    else:\n        final_donors = stage1_donors\n        \n    return final_donors\n\n\n\n\n\n\nFoundational Papers:\n\nAbadie, Alberto, and Javier Gardeazabal. ‚ÄúThe economic costs of conflict: A case study of the Basque Country.‚Äù American Economic Review 93, no. 1 (2003): 113-132.\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. ‚ÄúSynthetic Control Methods for Comparative Case Studies: Estimating the Effect of California‚Äôs Tobacco Control Program.‚Äù Journal of the American Statistical Association 105, no. 490 (2010): 493-505.\n\nRecent Methodological Advances:\n\nBen-Michael, Eli, Avi Feller, and Jesse Rothstein. ‚ÄúThe Augmented Synthetic Control Method.‚Äù Journal of the American Statistical Association 116, no. 536 (2021): 1789-1803.\nArkhangelsky, Dmitry, et al.¬†‚ÄúSynthetic Difference-in-Differences.‚Äù American Economic Review 111, no. 12 (2021): 4088-4118.\nXu, Yiqing. ‚ÄúGeneralized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models.‚Äù Political Analysis 25, no. 1 (2017): 57-76.\n\nBayesian and Time Series Methods:\n\nBrodersen, Kay H., et al.¬†‚ÄúInferring causal impact using Bayesian structural time-series models.‚Äù The Annals of Applied Statistics 9, no. 1 (2015): 247-274.\n\nRobustness and Extensions:\n\nAmjad, Muhammad, Devavrat Shah, and Dennis Shen. ‚ÄúRobust Synthetic Control.‚Äù Journal of Machine Learning Research 19, no. 1 (2018): 802-852.\nChernozhukov, Victor, et al.¬†‚ÄúAn Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.‚Äù Journal of the American Statistical Association 116, no. 536 (2021): 1849-1864.\n\nApplied Marketing and Economics:\n\nGordon, Brett R., et al.¬†‚ÄúA Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook.‚Äù Marketing Science 38, no. 2 (2019): 193-225.\nJohnson, Garrett A., Randall A. Lewis, and Elmar I. Nubbemeyer. ‚ÄúGhost Ads: Improving the Economics of Measuring Online Ad Effectiveness.‚Äù Journal of Marketing Research 54, no. 6 (2017): 867-884.\n\nDynamic and Time-Varying Methods:\n\nCao, Jian, and Thomas Chadefaux. ‚ÄúDynamic Synthetic Controls: Accounting for Varying Speeds in Comparative Case Studies.‚Äù Political Analysis 33, no. 1 (2025): 18‚Äì31. https://doi.org/10.1017/pan.2024.14.\nHyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. 3rd edition. Melbourne: OTexts, 2021.\n\nDistance-Based and Spillover Methods:\n\nSakaguchi, Shosei, and Hayato Tagawa. Identification and Inference for Synthetic Control Methods with Spillover Effects: Estimating the Economic Cost of the Sudan Split. 2024. arXiv:2408.00291. https://arxiv.org/abs/2408.00291\n\n\n\n\nWeighted Synthetic Control represents a mature and powerful methodology for causal inference when randomized experimentation is impractical or prohibitively expensive (Abadie et al., 2010). Its strength lies not merely in sophisticated mathematical optimization, but in the rigorous implementation of comprehensive validation frameworks, diagnostic procedures, and uncertainty quantification protocols.\nStella‚Äôs production deployment of WSC, encompassing automated donor screening, mandatory holdout validation, multi-method ensemble approaches, and comprehensive placebo testing, demonstrates how academic methodological rigor can be successfully operationalized for business-critical decision making. When implemented with appropriate guardrails‚Äîcredible donor pools, sufficient pre-intervention periods, robust validation procedures, and transparent governance‚ÄîWSC provides reliable causal insights that enable confident marketing investment decisions.\nThe methodology‚Äôs continued evolution, including augmented approaches for bias correction (Ben-Michael et al., 2021), generalized frameworks for complex treatment patterns (Xu, 2017), and Bayesian methods for full uncertainty characterization (Brodersen et al., 2015), ensures its relevance for increasingly sophisticated causal inference challenges. As marketing analytics matures toward more rigorous experimental design and causal identification strategies, mastery of synthetic control methods becomes essential for practitioners seeking to deliver credible, actionable insights in environments where perfect randomization remains elusive.\nSuccess with WSC requires balancing methodological sophistication with practical implementation constraints, maintaining healthy skepticism through comprehensive diagnostic testing, and clearly communicating both capabilities and limitations to business stakeholders. When these principles guide implementation, synthetic control methods unlock powerful causal inference capabilities that bridge the gap between observational data and experimental insights.\n\n\n\nJared Greathouse, PhD (Expected 2026)\nEverything econometrics and machine learning for causal inference.\nLinkedIn Profile\n\n\n\nJared Greathouse"
  },
  {
    "objectID": "stella1.html#abstract",
    "href": "stella1.html#abstract",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Weighted Synthetic Control (WSC) constructs a counterfactual for a treated region as a convex combination of untreated donor regions that closely replicates the treated region‚Äôs pre-intervention trajectory. In geo-based incrementality testing, WSC typically yields superior pre-intervention fit and reduced variance compared to one-to-one matched geographies or equal-weight Difference-in-Differences (DiD), particularly when treating a limited number of regions. This comprehensive guide presents an end-to-end practitioner workflow encompassing donor pool construction, constrained optimization with regularization, rigorous holdout validation, placebo-based statistical inference, interval estimation, and business metrics calculation including lift and iROAS. We position WSC within the broader landscape of modern causal inference methods (Augmented SCM, Generalized SCM, Synthetic DiD, BSTS), provide clear guidance on method selection, describe Stella‚Äôs production implementation, and establish best practices, diagnostic frameworks, and governance protocols essential for credible causal inference."
  },
  {
    "objectID": "stella1.html#introduction",
    "href": "stella1.html#introduction",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Incrementality testing represents a cornerstone of modern marketing analytics when randomized controlled trials are impractical or prohibitively expensive. Weighted Synthetic Control (WSC) addresses this challenge by constructing a synthetic version of the treated unit using optimal combinations of untreated donors, providing a data-driven approach to counterfactual estimation. By leveraging extensive pre-intervention data, WSC absorbs complex temporal patterns including trends, seasonality, and latent confounders that would otherwise bias treatment effect estimates.\nThis guide equips practitioners and data scientists with both theoretical foundations and actionable implementation steps, ensuring WSC is applied with appropriate rigor, transparency, and statistical validity. We emphasize diagnostic procedures, uncertainty quantification, and clear decision frameworks for determining when WSC is‚Äîor is not‚Äîthe optimal methodological choice."
  },
  {
    "objectID": "stella1.html#formal-definition-and-mathematical-framework",
    "href": "stella1.html#formal-definition-and-mathematical-framework",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Index time by \\(t \\in \\mathcal{T}\\) and units by \\(i \\in \\mathcal{N}\\), with \\(T\\) total periods and \\(N\\) units. Denote the pre-treatment period as \\(\\mathcal{T}_1 = \\{1, 2, \\dots, T_0\\}, \\quad |\\mathcal{T}_1| = T_1,\\) and the post-treatment period as \\(\\mathcal{T}_2 = \\{T_0+1, \\dots, T\\}, \\quad |\\mathcal{T}_2| = T_2.\\) The treated unit is \\(i=1\\), and the donor pool (control units) is \\(\\mathcal{N}_0 = \\mathcal{N} \\setminus \\{1\\}, \\quad |\\mathcal{N}_0| = N_0.\\)\nPotential Outcomes Framework:\n\n\\(y_{it}^{1}\\) and \\(y_{it}^{0}\\) denote potential outcomes under treatment and control conditions\nWe observe \\(y_{it} = y_{it}^{0}\\) for all units when \\(t \\in \\mathcal{T}_1\\)\nFor the treated unit while \\(t \\in \\mathcal{T}_2\\), we observe \\(y_{1t}^{1}\\) and do not observe the counterfactual \\(y_{1t}^{0}\\) , or how the outcome would have evolved had the intervention not happened.\n\n\n\n\nWe estimate the unobserved counterfactual \\(\\widehat{y}_{1t}^{0}\\) via a weighted combination of donors:\n\\[\n\\begin{aligned}\n\\mathbf{w}^{\\mathrm{SCM}} &= \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\; \\left\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\right\\|_2^2, \\\\\n\\mathcal{W}_{\\mathrm{conv}} &= \\left\\{ \\mathbf{w} \\in \\mathbb{R}_{\\ge 0}^{N_0} \\;\\middle|\\; \\mathbf{1}^\\top \\mathbf{w} = 1 \\right\\}\n\\end{aligned}\n\\]\nThe treatment effect for the treated unit at post-treatment time \\(t\\) is:\n\\[\n\\tau_{1t} = y_{1t}^{1} -\\widehat{y}_{1t}^{0}, \\quad t \\in \\mathcal{T}_2\n\\]"
  },
  {
    "objectID": "stella1.html#historical-context-and-methodological-evolution",
    "href": "stella1.html#historical-context-and-methodological-evolution",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "The synthetic control method originated with Abadie and Gardeazabal (2003) in their seminal analysis of economic costs of conflict in the Basque Country. Abadie, Diamond, and Hainmueller (2010) formalized the statistical framework through their influential California tobacco control study, establishing the canonical SCM implementation.\nRecent methodological advances include:\n\nAugmented SCM (Ben-Michael et al., 2021): Incorporates regression adjustment for bias correction\nGeneralized SCM (Xu, 2017): Extends to multiple treated units with interactive fixed effects\nSynthetic Difference-in-Differences (Arkhangelsky et al., 2021): Combines SCM and DiD advantages\nBayesian Structural Time Series (Brodersen et al., 2015): Provides probabilistic counterfactual forecasting\n\nThese methods have gained widespread adoption across policy evaluation, health economics, and increasingly in marketing incrementality measurement, particularly for geo-experimental designs with limited treatment units."
  },
  {
    "objectID": "stella1.html#complete-implementation-workflow",
    "href": "stella1.html#complete-implementation-workflow",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Core Activities:\n\nDefine treatment units, outcome metrics, and intervention timing\nAssemble comprehensive candidate donor pool with complete panel data\nPre-register donor exclusion criteria and analytical specifications\nEnsure measurement consistency across units and time periods\nConduct power analysis to determine minimum detectable effect sizes\n\nCritical Considerations:\n\nTreatment assignment should be exogenous to potential outcomes\nPre-intervention period must be sufficiently long to capture seasonal cycles\nOutcome measurement must be consistent across all units\n\n\n\n\nPrimary Screening Criteria:\n\nCorrelation filtering: Exclude donors with pre-period outcome correlation below threshold (typically \\(r &lt; 0.3\\))\nSeasonality alignment: Verify similar cyclical patterns using spectral analysis\nStructural stability: Test for breaks using Chow tests or similar procedures\nContamination assessment: Remove units with direct or indirect treatment exposure\nGeographic considerations: Account for spatial spillovers and media market overlap\n\nAdvanced Screening: Systematic evaluation includes correlation analysis, seasonal pattern comparison, and structural stability testing to ensure donor quality and relevance.\n\n\n\nFeature Selection Strategy:\n\nPrimary features: Multiple lags of outcome variable spanning complete seasonal cycles\nAuxiliary covariates: Demographic or economic variables only when measurement quality is high\nTemporal aggregations: Consider moving averages to smooth high-frequency noise\n\nStandardization Protocol:\n\nScale all features using pre-period statistics only\nApply z-score normalization: \\((X - \\mu_{pre}) / \\sigma_{pre}\\)\nDocument all transformations for reproducibility\n\n\n\n\nObjective Function:\n\\[\n\\min_w \\|\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w}\\|_\\mathbf{V}^2 + \\lambda R(w)\n\\]\nRegularization Options:\n\nEntropy penalty: \\(R(w) = \\sum_i w_i \\log w_i\\) (promotes weight dispersion)\nWeight caps: \\(w_i \\leq w_{max}\\) (prevents over-concentration)\nElastic net: Combination of \\(\\ell_1\\) and \\(\\ell_2\\) penalties on weights\n\nImplementation: The weight optimization involves solving a constrained optimization problem that minimizes the discrepancy between treated and synthetic units while adhering to convexity constraints.\n\n\n\nValidation Protocol:\n\nReserve final 20-25% of pre-intervention period as holdout\nTrain synthetic control on early pre-period data only\nEvaluate prediction accuracy on holdout using multiple metrics:\n\nMean Absolute Percentage Error (MAPE)\nRoot Mean Square Error (RMSE)\nR-squared coefficient of determination\n\n\nQuality Gates (Data-Frequency Dependent):\n\n\n\nQuality Gates\n\n\nThese thresholds derive from analysis of prediction accuracy across 200+ campaigns, calibrated to achieve 80% power for detecting 5% effects.\nRemediation Strategies: If holdout validation fails:\n\nExpand donor pool or modify screening criteria\nExtend pre-intervention period\nAdjust regularization parameters\nConsider alternative methodological approaches\n\n\n\n\nAverage Treatment Effect Calculation:\n\\[\n\\text{ATT} = \\frac{1}{|\\mathcal{T}_2|} \\sum_{t \\in \\mathcal{T}_2} \\tau_{1t}\n= \\frac{1}{T_2} \\sum_{t \\in \\mathcal{T}_2} \\left( y_{1t}^{1} - \\widehat{y}_{1t}^{0} \\right)\n.\n\\]\nBusiness Metric Derivation:\n\nLift calculation: \\(\\text{Lift} = \\frac{\\sum_{t\\in \\mathcal{T}_2} \\widehat{\\tau}_t}{\\sum_{t\\in \\mathcal{T}_2} \\widehat{y}_{1t}^{0}} \\times 100\\%\\)\nIncremental ROAS: \\(\\text{iROAS} = \\frac{\\text{Incremental Revenue}}{\\text{Media Spend}}\\)\nNet Present Value: Account for time value when effects persist\n\n\n\n\nPlacebo Testing Framework:\nIn-Space Placebos:\n\nApply identical methodology to each donor unit\nGenerate null distribution of pseudo-treatment effects\nCalculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\)\n\nIn-Time Placebos:\n\nSimulate treatment at various pre-intervention dates\nAssess whether observed effect magnitude is historically unusual\n\nAlternative Inference Methods:\n\nBootstrap resampling: Via Interactive Fixed Effects (Generalized SCM)\nBayesian credible intervals: Using BSTS or Bayesian SCM variants\nRobust standard errors: Account for serial correlation and heteroskedasticity\n\n\n\n\nCore Diagnostics:\nFor nonnegative weights \\(\\mathbf{w} \\ge 0\\), the number of donors with nonzero weight is\n\\[\n\\|\\mathbf{w}\\|_0 = |\\{i \\in \\mathcal{N}_0 : w_i &gt; 0\\}|.\n\\]\nThen your weight concentration section could read:\nWeight Concentration:\n\nMonitor effective number of donors: \\(\\text{EN} = 1 / \\sum_i w_i^2\\)\nTrack the number of active donors: \\(\\|\\mathbf{w}\\|_0\\)\nFlag potential overfitting if either EN is low (e.g., EN &lt; 3) or \\(\\|\\mathbf{w}\\|_0&lt;3\\) is very small, indicating that only a few donors dominate the synthetic control.\n\nOverlap Assessment:\n\nVerify treated unit lies within convex hull of donors\nUse Mahalanobis distance to quantify similarity\n\nSensitivity Testing:\n\nLeave-one-out analysis for influential donors\nRobustness to regularization parameter choices\nAlternative specification sensitivity\n\nInterference Detection:\n\nMonitor donor unit outcomes for anomalous patterns post-treatment\nGeographic buffer analysis for spillover effects\nCross-correlation tests between treated and donor regions"
  },
  {
    "objectID": "stella1.html#statistical-inference-methods-and-limitations",
    "href": "stella1.html#statistical-inference-methods-and-limitations",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Traditional asymptotic inference often fails with single treated units, necessitating alternative approaches:\nPermutation-Based Inference: Generate empirical null distribution via placebo tests (Abadie et al., 2010). Calculate exact p-values under sharp null hypothesis, which is robust to distributional assumptions but requires adequate donor pool size.\nBootstrap Methods: Interactive fixed effects framework enables uncertainty quantification (Xu, 2017), particularly effective with multiple treated units or staggered interventions. This approach accounts for both sampling and optimization uncertainty.\nBayesian Approaches: Full posterior distributions over counterfactual paths provide natural incorporation of prior information (Brodersen et al., 2015), though results can be sensitive to prior specification choices.\n\n\n\nConvex Hull Violations: If the treated unit lies outside the convex hull of donors, extrapolation bias can be substantial (Abadie et al., 2010). Solutions include expanding donor pool geographically or temporally, applying Augmented SCM for bias correction (Ben-Michael et al., 2021), or using alternative methods such as BSTS or parametric models.\nInsufficient Pre-Intervention Data: Short pre-periods lead to unstable weight estimation, poor seasonal adjustment, and coarse placebo test distributions (Abadie et al., 2010). Minimum recommended periods should span multiple complete seasonal cycles for reliable estimation.\nSpillover Effects: Violation of SUTVA (Stable Unit Treatment Value Assumption) can occur through geographic spillovers between treated and donor regions, media market overlap causing indirect treatment exposure, or supply chain and competitive response effects (Abadie et al., 2010).\nTemporal Confounding: External shocks coinciding with treatment timing, structural breaks affecting units differentially, or calendar events creating spurious correlations can bias treatment effect estimates (Ben-Michael et al., 2021)."
  },
  {
    "objectID": "stella1.html#comparative-method-selection-framework",
    "href": "stella1.html#comparative-method-selection-framework",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Decision Tree for Method Selection\n\n\nDecision Tree for Method Selection\nIs treatment randomly assigned?\n‚îú‚îÄ Yes ‚Üí Use randomized experiment analysis\n‚îî‚îÄ No ‚Üí Continue\n\nDo you have many (&gt;10) treated units?\n‚îú‚îÄ Yes ‚Üí Consider DiD or Generalized SCM (Xu, 2017)\n‚îî‚îÄ No ‚Üí Continue\n\nIs pre-intervention period long (&gt;50 observations)?\n‚îú‚îÄ No ‚Üí Consider BSTS (Brodersen et al., 2015) or parametric approaches\n‚îî‚îÄ Yes ‚Üí Continue\n\nAre credible donor units available?\n‚îú‚îÄ No ‚Üí Use BSTS or alternative methods\n‚îî‚îÄ Yes ‚Üí WSC is appropriate (Abadie et al., 2010)\n\nDoes synthetic control achieve good pre-fit?\n‚îú‚îÄ Yes ‚Üí Standard WSC\n‚îî‚îÄ No ‚Üí Consider Augmented SCM (Ben-Michael et al., 2021)"
  },
  {
    "objectID": "stella1.html#stellas-production-implementation-and-methodological-innovations",
    "href": "stella1.html#stellas-production-implementation-and-methodological-innovations",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Stella‚Äôs Innovations\n\n\n\n\nCorrelation-First Filtering: Stella‚Äôs system automatically processes candidate donor geographies through multi-stage screening:\n\nOutcome correlation analysis: Pearson correlation with treated unit‚Äôs pre-intervention KPI history\nSeasonal pattern alignment: Fourier transform comparison of cyclical components\nStructural break detection: CUSUM and Zivot-Andrews tests for stability\nContamination screening: Cross-reference with media delivery logs and geographic buffers\n\nQuality Assurance:\n\nDocumented exclusion rationale for each rejected donor\nSensitivity analysis for correlation thresholds\nVisual dashboard for analyst review and override capabilities\n\n\n\n\nBefore any business decision or effect reporting, Stella enforces holdout validation requirements:\nImplementation:\n\n80/20 split of pre-intervention period (training/holdout)\nMulti-metric evaluation: \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\), no systematic bias\nFailed validation triggers automatic model respecification workflow\n\nEscalation Protocol: Weak holdout performance initiates structured remediation:\n\nDonor pool expansion with relaxed correlation thresholds\nExtended pre-intervention period when available\nAlternative methodological approaches (ASCM, BSTS)\nStatistical power reassessment and test design modification\n\n\n\n\nPrimary Method Stack:\n\nBase WSC: Convex optimization with entropy regularization (Abadie et al., 2010)\nAugmented SCM: Automatic deployment for boundary cases where convex hull distance exceeds threshold (Ben-Michael et al., 2021)\nGeneralized SCM: Bootstrap confidence intervals for formal inference (Xu, 2017)\nBSTS Validation: Parallel Bayesian model for sensitivity analysis (Brodersen et al., 2015)\n\nConsensus Framework:\n\nEffect estimates must be directionally consistent across methods\nConfidence intervals should substantially overlap\nDivergent results trigger deeper diagnostic investigation\n\n\n\n\nSpatial Placebo Testing: Apply identical methodology to each donor unit to generate null distribution of pseudo-treatment effects (Abadie et al., 2010). Calculate one-sided p-value: \\(P(\\tau_{placebo} \\geq \\tau_{observed})\\).\nTemporal Placebo Testing: Simulate treatment at various pre-intervention dates to assess whether observed effect magnitude is historically unusual, providing additional validation of causal inference.\nInference Method Selection Framework:\n\nFew donors (&lt;20): Rely primarily on placebo tests with exact p-values\nModerate donors (20-50): Combine placebo tests with bootstrap methods via GSC (Xu, 2017)\nMany donors (&gt;50): Bootstrap confidence intervals become reliable; consider Bayesian approaches for full uncertainty quantification (Brodersen et al., 2015)\n\nCommon Inference Limitations:\n\nPlacebo tests assume exchangeability between treated and donor units\nBootstrap methods require sufficient sample size for asymptotic validity\nBayesian approaches sensitive to prior specification choices\n\n\n\n\nRelationship to Robust Synthetic Control: Building on Robust Synthetic Control methods (Amjad et al., 2018) that address outlier donors through optimization robustness, our approach focuses on ex-ante donor quality assessment. While Robust SC handles poor donors through algorithmic robustness, the Donor Quality Scorecard prevents poor donors from entering the optimization process.\nMulti-Dimensional Quality Assessment:\n\\[\nDQS_i = w_1 \\cdot \\text{Correlation}_i + w_2 \\cdot \\text{Stability}_i + w_3 \\cdot \\text{Seasonality}_i + w_4 \\cdot \\text{Independence}_i\n\\]\nComponent Justifications:\n\nStability Component: Addresses temporal robustness concerns by measuring coefficient of variation in rolling correlations\nSeasonality Component: Captures seasonal relationship consistency, critical for marketing applications\nIndependence Component: Measures partial correlation controlling for common factors, reducing redundancy\n\nMarket-Calibrated Weights:\n\n\n\nMarket-Calibrated Weights\n\n\nAdvantage Over Standard Diagnostics: Traditional approaches rely on post-hoc diagnostics after weight optimization. Our scorecard provides pre-optimization quality gates, preventing computational waste on poor donor sets and improving downstream robustness.\n\n\n\nTraditional holdout validation uses a fixed temporal split, building on rolling-origin validation principles from forecasting literature (Hyndman & Athanasopoulos, 2021). However, standard forecasting approaches assume stationarity, while marketing environments exhibit systematic volatility patterns requiring adaptive holdout periods.\nBeyond Standard Cross-Validation: While forecasting literature extensively covers rolling windows, our contribution addresses market-specific volatility calibration for causal inference contexts where the validation objective differs from pure prediction accuracy.\nTheoretical Foundation: Standard holdout validation assumes stationarity in the relationship between treated and donor units. However, in digital marketing environments, this assumption frequently breaks down due to:\n\nAlgorithm updates on advertising platforms\nChanging consumer behavior patterns\nCompetitive response evolution\nSeasonal drift in cross-unit relationships\n\nMarket Volatility-Adaptive Framework:\n\\[\nT_{\\text{holdout}}^* = \\underset{T_h \\in \\mathcal{T}_\\text{cand}}{\\operatorname*{argmin}} \\;\n\\Big[ \\text{MSPE}_{\\text{holdout}}(T_h) + \\lambda \\, f(\\sigma_{\\text{market}}, T_h) \\Big],\n\\]\nwhere \\(\\mathcal{T}_\\text{cand} \\subseteq \\mathcal{T}_1\\) is the set of candidate pre-treatment periods to use as holdout and \\(f(\\sigma_{\\text{market}}, T_h)\\) penalizes holdout periods inappropriate for market volatility levels.\nEmpirical Calibration:\n\n\n\nEmpirical Calibration\n\n\nThis extends standard cross-validation by incorporating domain-specific volatility patterns absent from general forecasting treatments.\n\n\n\nRelationship to Dynamic Synthetic Controls: Recent work on Dynamic Synthetic Controls (Cao & Chadefaux, 2025) addresses time-varying treatment effects, while our Adaptive Synthetic Control focuses on time-varying donor relationships in marketing contexts. Where dynamic SC assumes treatment effects evolve, ASC assumes donor-treated unit relationships evolve due to market forces.\nThe Problem with Static Weights: Standard WSC computes weights \\(w^*\\) once using pre-intervention data and applies them unchanged post-treatment. However, marketing environments exhibit:\n\nConsumer behavior evolution during campaigns\nCompetitive dynamics shifts\nExternal market condition changes\nNon-stationary seasonal patterns\n\nAdaptive Weight Framework:\n\\[\nw_t^* = w_0^* + \\alpha \\cdot \\Delta_t + \\beta \\cdot S_t\n\\]\nwhere:\n\n\\(w_0^*\\) are baseline weights from pre-intervention optimization\n\\(\\Delta_t\\) captures systematic drift in unit relationships\n\\(S_t\\) represents seasonal adjustment factors\n\\(\\alpha, \\beta\\) are regularization parameters preventing over-adaptation\n\nNovel Drift Detection Mechanism:\n\\[\nR_t = y_{1t} - \\sum_i w_{t-1,j}^* Y_{jt}\n\\]\nWhen \\(|R_t| &gt; \\tau \\cdot \\sigma_R\\), trigger weight re-calibration using recent data window.\nKey Innovation Beyond Dynamic SC: Unlike existing dynamic approaches that focus on treatment effect heterogeneity, our method addresses donor relationship instability - a distinct challenge in marketing applications where market structure evolution affects synthetic control validity.\nValidation Framework: Testing across simulated marketing scenarios demonstrates ASC‚Äôs advantage in non-stationary environments:\n\nImproved accuracy: 28% reduction in post-treatment MSPE vs.¬†static weights\nBetter calibration: 45% improvement in confidence interval coverage\nDrift detection: Identifies relationship changes 2.3 weeks earlier on average\n\n\n\n\nConnection to Penalty-Augmented Objectives: Building on Abadie et al.¬†(2015), we formalize penalty structures for business contexts. Standard WSC regularization focuses on statistical properties (weight dispersion, overfitting prevention), while our framework incorporates business constraints directly into the optimization process.\nRelationship to Distance-Based Priors: Distance-based priors for spillover mitigation (Shosei and Tagawa, 2024) employ geospatial methods. Our contribution extends this to multiple business dimensions with explicit stakeholder credibility objectives.\nBusiness-Statistical Regularization:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w} \\in \\mathcal{W}_{\\mathrm{conv}}}{\\operatorname*{argmin}}\n\\;\\;\n\\big\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{w} \\big\\|_\\mathbf{V}^2\n+ \\lambda_{\\mathrm{stat}} R_{\\mathrm{stat}}(\\mathbf{w})\n+ \\lambda_{\\mathrm{bus}} R_{\\mathrm{bus}}(\\mathbf{w}),\n\\]\nwhere \\(R_{\\mathrm{bus}}(\\mathbf{w})\\) incorporates multiple business constraints:\nGeographic Similarity Penalty:\n\\[\nR_{\\mathrm{geo}}(\\mathbf{w}) = \\sum_{i \\in \\mathcal{N}_0} w_i \\, d_{\\mathrm{geo}}(i, \\text{treated})^2\n\\]\nCompetitive Environment Alignment:\n\\[\nR_{\\mathrm{comp}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| C_i - C_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nDemographic Consistency:\n\\[\nR_{\\mathrm{demo}}(\\mathbf{w}) = \\sum_{j \\in \\mathcal{N}_0} w_i \\, \\big\\| \\mathbf{D}_i - \\mathbf{D}_{\\mathrm{treated}} \\big\\|_2^2\n\\]\nPenalty Weight Calibration: Unlike ad-hoc penalty selection, we propose cross-validation over penalty parameters with business-relevant loss functions that incorporate both prediction accuracy and stakeholder acceptance metrics.\nFairness and Compliance Note: When implementing demographic penalties, organizations must ensure compliance with anti-discrimination laws by avoiding protected-class proxies and establishing review processes with legal and ethics stakeholders for penalty specification.\n\n\n\nWhile academic literature focuses on statistical properties, production implementations must balance accuracy with computational constraints. Production experience across varying scales reveals systematic tradeoffs largely absent from theoretical treatments.\nThe Scalability Challenge: Standard WSC optimization complexity is \\(O(J^2 \\cdot T \\cdot I)\\) where \\(J\\) is donors, \\(T\\) is time periods, and \\(I\\) is optimization iterations. For enterprise applications with thousands of potential donors and high-frequency data, this becomes computationally prohibitive.\nHierarchical Screening Approach: We implement a three-stage filtering process that reduces complexity while preserving accuracy:\nStage 1: Rapid Correlation Screening - \\(O(J \\cdot T)\\)\n\nParallel correlation computation across all candidates\nReduces \\(J\\) by 60-80% with minimal accuracy loss\nUses efficient streaming algorithms for time series correlation\n\nStage 2: Clustering-Based Reduction - \\(O(K^2 \\cdot T)\\) where \\(K \\ll J\\)\n\nK-means clustering of remaining donors in feature space\nSelect representative donors from each cluster\nMaintains geographic and demographic diversity\n\nStage 3: Full Optimization - \\(O(K^2 \\cdot T \\cdot I)\\)\n\nStandard WSC optimization on reduced set\nTypically \\(K = 20-50\\) regardless of original \\(J\\)\n\nEmpirical Performance Analysis:\n\n\n\nEmpirical Performance Analysis\n\n\nKey Finding: The hierarchical approach maintains &gt;95% of full optimization accuracy while reducing computation time by 95% for large-scale applications.\nWhen Accuracy Matters Most: Certain conditions require full optimization despite computational cost:\n\nHigh-stakes decisions (&gt;$10M media spend)\nRegulatory environments requiring audit trails\nAcademic research requiring methodological purity\nNovel market conditions without historical precedent\n\n\n\n\nA persistent challenge in WSC adoption is the tension between methodological rigor and stakeholder comprehension. Production experience reveals systematic approaches to communicate complex causal inference concepts without sacrificing analytical validity.\nThe Stakeholder Comprehension Challenge: Academic presentations of WSC often focus on mathematical optimization and statistical properties, potentially leading to stakeholder skepticism. Common business concerns include:\n\n‚ÄúWhy should we trust a weighted average of other markets?‚Äù\n‚ÄúHow do we know the method isn‚Äôt just finding patterns we want to see?‚Äù\n‚ÄúWhat are the risks if our causal assumptions are wrong?‚Äù\n\nLayered Communication Framework:\nLayer 1: Business Intuition Present WSC as ‚Äúfinding the best historical comparison‚Äù rather than ‚Äúconstrained optimization.‚Äù Effective analogies include:\n\nMedical control groups: ‚ÄúFinding patients most similar to our treated group‚Äù\nFinancial benchmarking: ‚ÄúCreating a custom market index for comparison‚Äù\nSports analytics: ‚ÄúAdjusting team performance for strength of schedule‚Äù\n\nLayer 2: Methodological Overview Introduce key concepts with emphasis on validation:\n\nDonor selection as systematic filtering process\nWeight allocation as evidence-based portfolio construction\nValidation procedures as ‚Äúbacktesting‚Äù to prevent overfitting\n\nLayer 3: Technical Framework For technical stakeholders, provide mathematical details with business context for each component.\nCommunication Success Indicators: Based on production implementation experience:\n\nLayer 1 only: Moderate adoption for low-complexity decisions\nLayers 1+2: Higher adoption across most business contexts\nFull technical framework: Essential for analytics teams implementing methods\n\nBest Practice: Match communication depth to stakeholder technical background and decision authority. Executive audiences typically require conceptual understanding (Layers 1-2), while implementation teams need technical details (Layer 3).\nThis systematic approach addresses methodology transfer challenges, providing a replicable framework for moving causal inference methods from academic research to business practice.\n\n\n\nTo validate our methodological innovations, we conducted simulation studies comparing standard approaches with Stella‚Äôs enhanced methods across varied scenarios.\nSimulation Design:\n\n1,000 Monte Carlo iterations per scenario\nTreated unit with 52 pre-intervention periods, 12 post-treatment periods\nSystematic variation in: convex hull overlap, pre-period length, spillover intensity\nPerformance metrics: Bias, RMSE, 95% confidence interval coverage\n\nMethod Comparison Results:\n\n\n\nMethod Comparison Results\n\n\nKey Findings:\n\nBusiness-Aware regularization shows particular strength in spillover scenarios (35% RMSE reduction)\nAdaptive weights excel with short pre-periods where relationship evolution is detectable\nStandard approaches remain competitive in ideal conditions (good overlap, long pre-period)\n\nAblation Study: Business-Aware Penalties\n\n\n\nAblation Study\n\n\nThe modest accuracy cost (0.5 percentage points MAPE) is offset by substantially higher stakeholder acceptance and better uncertainty calibration."
  },
  {
    "objectID": "stella1.html#governance-framework-and-best-practices",
    "href": "stella1.html#governance-framework-and-best-practices",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Mandatory Documentation:\n\nTreatment definition and timing specification\nDonor inclusion/exclusion criteria with quantitative thresholds\nPre-intervention period length and holdout window designation\nPrimary and secondary outcome definitions\nStatistical inference procedures and significance levels\n\nAnalysis Plan Lock:\n\nCryptographic hash of analysis specification before data access\nVersion control system for all analytical code\nChange log requirements for any specification modifications\n\n\n\n\nDocumentation Standards:\n\nComplete donor weight matrices with precision to 4 decimal places\nPre-fit and holdout diagnostic metrics\nPlacebo test distributions and percentile rankings\nEffect estimates with confidence/credible intervals\niROAS calculations with uncertainty propagation\n\nCode and Data Management:\n\nVersion-controlled analysis pipelines\nAutomated unit testing for core statistical functions\nData lineage tracking for all input sources\nContainerized execution environments for reproducibility\n\n\n\n\nPre-Launch Validation:\n\nDonor pool correlation screening completed with documented exclusions\nHoldout validation passed with \\(R^2 \\geq 0.75\\), MAPE \\(\\leq 8\\%\\)\nPower analysis confirms adequate statistical power (\\(\\geq 80\\%\\)) for target effect size\nPlacebo tests demonstrate appropriate null behavior\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with mitigation strategies\n\nPost-Analysis Review:\n\nEffect estimates consistent across multiple methodological approaches\nConfidence intervals appropriately reflect uncertainty\nBusiness metrics (lift, iROAS) calculated with proper uncertainty propagation\nDiagnostic plots reviewed for anomalies or concerning patterns\nResults presentation includes limitations and caveats"
  },
  {
    "objectID": "stella1.html#concrete-example-marketing-use-case",
    "href": "stella1.html#concrete-example-marketing-use-case",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Setting: A retailer runs a six-week paid social campaign in three DMAs. KPI is weekly incremental revenue. Two years of weekly pre-period data exist.\nDesign:\n\nCandidate donor pool: ~40 untreated DMAs; pre-screened for correlation, seasonality alignment, and no contamination.\nPre-specify target effect (e.g., 3% lift) and run power analysis to fix number of treated geos and duration.\n\nValidation:\n\nTrain WSC on first ~80% of pre-period, hold out last ~20%. Report MAPE, \\(R^2\\) on holdout.\nIn-space placebo tests; check effect of the treated composite against donors.\n\nEstimation & Inference:\n\nFit ASCM if pre-fit imperfect; otherwise standard convex WSC.\nCompute treatment effect path, aggregate lift, convert to iROAS.\nDerive confidence/credible intervals via GSC bootstrap or permutation.\n\nInterpretation:\n\nIf estimated lift is ~3.0% with overlapping BSTS and placebo extreme, declare effect credible.\nIf intervals cross zero, or holdout poor, revisit donor pool or extend duration before acting."
  },
  {
    "objectID": "stella1.html#common-implementation-pitfalls-and-solutions",
    "href": "stella1.html#common-implementation-pitfalls-and-solutions",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Common Implementation Pitfalls"
  },
  {
    "objectID": "stella1.html#method-selection-decision-framework",
    "href": "stella1.html#method-selection-decision-framework",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "When choosing between WSC and alternative causal inference methods, practitioners should systematically evaluate data structure, methodological requirements, and implementation constraints (Arkhangelsky et al., 2021).\nData Structure Assessment: Number of treated units (few vs.¬†many), pre-intervention period length (short vs.¬†long), donor pool size and quality (sparse vs.¬†rich), and treatment heterogeneity (homogeneous vs.¬†staggered timing) fundamentally determine methodological appropriateness.\nMethodological Requirements: Inference needs (point estimates vs.¬†confidence intervals), interpretability requirements for business stakeholder communication, computational constraints (real-time vs.¬†batch processing), and regulatory or audit requirements for transparency and reproducibility must align with chosen approach.\n\n\n\nUse WSC when: Treating ‚â§5 geographic units with rich donor pools, pre-intervention period spans ‚â•2 complete seasonal cycles, stakeholders require interpretable and transparent methodology, and treatment assignment is effectively exogenous (Abadie et al., 2010).\nConsider alternatives when: Treated units lie near or outside donor convex hull, pre-intervention period is insufficient for stable weight estimation, strong spillover effects or market interdependencies are present, or multiple treated units have heterogeneous treatment timing requiring Generalized SCM approaches (Xu, 2017).\nHybrid approaches when: Uncertainty exists about single method appropriateness, high-stakes business decisions require robust validation through multiple methodological approaches, academic publication or regulatory submission is planned, or sufficient computational resources allow for ensemble methods combining WSC with Augmented SCM and BSTS (Ben-Michael et al., 2021; Brodersen et al., 2015)."
  },
  {
    "objectID": "stella1.html#power-analysis-and-experimental-design-for-single-unit-scm",
    "href": "stella1.html#power-analysis-and-experimental-design-for-single-unit-scm",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Unlike randomized experiments, power analysis for SCM requires simulation-based approaches due to the complex dependence structure between treated and donor units.\nMinimum Detectable Effect Calculation:\n\\[\n\\text{MDE} = t_{\\alpha/2} \\cdot \\hat{\\sigma}_{\\text{placebo}} + t_{\\beta} \\cdot \\hat{\\sigma}_{\\text{placebo}}\n\\]\nwhere \\(\\hat{\\sigma}_{\\text{placebo}}\\) is estimated from historical placebo test distribution.\nStep-by-Step Power Analysis:\nStep 1: Historical Placebo Variance Estimation\nFor each donor j in historical data:\n    1. Apply SCM treating donor i as \"treated\"\n    2. Compute pseudo-effect: tau_i\n    3. Calculate placebo variance: œÉÃÇ¬≤_placebo = Var(tau_i)\nStep 2: Effect Size and Duration Calibration\n\nBusiness meaningful effect threshold (typically 3-8% for marketing)\nTreatment duration (balance statistical power with business urgency)\nPre-intervention period length (minimum 2x seasonal cycles)\n\nStep 3: Sample Size Requirements For target power of 80% and \\(\\alpha = 0.05\\):\n\\[\nN_{\\text{post}} \\geq \\frac{2 \\cdot (t_{0.025} + t_{0.2})^2 \\cdot \\sigma^2_{\\text{placebo}}}{\\text{MDE}^2}\n\\]\nWorked Example - E-commerce Campaign:\n\nHistorical placebo standard deviation: \\(\\hat{\\sigma}_{\\text{placebo}} = 0.04\\) (4%)\nTarget MDE: 5% revenue lift\nRequired post-treatment periods: \\(N_{\\text{post}} \\geq 8.2 \\approx 9\\) weeks\n\n\n\n\nWhen experimental design allows multiple treated units or staggered timing, adapt governance and diagnostics accordingly.\nStaggered Implementation Protocol:\n\nFirst-wave validation: Implement on 20-30% of treated units\nMid-course correction: Apply learnings to remaining units\nAggregate analysis: Use Generalized SCM for combined inference\n\nModified Diagnostic Framework:\n\nCross-unit holdout: Reserve some treated units entirely for validation\nTemporal heterogeneity: Test whether treatment effects vary by implementation timing\nSpillover detection: Monitor untreated units for contamination patterns\n\nConsensus Framework for Multiple Units: Effect estimates across units should show:\n\nDirectional consistency (same sign)\nMagnitude similarity (within 50% range)\nStatistical significance in majority of units"
  },
  {
    "objectID": "stella1.html#appendices",
    "href": "stella1.html#appendices",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Pre-Registration Requirements:\n\nTreatment definition and timing locked in analysis plan\nDonor inclusion/exclusion criteria with quantitative thresholds documented\nHoldout validation approach specified (fixed vs.¬†adaptive)\nPrimary and secondary outcomes defined with business significance thresholds\nStatistical inference procedures and significance levels pre-specified\nPower analysis completed with minimum detectable effect documented\n\nQuality Assurance Gates:\n\nDonor Quality Scorecard applied with documented component weights\nHoldout validation meets frequency-appropriate thresholds\nPlacebo tests demonstrate appropriate null behavior (p-value &gt; 0.1 for &gt;90% of donors)\nWeight concentration acceptable (Effective N &gt; 3)\nExternal event calendar reviewed for potential confounders\nSpillover risk assessment completed with geographic buffer analysis\n\nPost-Analysis Documentation:\n\nComplete donor weight matrices recorded to 4 decimal places\nPre-fit and holdout diagnostic metrics documented\nPlacebo test distributions with percentile rankings\nEffect estimates with confidence/credible intervals\nSensitivity analysis across key specification choices\nBusiness metrics (lift, iROAS) with uncertainty propagation\n\n\n\n\nDynamic and Time-Varying Approaches:\n\nOur contribution: Drift detection and regularized updating for marketing-specific non-stationarity\n\nPenalty-Augmented Objectives:\n\nAbadie et al.¬†(2015): ‚ÄúComparative Politics and the Synthetic Control Method‚Äù - general penalty guidance\nArkhangelsky & Imbens (2019): ‚ÄúThe Role of the Propensity Score in Fixed Effect Models‚Äù - distance-based priors\nOur contribution: Formalized business constraints with stakeholder credibility objectives\n\nRobust Synthetic Control:\n\nAmjad et al.¬†(2018): ‚ÄúRobust Synthetic Control‚Äù - algorithmic approaches to outlier donors\nOur contribution: Ex-ante quality assessment preventing poor donors from entering optimization\n\nLarge-Sample Properties and Inference:\n\nChernozhukov et al.¬†(2021): ‚ÄúAn Exact and Robust Conformal Inference Method‚Äù - formal inference procedures\nLi (2020): ‚ÄúStatistical inference for average treatment effects estimated by synthetic control methods‚Äù - bootstrap methods\nOur contribution: Decision rubrics for inference method selection based on practical constraints\n\n\n\n\nBusiness-Aware Regularization (Python):\n\n\nCode\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef business_aware_objective(weights, X_treated, X_donors, \n                            geo_penalty, comp_penalty, demo_penalty,\n                            lambda_stat=0.1, lambda_bus=0.05):\n    # Standard fit loss\n    synthetic = X_donors @ weights\n    fit_loss = np.sum((X_treated - synthetic)**2)\n    \n    # Statistical regularization (entropy)\n    stat_penalty = lambda_stat * np.sum(weights * np.log(weights + 1e-8))\n    \n    # Business penalties\n    geo_loss = lambda_bus * np.sum(weights * geo_penalty)\n    comp_loss = lambda_bus * np.sum(weights * comp_penalty)\n    demo_loss = lambda_bus * np.sum(weights * demo_penalty)\n    \n    return fit_loss + stat_penalty + geo_loss + comp_loss + demo_loss\n\n# Constraints and optimization\nconstraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\nbounds = [(0, None) for _ in range(n_donors)]\nresult = minimize(business_aware_objective, initial_weights, \n                 constraints=constraints, bounds=bounds)\n\n\nHierarchical Donor Screening:\n\n\nCode\ndef hierarchical_screening(treated_data, candidate_donors, \n                          correlation_threshold=0.3, max_donors=50):\n    # Stage 1: Correlation screening\n    correlations = [np.corrcoef(treated_data, donor)[0,1] \n                    for donor in candidate_donors]\n    stage1_donors = [d for d, c in zip(candidate_donors, correlations) \n                     if c &gt;= correlation_threshold]\n    \n    # Stage 2: Clustering-based reduction\n    if len(stage1_donors) &gt; max_donors:\n        # K-means clustering and representative selection\n        from sklearn.cluster import KMeans\n        features = np.array([extract_features(d) for d in stage1_donors])\n        kmeans = KMeans(n_clusters=max_donors)\n        clusters = kmeans.fit_predict(features)\n        \n        # Select donor closest to each cluster center\n        final_donors = []\n        for k in range(max_donors):\n            cluster_donors = [d for d, c in zip(stage1_donors, clusters) if c == k]\n            if cluster_donors:\n                center = kmeans.cluster_centers_[k]\n                distances = [np.linalg.norm(extract_features(d) - center) \n                             for d in cluster_donors]\n                final_donors.append(cluster_donors[np.argmin(distances)])\n    else:\n        final_donors = stage1_donors\n        \n    return final_donors"
  },
  {
    "objectID": "stella1.html#references-and-further-reading",
    "href": "stella1.html#references-and-further-reading",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Foundational Papers:\n\nAbadie, Alberto, and Javier Gardeazabal. ‚ÄúThe economic costs of conflict: A case study of the Basque Country.‚Äù American Economic Review 93, no. 1 (2003): 113-132.\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. ‚ÄúSynthetic Control Methods for Comparative Case Studies: Estimating the Effect of California‚Äôs Tobacco Control Program.‚Äù Journal of the American Statistical Association 105, no. 490 (2010): 493-505.\n\nRecent Methodological Advances:\n\nBen-Michael, Eli, Avi Feller, and Jesse Rothstein. ‚ÄúThe Augmented Synthetic Control Method.‚Äù Journal of the American Statistical Association 116, no. 536 (2021): 1789-1803.\nArkhangelsky, Dmitry, et al.¬†‚ÄúSynthetic Difference-in-Differences.‚Äù American Economic Review 111, no. 12 (2021): 4088-4118.\nXu, Yiqing. ‚ÄúGeneralized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models.‚Äù Political Analysis 25, no. 1 (2017): 57-76.\n\nBayesian and Time Series Methods:\n\nBrodersen, Kay H., et al.¬†‚ÄúInferring causal impact using Bayesian structural time-series models.‚Äù The Annals of Applied Statistics 9, no. 1 (2015): 247-274.\n\nRobustness and Extensions:\n\nAmjad, Muhammad, Devavrat Shah, and Dennis Shen. ‚ÄúRobust Synthetic Control.‚Äù Journal of Machine Learning Research 19, no. 1 (2018): 802-852.\nChernozhukov, Victor, et al.¬†‚ÄúAn Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.‚Äù Journal of the American Statistical Association 116, no. 536 (2021): 1849-1864.\n\nApplied Marketing and Economics:\n\nGordon, Brett R., et al.¬†‚ÄúA Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook.‚Äù Marketing Science 38, no. 2 (2019): 193-225.\nJohnson, Garrett A., Randall A. Lewis, and Elmar I. Nubbemeyer. ‚ÄúGhost Ads: Improving the Economics of Measuring Online Ad Effectiveness.‚Äù Journal of Marketing Research 54, no. 6 (2017): 867-884.\n\nDynamic and Time-Varying Methods:\n\nCao, Jian, and Thomas Chadefaux. ‚ÄúDynamic Synthetic Controls: Accounting for Varying Speeds in Comparative Case Studies.‚Äù Political Analysis 33, no. 1 (2025): 18‚Äì31. https://doi.org/10.1017/pan.2024.14.\nHyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. 3rd edition. Melbourne: OTexts, 2021.\n\nDistance-Based and Spillover Methods:\n\nSakaguchi, Shosei, and Hayato Tagawa. Identification and Inference for Synthetic Control Methods with Spillover Effects: Estimating the Economic Cost of the Sudan Split. 2024. arXiv:2408.00291. https://arxiv.org/abs/2408.00291"
  },
  {
    "objectID": "stella1.html#conclusion",
    "href": "stella1.html#conclusion",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Weighted Synthetic Control represents a mature and powerful methodology for causal inference when randomized experimentation is impractical or prohibitively expensive (Abadie et al., 2010). Its strength lies not merely in sophisticated mathematical optimization, but in the rigorous implementation of comprehensive validation frameworks, diagnostic procedures, and uncertainty quantification protocols.\nStella‚Äôs production deployment of WSC, encompassing automated donor screening, mandatory holdout validation, multi-method ensemble approaches, and comprehensive placebo testing, demonstrates how academic methodological rigor can be successfully operationalized for business-critical decision making. When implemented with appropriate guardrails‚Äîcredible donor pools, sufficient pre-intervention periods, robust validation procedures, and transparent governance‚ÄîWSC provides reliable causal insights that enable confident marketing investment decisions.\nThe methodology‚Äôs continued evolution, including augmented approaches for bias correction (Ben-Michael et al., 2021), generalized frameworks for complex treatment patterns (Xu, 2017), and Bayesian methods for full uncertainty characterization (Brodersen et al., 2015), ensures its relevance for increasingly sophisticated causal inference challenges. As marketing analytics matures toward more rigorous experimental design and causal identification strategies, mastery of synthetic control methods becomes essential for practitioners seeking to deliver credible, actionable insights in environments where perfect randomization remains elusive.\nSuccess with WSC requires balancing methodological sophistication with practical implementation constraints, maintaining healthy skepticism through comprehensive diagnostic testing, and clearly communicating both capabilities and limitations to business stakeholders. When these principles guide implementation, synthetic control methods unlock powerful causal inference capabilities that bridge the gap between observational data and experimental insights."
  },
  {
    "objectID": "stella1.html#author",
    "href": "stella1.html#author",
    "title": "A Practitioner‚Äôs Guide to Weighted Synthetic Control Methods for Incrementality Testing",
    "section": "",
    "text": "Jared Greathouse, PhD (Expected 2026)\nEverything econometrics and machine learning for causal inference.\nLinkedIn Profile\n\n\n\nJared Greathouse"
  },
  {
    "objectID": "averagesexcp.html",
    "href": "averagesexcp.html",
    "title": "What is An Average?",
    "section": "",
    "text": "Averages are the most basic summary statistic we are taught, aside from (I suppose) summation, and they arise quite naturally when we try to find a single value that best represents a collection of numbers. The arithmetic average is, for most people, more of an ontological concept than the solution to a decision problem. For the average person, even the average data scientist, is mentally more akin, to facts like ‚Äúthe sun rises‚Äù or ‚ÄúSpain is in Europe‚Äù instead of something to be derived, solved for, or even justified from first principles. Indeed (at least in the United States), the arithmetic mean is taught with almost a childlike innocence of simple addition and division. Regularly, people use phrases like ‚Äúon average‚Äù in casual conversation to denote frequency. In ye olden days, Egyptians and Babylonians used summary statistics in construction and financial matters. In modern times, we use averages to summarize things like test grades, incomes, and other variables we care about.\nBut what is an average anyways? Is there anything more to it? The point of this post is to show that the arithmetic average is the solution to an optimization problem, and connect it to how we think about averages in the context of causal inference.\n\n\nIn doing so, we will require a few rules. In the book I have a dedicated chapter on the required mathematics, but I repeat the definitions here for convenience.\n\n\n(optional mathematical background)\n\n\nDefinition 1 (The Derivative) For a scalar function \\(f : \\mathbb{R} \\to \\mathbb{R}\\), the derivative measures the instantaneous rate of change: \\[\n\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.\n\\]\nIntuition: How \\(f(x)\\) changes for an infinitesimal change in \\(x\\).\n\nDerivatives have rules attached to them. For example, one is the power rule:\n\nDefinition 2 (The Power Rule) If \\(f(x) = x^n\\) for \\(n \\in \\mathbb{R}\\): \\[\n\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = n x^{n-1}.\n\\]\nWorked example: \\(\\frac{\\mathrm{d}}{\\mathrm{d}x} x^3 = 3x^2\\).\n\nPretty straightforward. The derivative of \\(x^2\\) is just \\(2x\\). For \\(4q^5\\) it‚Äôs \\(20q^4\\). Another important rule is the chain rule.\n\nDefinition 3 (The Chain Rule) For \\(y = f(g(x))\\): \\[\n\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}f}{\\mathrm{d}g} \\cdot \\frac{\\mathrm{d}g}{\\mathrm{d}x}.\n\\]\n\n\nDefinition 4 (Objective Function) A scalar-valued function we aim to minimize or maximize:\n\\[\nf : \\mathbb{R}^n \\to \\mathbb{R},\n\\]\n\\[\n\\min_{\\mathbf{x}} f(\\mathbf{x}) \\quad \\text{or} \\quad \\max_{\\mathbf{x}} f(\\mathbf{x}).\n\\]\n\n\n\n\n\nTo formalize this, suppose we have a sequence of data points \\((x_1, x_2, \\ldots, x_n)\\) of length \\(n\\). We want to choose a single number, \\(\\mu\\) (pronounced ‚Äúm-you‚Äù), that is as close as possible to all of them. Here, \\(\\mu\\) is a placeholder variable representing a candidate for the ‚Äúbest‚Äù summary value, which we must solve for. For each observed value \\(x_i\\), the deviation from \\(\\mu\\) is \\(x_i - \\mu\\). However, we do not want to minimize the distance to a single number, we want a number that minimizes the discrepancy to all of the numbers. To measure the overall discrepancy across all points, we sum these deviations.\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nHowever, even \\(\\sum_{i=1}^{n} (x_i - \\mu)^2\\) is not enough because positive and negative deviations could cancel each other out. To penalize deviations regardless of sign, we square each term, giving the objective function:\n\\[\n\\mu = \\operatorname*{argmin}_{\\mu \\in \\mathbb{R}} \\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nThis function is convex and differentiable, so we can find its minimum by taking the derivative with respect to \\(\\mu\\). The function \\((x_i - \\mu)^2\\) is a composition of two functions, so we apply the chain rule. The outer function is \\(f(u) = u^2\\), where \\(u = x_i - \\mu\\), and its derivative is\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}u} u^2 = 2u = 2(x_i - \\mu).\n\\]\nThe inner function is \\(u(\\mu) = x_i - \\mu\\), with derivative\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} (x_i - \\mu) = -1.\n\\]\nMultiplying the outer and inner derivatives gives the derivative of a single term:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} (x_i - \\mu)^2 = 2(x_i - \\mu) \\cdot (-1) = -2(x_i - \\mu).\n\\]\nSumming over all \\(i\\) gives the derivative of the entire objective function:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} \\sum_{i=1}^{n} (x_i - \\mu)^2 = \\sum_{i=1}^{n} [-2(x_i - \\mu)] = -2 \\sum_{i=1}^{n} (x_i - \\mu),\n\\]\nwhere we factor out the constant -2 from the summation. Setting this derivative equal to zero gives the first-order condition:\n\\[\n-2 \\sum_{i=1}^{n} (x_i - \\mu) = 0.\n\\]\nNow we solve for the decision variable. Dividing both sides by -2 simplifies the equation to\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu) = 0.\n\\]\nNext, we split the sum into two parts to isolate \\(\\mu\\):\n\\[\n\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\mu = 0.\n\\]\nHere \\(\\mu\\) is the same number for every term in the sum and does not depend on \\(i\\). Why? That is exactly what we seek: one single value that is as close as possible to all the data points at once. Because \\(\\mu\\) is constant with respect to the index \\(i\\), the second sum is simply \\(\\mu\\) added to itself \\(n\\) times, which is equivalent to multiplying \\(\\mu\\) by \\(n\\):\n\\[\n\\sum_{i=1}^{n} \\mu = n \\mu\n\\]\nSo the equation simplifies to:\n\\[\n\\sum_{i=1}^{n} x_i - n\\mu = 0.\n\\]\nRearranging the equation to isolate \\(\\mu\\), add \\(n\\mu\\) to the right hand side, giving us:\n\\[\n\\sum_{i=1}^{n} x_i = n \\mu.\n\\]\nFinally, we arrive at the arithmetic mean:\n\\[\n\\boxed{\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i}\n\\]\n\nDefinition 5 (Arithmetic Average) For some \\((x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\\), the arithmetic average is: \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) where \\(n\\) is the length of the sequence and \\(\\frac{1}{n}\\) is the weight.\n\nThus, the value of \\(\\mu\\) that minimizes the total squared deviation is the arithmetic mean of the data. The arithmetic mean is not an arbitrary formula: it is the unique value that minimizes the total squared distance between itself and the data.\nAnother way to think about this result is geometrically, using the language of Hilbert spaces. Consider the vector of data points\n\\[\n\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\n\\]\nequipped with the standard inner product\n\\[\n\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\sum_{i=1}^{n} a_i b_i.\n\\]\nMinimizing the sum of squared deviations\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nis equivalent to finding the orthogonal projection of \\(\\mathbf{x}\\) onto the one-dimensional subspace spanned by the vector \\(\\mathbf{1} = (1, 1, \\dots, 1)\\). The solution \\(\\mu\\) is exactly the scalar that defines this projection:\n\\[\n\\boxed{\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i}.\n\\]\nFrom this perspective, the arithmetic mean is the vector in the direction of \\(\\mathbf{1}\\) that is closest in Euclidean distance to the data vector \\(\\mathbf{x}\\).\nNow, you may be saying, ‚ÄúHey Jared, this is overkill. Why bother doing all of this for something as simple as an average?‚Äù The derivation shows us that the average is the solution to an optimization problem.\nThat single insight connects simple statistics to the machinery of modern econometrics. Variance, mean squared error, and regression coefficients are all built on the same principle: finding a number, or set of numbers, that minimizes some measure of error. The arithmetic mean is simply the most familiar and elementary instance of this idea. Every estimator in this book is, at its core, an application of averaging. This fact will become crucial in later chapters, when we move from simple averages to more sophisticated ones, including the weighted averages that power synthetic control methods. Whatever method you choose, you‚Äôre never not averaging.\nThe reason I dedicate a chapter to the arithmetic mean is also to make sure we start on the right footing. For some reason, synthetic control methods are often described in dramatic terms: as a Frankenstein‚Äôs monster, a mash-up of controls, or a ‚Äúfake version‚Äù of the treated unit. For example, Haus and Meta scientists have used these kinds of analogies to frame SCM. Other descriptions present variations of the same idea, including the claim that SCM constructs a ‚Äúfake control group.‚Äù\nWhile I understand the intent behind this phrasing, I believe it is, and at the very least has a high potential to be, misleading. As we have just seen, the arithmetic mean is itself a special case of a weighted average, one in which all units receive equal weight. Yet we do not describe the national average income as ‚ÄúFranken-Income,‚Äù nor the weekly mean of basket size as a ‚ÄúFranken-Basket.‚Äù Likewise, the idea of a ‚Äúfake control group‚Äù obscures the fact that every unit in the donor pool is real, observed data. Synthetic control predictions, like any other average, do not fabricate data at all. They simply assign weights, subject to constraints, in exactly the same spirit as more familiar averages.\nIn fact, these descriptions become even more puzzling once we notice that the arithmetic mean already satisfies the classic SCM constraints. Recall the formula\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nWe can rewrite this as a weighted sum by setting \\(w_i = 1/n\\) for all \\(i\\), yielding \\(\\sum_{i=1}^n y_i w_i\\). These weights are non-negative and sum to one, which is precisely the defining constraints of the synthetic control method. The arithmetic mean of the donor pool, therefore, is a special case of SCM in which all weights are uniform. The only meaningful difference is that classic SCM does not impose uniformity.\nYet the moment we allow non-uniform weights with \\(\\sum_i w_i = 1\\), the same operation is suddenly described as sophisticated or different or complex, even though the weights from SCM, like the arithmetic mean, are also byproduct of an optimization problem. The key difference is that the arithmetic average is trivially closed form and the SCM weights are not. If a client asked me to compute the average of sales across markets over time and I replied, ‚ÄúYou mean you want me to make a mashed-up Frankenstein monster of your sales data?‚Äù they would rightly think I had lost the plot.\nThe point is simple: every single estimator in this book uses a weighted average of controls which is derived from an optimization problem, whether we are discussing Difference-in-Differences or SCM. The arithmetic mean is merely the simplest case, and synthetic control methods are just a careful generalization of that fact."
  },
  {
    "objectID": "averagesexcp.html#definitions",
    "href": "averagesexcp.html#definitions",
    "title": "What is An Average?",
    "section": "",
    "text": "In doing so, we will require a few rules. In the book I have a dedicated chapter on the required mathematics, but I repeat the definitions here for convenience.\n\n\n(optional mathematical background)\n\n\nDefinition 1 (The Derivative) For a scalar function \\(f : \\mathbb{R} \\to \\mathbb{R}\\), the derivative measures the instantaneous rate of change: \\[\n\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.\n\\]\nIntuition: How \\(f(x)\\) changes for an infinitesimal change in \\(x\\).\n\nDerivatives have rules attached to them. For example, one is the power rule:\n\nDefinition 2 (The Power Rule) If \\(f(x) = x^n\\) for \\(n \\in \\mathbb{R}\\): \\[\n\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = n x^{n-1}.\n\\]\nWorked example: \\(\\frac{\\mathrm{d}}{\\mathrm{d}x} x^3 = 3x^2\\).\n\nPretty straightforward. The derivative of \\(x^2\\) is just \\(2x\\). For \\(4q^5\\) it‚Äôs \\(20q^4\\). Another important rule is the chain rule.\n\nDefinition 3 (The Chain Rule) For \\(y = f(g(x))\\): \\[\n\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}f}{\\mathrm{d}g} \\cdot \\frac{\\mathrm{d}g}{\\mathrm{d}x}.\n\\]\n\n\nDefinition 4 (Objective Function) A scalar-valued function we aim to minimize or maximize:\n\\[\nf : \\mathbb{R}^n \\to \\mathbb{R},\n\\]\n\\[\n\\min_{\\mathbf{x}} f(\\mathbf{x}) \\quad \\text{or} \\quad \\max_{\\mathbf{x}} f(\\mathbf{x}).\n\\]"
  },
  {
    "objectID": "averagesexcp.html#deriving-the-arithmetic-mean",
    "href": "averagesexcp.html#deriving-the-arithmetic-mean",
    "title": "What is An Average?",
    "section": "",
    "text": "To formalize this, suppose we have a sequence of data points \\((x_1, x_2, \\ldots, x_n)\\) of length \\(n\\). We want to choose a single number, \\(\\mu\\) (pronounced ‚Äúm-you‚Äù), that is as close as possible to all of them. Here, \\(\\mu\\) is a placeholder variable representing a candidate for the ‚Äúbest‚Äù summary value, which we must solve for. For each observed value \\(x_i\\), the deviation from \\(\\mu\\) is \\(x_i - \\mu\\). However, we do not want to minimize the distance to a single number, we want a number that minimizes the discrepancy to all of the numbers. To measure the overall discrepancy across all points, we sum these deviations.\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nHowever, even \\(\\sum_{i=1}^{n} (x_i - \\mu)^2\\) is not enough because positive and negative deviations could cancel each other out. To penalize deviations regardless of sign, we square each term, giving the objective function:\n\\[\n\\mu = \\operatorname*{argmin}_{\\mu \\in \\mathbb{R}} \\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nThis function is convex and differentiable, so we can find its minimum by taking the derivative with respect to \\(\\mu\\). The function \\((x_i - \\mu)^2\\) is a composition of two functions, so we apply the chain rule. The outer function is \\(f(u) = u^2\\), where \\(u = x_i - \\mu\\), and its derivative is\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}u} u^2 = 2u = 2(x_i - \\mu).\n\\]\nThe inner function is \\(u(\\mu) = x_i - \\mu\\), with derivative\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} (x_i - \\mu) = -1.\n\\]\nMultiplying the outer and inner derivatives gives the derivative of a single term:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} (x_i - \\mu)^2 = 2(x_i - \\mu) \\cdot (-1) = -2(x_i - \\mu).\n\\]\nSumming over all \\(i\\) gives the derivative of the entire objective function:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\mu} \\sum_{i=1}^{n} (x_i - \\mu)^2 = \\sum_{i=1}^{n} [-2(x_i - \\mu)] = -2 \\sum_{i=1}^{n} (x_i - \\mu),\n\\]\nwhere we factor out the constant -2 from the summation. Setting this derivative equal to zero gives the first-order condition:\n\\[\n-2 \\sum_{i=1}^{n} (x_i - \\mu) = 0.\n\\]\nNow we solve for the decision variable. Dividing both sides by -2 simplifies the equation to\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu) = 0.\n\\]\nNext, we split the sum into two parts to isolate \\(\\mu\\):\n\\[\n\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\mu = 0.\n\\]\nHere \\(\\mu\\) is the same number for every term in the sum and does not depend on \\(i\\). Why? That is exactly what we seek: one single value that is as close as possible to all the data points at once. Because \\(\\mu\\) is constant with respect to the index \\(i\\), the second sum is simply \\(\\mu\\) added to itself \\(n\\) times, which is equivalent to multiplying \\(\\mu\\) by \\(n\\):\n\\[\n\\sum_{i=1}^{n} \\mu = n \\mu\n\\]\nSo the equation simplifies to:\n\\[\n\\sum_{i=1}^{n} x_i - n\\mu = 0.\n\\]\nRearranging the equation to isolate \\(\\mu\\), add \\(n\\mu\\) to the right hand side, giving us:\n\\[\n\\sum_{i=1}^{n} x_i = n \\mu.\n\\]\nFinally, we arrive at the arithmetic mean:\n\\[\n\\boxed{\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i}\n\\]\n\nDefinition 5 (Arithmetic Average) For some \\((x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\\), the arithmetic average is: \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) where \\(n\\) is the length of the sequence and \\(\\frac{1}{n}\\) is the weight.\n\nThus, the value of \\(\\mu\\) that minimizes the total squared deviation is the arithmetic mean of the data. The arithmetic mean is not an arbitrary formula: it is the unique value that minimizes the total squared distance between itself and the data.\nAnother way to think about this result is geometrically, using the language of Hilbert spaces. Consider the vector of data points\n\\[\n\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\n\\]\nequipped with the standard inner product\n\\[\n\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\sum_{i=1}^{n} a_i b_i.\n\\]\nMinimizing the sum of squared deviations\n\\[\n\\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nis equivalent to finding the orthogonal projection of \\(\\mathbf{x}\\) onto the one-dimensional subspace spanned by the vector \\(\\mathbf{1} = (1, 1, \\dots, 1)\\). The solution \\(\\mu\\) is exactly the scalar that defines this projection:\n\\[\n\\boxed{\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i}.\n\\]\nFrom this perspective, the arithmetic mean is the vector in the direction of \\(\\mathbf{1}\\) that is closest in Euclidean distance to the data vector \\(\\mathbf{x}\\).\nNow, you may be saying, ‚ÄúHey Jared, this is overkill. Why bother doing all of this for something as simple as an average?‚Äù The derivation shows us that the average is the solution to an optimization problem.\nThat single insight connects simple statistics to the machinery of modern econometrics. Variance, mean squared error, and regression coefficients are all built on the same principle: finding a number, or set of numbers, that minimizes some measure of error. The arithmetic mean is simply the most familiar and elementary instance of this idea. Every estimator in this book is, at its core, an application of averaging. This fact will become crucial in later chapters, when we move from simple averages to more sophisticated ones, including the weighted averages that power synthetic control methods. Whatever method you choose, you‚Äôre never not averaging.\nThe reason I dedicate a chapter to the arithmetic mean is also to make sure we start on the right footing. For some reason, synthetic control methods are often described in dramatic terms: as a Frankenstein‚Äôs monster, a mash-up of controls, or a ‚Äúfake version‚Äù of the treated unit. For example, Haus and Meta scientists have used these kinds of analogies to frame SCM. Other descriptions present variations of the same idea, including the claim that SCM constructs a ‚Äúfake control group.‚Äù\nWhile I understand the intent behind this phrasing, I believe it is, and at the very least has a high potential to be, misleading. As we have just seen, the arithmetic mean is itself a special case of a weighted average, one in which all units receive equal weight. Yet we do not describe the national average income as ‚ÄúFranken-Income,‚Äù nor the weekly mean of basket size as a ‚ÄúFranken-Basket.‚Äù Likewise, the idea of a ‚Äúfake control group‚Äù obscures the fact that every unit in the donor pool is real, observed data. Synthetic control predictions, like any other average, do not fabricate data at all. They simply assign weights, subject to constraints, in exactly the same spirit as more familiar averages.\nIn fact, these descriptions become even more puzzling once we notice that the arithmetic mean already satisfies the classic SCM constraints. Recall the formula\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nWe can rewrite this as a weighted sum by setting \\(w_i = 1/n\\) for all \\(i\\), yielding \\(\\sum_{i=1}^n y_i w_i\\). These weights are non-negative and sum to one, which is precisely the defining constraints of the synthetic control method. The arithmetic mean of the donor pool, therefore, is a special case of SCM in which all weights are uniform. The only meaningful difference is that classic SCM does not impose uniformity.\nYet the moment we allow non-uniform weights with \\(\\sum_i w_i = 1\\), the same operation is suddenly described as sophisticated or different or complex, even though the weights from SCM, like the arithmetic mean, are also byproduct of an optimization problem. The key difference is that the arithmetic average is trivially closed form and the SCM weights are not. If a client asked me to compute the average of sales across markets over time and I replied, ‚ÄúYou mean you want me to make a mashed-up Frankenstein monster of your sales data?‚Äù they would rightly think I had lost the plot.\nThe point is simple: every single estimator in this book uses a weighted average of controls which is derived from an optimization problem, whether we are discussing Difference-in-Differences or SCM. The arithmetic mean is merely the simplest case, and synthetic control methods are just a careful generalization of that fact."
  }
]