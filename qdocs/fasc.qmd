---
title: "Forward Selected Augmented Synthetic Controls"
date: 2025-08-17
categories: [Causal Inference, Econometrics]
---

# Introduction

[Synthetic Control Methods](https://archive.is/20230120121731/https://www.washingtonpost.com/news/wonk/wp/2015/10/30/how-to-measure-things-in-a-world-of-competing-claims/) (SCM) is a widely used framework for estimating causal effects when randomized experiments are not feasible. At its core, SCM constructs a weighted average of control (donor) units to approximate the treated unit’s pre-treatment trajectory. The goal is to find an in-sample/pre--treatment average of controls that closely mirrors the treated unit before the intervention.

Much of the method’s credibility hinges on the quality of this pre-treatment fit. Econometricians [regularly](https://mixtape.scunning.com/10-synthetic_control) [warn](https://arxiv.org/pdf/2203.06279) that poor pre-treatment fit undermines the validity of SCM estimates. Even if the optimization problem is formally well-posed, poor alignment between the treated unit and its in-sample match can lead to substantial bias. The intuition is straightforward: if similar units are [assumed to behave similarly](https://doi.org/10.3982/ECTA21248), a control group that fails to mimic the treated unit before treatment is unlikely to produce a credible counterfactual afterward. Just as important as pre-treatment fit is the composition of the donor pool. Including irrelevant or poorly matched units, or omitting relevant ones, can distort the synthetic weights and lead to misleading inferences. But how should the donor pool be chosen?

One increasingly popular solution to the imperfect match is the Augmented Synthetic Control Method (ASCM), known in industry through Meta’s [GeoLift](https://facebookincubator.github.io/GeoLift/) library. Shops like [Recast](https://geolift-docs.getrecast.com/docs/) use it, and data scientists such as [Mandy Liu](https://medium.com/data-science/how-to-analyze-geo-based-campaigns-with-synthetic-control-a90b839479a8) and [Svet Semov](https://www.linkedin.com/posts/svet-semov-79072913_metas-geolift-activity-7341471937493200896-SFvd?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2Q8asBlsCYlwKJgJ488VWbcV1CX14FdOw) have helped bring it to applied audiences.

Methods for donor pool selection have also received attention. In fact, this is part of what makes GeoLift so popular: it [attempts to identify](https://github.com/facebookincubator/GeoLift/blob/main/R/pre_test_power.R#L37) the most similar markets to a treated group before the intervention. In academic settings, approaches like [forward](https://doi.org/10.1016/j.jeconom.2021.04.009) [selection](https://pubsonline.informs.org/doi/10.1287/mksc.2022.0212), and even [random forests](https://doi.org/10.1002/jae.3123) have been proposed to automate or guide the choice of appropriate donors.


But what if we can do better?

In previous posts, I’ve written about donor selection strategies and how to handle imperfect pre-treatment fit. In this post, I introduce a synthesis of both: the Augmented Forward-Selected Synthetic Control estimator. By combining forward selection with a bias correction estimator, I show that we can reduce in-sample risk relative to the standard ASCM and Forward SCM alone. This approach is illustrated using two popular SCM case studies: the Kansas tax cut experiment and California’s Proposition 99.

### Notations

Formally, let $\mathbb{R}$ denote the set of real numbers. A calligraphic letter, such as $\mathcal{S}$, represents a discrete set with cardinality $S = |\mathcal{S}|$. Let $j \in \mathbb{N}$ index a total of $N$ units and $t \in \mathbb{N}$ index time. Unit $j=1$ is the treated unit, with the set of control units defined as $\mathcal{N}_0 = \mathcal{N} \setminus \{1\}$, of cardinality $N_0$. The pre-treatment period is $\mathcal{T}_1 = \{ t \in \mathbb{N} : t \leq T_0 \}$, where $T_0$ is the last period before treatment, and the post-treatment period is $\mathcal{T}_2 = \{ t \in \mathbb{N} : t > T_0 \}$. The observed outcome for unit $j$ at time $t$ is $y_{jt}$, and the generic outcome vector for unit $j$ is $\mathbf{y}_j \in \mathbb{R}^T$, so that $\mathbf{y}_j = (y_{j1}, y_{j2}, \dots, y_{jT})^\top$. The treated unit’s outcome vector is $\mathbf{y}_1$, and the donor matrix is defined as $\mathbf{Y}_0 \coloneqq \begin{bmatrix} \mathbf{y}_j \end{bmatrix}_{j \in \mathcal{N}_0} \in \mathbb{R}^{T \times N_0}$, with each column corresponding to a donor unit and each row corresponding to a time period.

I will frequently refer to linear combinations of the donor units’ outcomes. Two important geometric concepts help clarify the space of possible synthetic controls: The convex hull of the donor units, denoted $\operatorname{conv}(\mathbf{y}_j)_{j \in \mathcal{N}_0}$, is the set of all weighted averages of donor vectors where the weights are non-negative and sum to one:
  
$$
\operatorname{conv}(\mathbf{y}_j) = \left\{ \sum_{j \in \mathcal{N}_0} w_j \mathbf{y}_j \,\middle|\, w_j \geq 0, \sum_{j} w_j = 1 \right\}.
$$
This is the feasible region for standard SCM weights $\mathbf{w} \in \Delta^{N_0}$. The affine hull, denoted $\operatorname{aff}(\mathbf{y}_j)_{j \in \mathcal{N}_0}$, is the set of all affine combinations of donor vectors — that is, the set of linear combinations where the weights sum to one, but may include negative values:

$$
  \operatorname{aff}(\mathbf{y}_j) = \left\{ \sum_{j \in \mathcal{N}_0} w_j \mathbf{y}_j \,\middle|\, \sum_{j} w_j = 1 \right\}.
$$
This is the feasible region for the ASC estimator. While both are weighted averages, the convex hull is a "weighted average" over the subsapce of the donor outcomes, while the affine hull is a "shifted subspace" that contains the convex hull but allows for extrapolation beyond it.

We define the corresponding sets of feasible weights as:

$$
\mathcal{W}_{\mathrm{conv}} = \left\{ \mathbf{w} \in \mathbb{R}^{N_0} \mid \mathbf{w} \geq \mathbf{0}, \; \|\mathbf{w}\|_1 = 1 \right\},
$$

$$
\mathcal{W}_{\mathrm{aff}} = \left\{ \mathbf{w} \in \mathbb{R}^{N_0} \mid \mathbf{1}^\top \mathbf{w} = 1 \right\},
$$

where $\|\mathbf{w}\|_1 = \sum_{j=1}^{N_0} |w_j|$ denotes the $\ell_1$ norm of the weight vector $\mathbf{w}$, and $\mathbf{1}$ is the vector of ones in $\mathbb{R}^{N_0}$. Note that for $\mathcal{W}_{\mathrm{conv}}$, the weights are non-negative and sum to one (equivalently, their $\ell_1$ norm is one), whereas for $\mathcal{W}_{\mathrm{aff}}$ weights can be negative but must sum to one.

```{python}

#| fig-align: center
#| echo: false


import numpy as np
import matplotlib.pyplot as plt

# Two donor points in 2D
donors = np.array([
    [0, 0],
    [2, 2]
])

plt.figure(figsize=(8, 6))

# Plot donor points
plt.scatter(donors[:, 0], donors[:, 1], color='blue', label='Donor units')

# Plot convex hull: line segment between the two points
plt.plot(donors[:, 0], donors[:, 1], 'k-', label='Convex hull (line segment)')

# Define the affine hull line (infinite line)
line_x = np.linspace(-2, 4, 500)
slope = (donors[1,1] - donors[0,1]) / (donors[1,0] - donors[0,0])
line_y = slope * (line_x - donors[0,0]) + donors[0,1]

# Plot affine hull line
plt.plot(line_x, line_y, 'gray', linestyle='--', label='Affine hull (infinite line)')

# Shade a band around the affine hull line
band_width = 0.4  # vertical width of band

# Compute offsets perpendicular to the line
# Perp vector to (dx, dy) = (-dy, dx)
dx = donors[1,0] - donors[0,0]
dy = donors[1,1] - donors[0,1]
length = np.sqrt(dx**2 + dy**2)
perp = np.array([-dy, dx]) / length

# Points for upper and lower band boundaries
upper_band = np.vstack([line_x, line_y]).T + perp * band_width / 2
lower_band = np.vstack([line_x, line_y]).T - perp * band_width / 2

# Fill between upper and lower bands
plt.fill_between(line_x,
                 upper_band[:,1],
                 lower_band[:,1],
                 color='gray', alpha=0.15, label='Affine Hull')

# Convex combination (inside segment)
w_convex = np.array([0.3, 0.7])  # weights sum to 1, >= 0
point_convex = donors.T @ w_convex
plt.scatter(*point_convex, color='green', s=100, label='Convex Point')

# Affine combination (outside segment but on line)
w_affine = np.array([1.5, -0.5])  # weights sum to 1, one negative allowed
point_affine = donors.T @ w_affine
plt.scatter(*point_affine, color='red', s=100, label='Affine Point')

# Point outside affine hull (off the line)
point_outside = np.array([1, 3])
plt.scatter(*point_outside, color='purple', s=100, label='Outside')

plt.legend()
plt.title("Convex Hull vs. Affine Hull")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.grid(True)
plt.axis('equal')
plt.show()




```

## Synthetic Controls

SCM solves the program

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{conv}}}{\operatorname*{argmin}} \; \left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \right\|_2^2 \quad \forall t \in \mathcal{T}_1.
$$

We seek the weight vector $\mathbf{w}$ that minimizes the mean squared error between the treated unit outcomes and the weighted average of control units in the pre-treatment period. The Forward Selection variant (ideally) builds a sparse synthetic control by greedily adding one donor at a time to minimize the pre-treatment fit error. Let $S = \emptyset$ denote the initial (empty) set of selected donors. At each iteration, the algorithm considers every donor $j \notin S$, temporarily forming a new set $S' = S \cup \{j\}$. For each such candidate set $S'$, it solves the synthetic control optimization problem restricted to the columns of $\mathbf{Y}_0^{S'}$, where $\mathbf{Y}_0^{S'}$ denotes the submatrix of donor outcomes for units in $S'$. The donor $j^{\ast}$ whose inclusion yields the smallest pre-treatment RMSE is then added to $S$. This process continues [at the discretion of the user](https://mlsynth.readthedocs.io/en/latest/fscm.html#fscm-function), until either all/some portion of donors have been exhausted, or a model selection criterion, the modified BIC, begins to increase and `full_selection = FALSE`. The modified BIC is defined as:

$$
\text{mBIC}(S) = T_0 \cdot \log(\text{MSE}) + |S| \cdot \log(T_0)
$$

where

$$
\text{MSE} = \frac{1}{T_0} \left\| \mathbf{y}_1 - \mathbf{Y}_0^S \mathbf{w}_S \right\|_2^2.
$$

This penalized error criterion trades off in-sample fit and model complexity, enabling the construction of a sparse but well-performing synthetic control using only a small, carefully chosen subset of donor units.

## The Augmented Synthetic Control Estimator

Building on this baseline formulation, the ASCM introduces a regularization term that penalizes deviations of the weight vector from a reference or initial weight vector, $\mathbf{w}_0$. The augmented objective can be written as

$$
\mathbf{w}^\ast_{\mathrm{aug}} = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \; \left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \right\|_2^2 + \lambda \left\| \mathbf{w} - \mathbf{w}_0 \right\|_2^2 \quad \forall t \in \mathcal{T}_1.
$$

where $\lambda \ge 0$ controls the strength of the penalty.

<details>
  <summary>About that lambda...</summary>
::: {.callout-note}
One may ask *"Jared, why did you include the penalty on the weight deviation term instead of the fit term, as Ben-Michael and co. do in Equation 18 of their paper?"* Here’s why.

In ASCM, the placement of the regularization parameter $\lambda$ determines how the estimator balances pre-treatment fit and fidelity to the original SCM weights. Their formulation (e.g., Ben-Michael et al. 2021) minimizes:

$$
\mathbf{w}^\ast_{\text{alt}} = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \; \lambda \|\mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w}\|_2^2 + \|\mathbf{w} - \mathbf{w}_0\|_2^2
$$

while ours solves:

$$
\mathbf{w}^\ast_{\text{aug}} = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \; \|\mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w}\|_2^2 + \lambda \|\mathbf{w} - \mathbf{w}_0\|_2^2
$$

It turns out these are mathematically equivalent under a simple reparameterization. If we define $\lambda_{\text{alt}} = \frac{1}{\lambda_{\text{aug}}}$, then both objectives yield the same solution. This follows directly from the first-order conditions of each problem, which differ only by a scaling of the Lagrange multiplier.

So in truth, it’s just a matter of which interpretation you find more natural.

I personally prefer our formulation. Here’s why: in the affine-regularized version of the forward-selected synthetic control, $\mathbf{w}_0$ comes from a deliberately chosen sparse model. As $\lambda \to \infty$, the penalty on deviation dominates, and $\mathbf{w}^\ast_{\text{aug}}$ collapses to the projection of $\mathbf{w}_0$ onto the affine constraint $\sum_j w_j = 1$. This makes intuitive sense: when the original FSCM fit is already good, we want to stick close to it.

Conversely, as $\lambda \to 0$, the regularization term disappears and the solution becomes the best-fitting affine combination of the donor units — completely unconstrained by the initial weights. That’s appropriate when the original fit is poor and we’re willing to learn something new.

So while the math is equivalent, the perspective isn’t. I find it more natural to think of $\lambda$ as controlling how much I "trust" the prior weights. And that’s easier to reason about when $\lambda$ is attached to the deviation term.
:::
</details>

The intuition here is pretty simple. If the SCM weights are already giving us good pre-treatment fit, then there is little incentive to extrapolate away from the original (F)SCM solution. However, if there's need for better fit, then we will extrapolate away from the convex hull solution. In practice, BMFR advocate for choosing lambda via cross validation.

# Forward-Selected Augmented Synthetic Controls

The Forward-Selected Augmented Synthetic Control (FASC) estimator synthesizes two complementary strategies in SCM research: forward selection of donors and bias correction via the discrepancy correction It begins with a Forward SCM, selecting a small set of donors that best approximate the treated unit in the pre-treatment period via forward selection. Then, instead of stopping there, it augments this base model with a bias-corrected adjustment by seeing whether an affine combination can do much better than the convex combination, using the original weights as a warm start. The FS-ASC estimator solves:

$$
\mathbf{w}^\ast_{\text{FASC}} = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \; \left\| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \right\|_2^2 + \lambda \left\| \mathbf{w} - \mathbf{w}_0 \right\|_2^2.
$$

The solution lies in the affine hull of all donors, but it is regularized to remain close to the sparse, interpretable model obtained from forward selection on the condition that the originally selected model is good. The strength of this regularization is governed by the parameter $\lambda$.

The regularization parameter $\lambda$ is selected via time-split cross-validation on the pre-treatment period. First, use the first half ($t = 1, \dots, \lfloor T_0/2 \rfloor$) to fit candidate weights $\mathbf{w}(\lambda)$. Then, use the second half ($t = \lfloor T_0/2 \rfloor + 1, \dots, T_0$) to evaluate prediction error. For each candidate $\lambda$, we compute:

$$
\mathbf{w}(\lambda) = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \left\| \mathbf{y}^{(1)} - \mathbf{Y}_0^{(1)} \mathbf{w} \right\|_2^2 + \lambda \left\| \mathbf{w} - \mathbf{w}_0 \right\|_2^2.
$$
The expression $\text{CV-RMSE}(\lambda)$ measures the out-of-sample prediction error on the second half of the pre-treatment period when using weights $\mathbf{w}(\lambda)$ fitted on the first half. Specifically, it calculates the Euclidean norm (root sum of squared differences) between the observed outcomes $\mathbf{y}^{(2)}$ of the treated unit and the synthetic control’s predicted outcomes $\mathbf{Y}_0^{(2)} \mathbf{w}(\lambda)$. Minimizing this cross-validated root mean squared error helps select the regularization parameter $\lambda$ that balances fit and regularization to improve generalization to unseen (post-first-half) data.
$$
\text{CV-RMSE}(\lambda) = \left\| \mathbf{y}^{(2)} - \mathbf{Y}_0^{(2)} \mathbf{w}(\lambda) \right\|_2.
$$

To efficiently explore the hyperparameter space, I use `skopt`'s Bayesian optimization over $\log_{10}(\lambda) \in [-2, 3]$, corresponding to $\lambda \in [10^{-2}, 10^3]$. The optimization proceeds as follows: A Gaussian Process (GP) prior is placed over the unknown function $\lambda \mapsto \text{CV-RMSE}(\lambda)$. This GP models both the mean and uncertainty of the objective function. The algorithm begins with a small number of initial random evaluations. At each step, the next $\lambda$ to evaluate is chosen by maximizing an acquisition function, in this case the Expected Improvement (EI), which balances exploration and exploitation. This process continues for a total of 50 steps (or as specified). Once the optimal $\lambda^\ast$ is identified, the final weight vector is estimated by solving:

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathcal{W}_{\mathrm{aff}}}{\operatorname*{argmin}} \left\| \mathbf{y} - \mathbf{Y}_0 \mathbf{w} \right\|_2^2 + \lambda^{\ast} \left\| \mathbf{w} - \mathbf{w}_0 \right\|_2^2.
$$

The resulting weights lie in the affine hull of the donor pool and strike a balance between fitting the treated unit and remaining close to the original sparse solution. As above, when $\lambda$ is large, the augmented weights remain close to $\mathbf{w}_0$; when $\lambda$ is small, the estimator prioritizes fit over sparsity and may extrapolate well beyond the original donor set. In essence, FS-ASC gives us the best of both worlds: interpretability and parsimony from sparse donor selection, and flexibility and reduced bias from affine augmentation. Empirically (we will see this below), it yields lower pre-treatment error than either method alone.

```{python}
#| fig-align: center
#| echo: false

import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch

def draw_box(ax, text, xy, boxstyle="round,pad=0.3", fc="lightblue", ec="black", fontsize=12):
    box = FancyBboxPatch(
        (xy[0] - 1.5, xy[1] - 0.5), 3, 1,
        boxstyle=boxstyle,
        linewidth=1.5,
        facecolor=fc,
        edgecolor=ec
    )
    ax.add_patch(box)
    ax.text(xy[0], xy[1], text, ha="center", va="center", fontsize=fontsize, fontweight='bold', color='black')

def draw_arrow(ax, start, end):
    arrow = FancyArrowPatch(
        start, end,
        arrowstyle='-|>', mutation_scale=20,
        linewidth=2,
        color='darkblue'
    )
    ax.add_patch(arrow)

fig, ax = plt.subplots(figsize=(7, 9))
ax.set_xlim(0, 4)
ax.set_ylim(0, 10)
ax.axis("off")

# Positions for boxes (x, y)
positions = [
    (2, 9),  # Donor Pool Data
    (2, 8),  # Forward Selection
    (2, 7),  # Obtain Sparse Weights w0
    (2, 5),  # Cross-Validation to tune lambda
    (2, 4),  # Augmented SCM Optimization
    (2, 2),  # Final weights w*
]

texts = [
    "Donor Pool Data",
    "Forward Selection:\nIterative Donor Choice",
    "Obtain Sparse Weights\n$\\mathbf{w}_0$",
    "Cross-Validation\nto Tune $\\lambda$",
    "Augmented SCM\nOptimization",
    "Final Weights\n$\\mathbf{w}^*$"
]

# Colors for each box
colors = [
    "#FFD700",  # gold
    "#87CEEB",  # skyblue
    "#40E0D0",  # turquoise
    "#FFA07A",  # lightsalmon
    "#9370DB",  # mediumpurple
    "#90EE90",  # lightgreen
]

# Draw boxes with colors
for pos, text, col in zip(positions, texts, colors):
    draw_box(ax, text, pos, fc=col)

# Draw arrows between boxes
arrow_pairs = [(0,1), (1,2), (2,3), (3,4), (4,5)]
for i, j in arrow_pairs:
    start = (positions[i][0], positions[i][1] - 0.5)
    end = (positions[j][0], positions[j][1] + 0.5)
    draw_arrow(ax, start, end)

plt.tight_layout()
plt.show()
```

## Conformal Prediction Intervals

To quantify uncertainty around the estimated counterfactual trajectory, we apply [conformal prediction intervals](https://matheusfacure.github.io/python-causality-handbook/Conformal-Inference-for-Synthetic-Control.html) based on block-permuted pre-treatment residuals. These intervals are distribution-free, require no assumptions about the data-generating process, and provide valid finite-sample marginal coverage. Let $\hat{y}_t^{\text{cf}}$ denote the estimated counterfactual outcome at time $t$, and let $y_t^{\text{obs}}$ be the observed outcome. We begin by computing residuals for all time periods:

$$
\varepsilon_t = y_t^{\text{obs}} - \hat{y}_t^{\text{cf}}.
$$

We then construct a conformal score by calculating the mean absolute residual over the post-treatment period. To simulate the distribution of this score under the null (i.e., assuming no treatment effect), we perform circular block permutations of the residual vector and recompute the same statistic for each shifted version.

This yields an empirical distribution of conformal scores under the null. We take the $(1 - \alpha)$ quantile of this distribution as our conformal threshold, denoted $q_{1 - \alpha}$. To center the interval, we compute the mean residual over the pre-treatment period:

$$
\bar{\varepsilon} = \frac{1}{T_0} \sum_{t \leq T_0} \varepsilon_t.
$$

The conformal prediction interval for each post-treatment time $t > T_0$ is then given by:

$$
\left[ \hat{y}_t^{\text{cf}} + \bar{\varepsilon} - q_{1 - \alpha},\ \hat{y}_t^{\text{cf}} + \bar{\varepsilon} + q_{1 - \alpha} \right].
$$

This approach ensures that the prediction intervals account for uncertainty in the counterfactual trajectory while adjusting for systematic bias in the pre-treatment fit. The shaded regions in our figures visualize these conformal intervals.



# Kansas Tax Cuts

Here are the results of the FASC method.


```{python}
#| fig-align: center
#| echo: false

import pandas as pd

from mlsynth import FSCM

url = "http://fmwww.bc.edu/repec/bocode/s/scul_Taxes.dta"

# Feel free to change "smoking" with "basque" above in the URL

data = pd.read_stata(url)

state_col = data.columns[0]
date_col = data.columns[-1]

q2_2012_start = pd.Timestamp('2012-04-01')

# Create the 'Kansas Tax Cuts' indicator column
data['Kansas Tax Cuts'] = ((data[state_col] == 'Kansas') & (data[date_col] >= q2_2012_start)).astype(int)

config = {
    "df": data,
    "outcome": data.columns[4],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[-2],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["red", "blue"], "use_augmented": True}

arco = FSCM(config).fit()

```

I compare the pretreatment and posttreatment fit of Forward SCM (FSCM) and its Augmented variant (ASCM). Both estimators were applied to the same treated unit over a common pre- and post-treatment period.

| Estimator           | Pre-RMSE | Post-RMSE | R²     | Notes                                |
|---------------------|----------|-----------|--------|--------------------------------------|
| Forward SCM         | 0.012    | 0.014     | 0.999  | Baseline sparse estimator            |
| Augmented SCM       | 0.012    | 0.014     | 0.999  | Ridge-augmented refinement           |
| Ben Michael (SCM)   | 0.90     | –         | –      | Some imbalance           |
| Ben Michael (ASCM)  | 0.65     | –         | –      | RMSE reduced by ~28% from SCM        |


Both FSCM and its ridge-augmented variant achieve nearly identical performance, with a pre-RMSE of 0.012 and an out-of-sample RMSE of 0.014. Since the donor selection is already good, the addition of regularization does not improve predictive accuracy, indicating that the original FSCM fit is already near-optimal. In contrast, the original SCM fit exhibits a much larger pre-RMSE of 0.90. While ASCM reduces this error to 0.65, the improvement is modest by comparison to FASC. Relative to these benchmarks, FASC approach achieves a *98.7% reduction in pre-RMSE* compared to standard SCM, and a *98.2% reduction* compared to augmented SCM with ridge. Of course, these were just the default settings; for higher dimensional donor pools, we could use stopping rules to make the selection process quicker. Also notice that inference has not yet been programmed just yet.

# Proposition 99

I compare the effect of Proposition 99 on cigarette sales in California using FASC, FSCM, and the original SCM. While all three detect a significant reduction in sales, the quality of pretreatment fit varies meaningfully. 

```{python}


import pandas as pd # To work with panel data

from mlsynth import FSCM

url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/smoking_data.csv"

data = pd.read_csv(url)

config = {
    "df": data,
    "outcome": data.columns[2],
    "treat": data.columns[-1],
    "unitid": data.columns[0],
    "time": data.columns[1],
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue", "#00F0FF"], "use_augmented": True}

arco = FSCM(config).fit()
```


| Estimator      | ATT    | Pre-RMSE  | $R^2$        |
| ----------- | ------ | --------- | --------- |
| FASC        | -16.76 | **0.935** | **0.993** |
| FSCM        | -19.51 | 1.656     | 0.979     |
| ADH (Stata) | -19.00 | 1.76      | —         |

FASC achieves the best pretreatment fit, with a root mean squared error of 0.935 and an $R^2$ of 0.993. This represents a **46% reduction in pre-RMSE compared to FSCM** and a **47% reduction relative to the original ADH implementation**. The improved fit comes with a slightly smaller estimated ATT (-16.76) compared to FSCM (-19.51) and ADH (-19.00), suggesting that regularized, forward-selected models may, if slightly, temper exaggerated effects while maintaining excellent in-sample risk.

FASC produces weights that differ notably from both FSCM and the original SCM with covariates. While the vanilla FSCM weights are sparse and non-negative, concentrated on a small set of donors such as Utah, Montana, Nevada, and Connecticut, the augmented FSCM weights are less sparse and include small negative values, as we would expect. These negative weights allow the augmented estimator to extrapolate slightly beyond the convex hull formed by the donors, thus providing controlled flexibility to improve the pre-treatment fit. Importantly, the augmented weights *still maintain the core donor pool* emphasized by the vanilla FSCM, with similar magnitudes on key states — for example, Utah (~0.38), Montana (~0.24), Nevada (~0.21), and Connecticut (~0.14). The original SCM with covariates also concentrates weight on a similar subset of donors, including Utah, Nevada, Montana, Colorado, and Connecticut, demonstrating stability in donor selection across methods.

The presence of small negative weights in the augmented FSCM, assigned to peripheral donors such as Alabama, Mississippi, and Tennessee, acts as fine-tuning adjustments to better capture the treated unit’s trajectory. Machine learning scientsts [have remarked](https://jmlr.csail.mit.edu/papers/volume19/17-777/17-777.pdf) that there's not obvious reason why the donor weights need to bo convex; Ben-Michael, Feller, and Rothstein argue that such extrapolation, provided that it is controlled, is worth it in settings where the base estimator produces poor in-sample risk. Overall, these patterns underscore how augmentation refines the synthetic control weights, offering a nuanced improvement over vanilla forward selection by reducing bias without sacrificing the stability or interpretability of the donor composition.


