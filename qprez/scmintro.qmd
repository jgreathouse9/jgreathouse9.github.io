---
title: "The Synthetic Control Method"
date: today
author:
  - name: "Jared Greathouse"
    url: "https://jgreathouse9.github.io"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: fade
    scrollable: true
    background-color: "#EAF7F1"
highlight-style: github
---

## Synthetic Controls

This is the lecture on synthetic control methodologies (SCM). SCM is a quasi-experimental method we can use to estimate the causal impact of policies, marketing campaigns, or other treatments we care about.

## What Is An Average?

Before we cover SCM, we need to be very clear about what an average is.

::: {.panel-tabset}

### Scalars

- With a finite set of scalars  $S = \{x_1, x_2, \dots, x_n\} \subset \mathbb{R}$, the arithmetic mean is:  

$$
\bar{x} = \frac{1}{n} \sum_{x \in S} x
= \tfrac{x_1 + x_2 + \cdots + x_n}{n}.
$$

- For example, say you have 12 apples and your friend has 18. This can be called set $A = \{12, 18\} \subset \mathbb{R}$:

$$
15 = \bar{x} = \frac{1}{2} \sum_{x \in \{12,18\}} x
= (\tfrac{1}{2} \cdot 12) + (\tfrac{1}{2} \cdot 18)
= \frac{12 + 18}{2}.
$$

On average, you have 15 apples.

---

### Vectors

- We can write the same example as a row vector:

$$
\mathbf{x} = 
\begin{bmatrix} 12 & 18 \end{bmatrix} 
\in \mathbb{R}^{1 \times 2},
$$

where we have one row, and each column entry corresponds to a person’s apples.  

- The weights are 1 divided by the number of people. Since it's just you and your friend, you both get the same weight:

$$
\mathbf{w} = 
\begin{bmatrix} \frac{1}{2} \\ \frac{1}{2} \end{bmatrix} 
\in \mathbb{R}^{2 \times 1}.
$$

- Then, we multiply each value in the column by its corresponding weight (1/2). Summing these products gives a single number, the average number of apples:

$$
\bar{x} = \mathbf{x}\mathbf{w} 
= 
\begin{bmatrix} 12 & 18 \end{bmatrix}
\begin{bmatrix} \tfrac{1}{2} \\[6pt] \tfrac{1}{2} \end{bmatrix}
= 
\begin{bmatrix} 12 \cdot \tfrac{1}{2} & \; 18 \cdot \tfrac{1}{2} \end{bmatrix}
= 6 + 9 = 15.
$$

See? We still have 15 apples, on average.

---

### Matrices

- Let’s extend this idea to multiple time periods. Suppose today you have 12 apples and tomorrow 30, and your friend has 18 today and 22 tomorrow. We can organize this into a matrix, where each row is a time period and each column is a person:

$$
\mathbf{Y} =
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\in \mathbb{R}^{2 \times 2}.
$$

- Now we want the average number of apples per time period. Using the same column weights as before (1/2 for each person), we multiply each row by the weights and sum:

$$
\bar{\mathbf{x}} = \mathbf{Y} \mathbf{w} 
=
\begin{bmatrix}
12 & 18 \\ 
30 & 22
\end{bmatrix} 
\begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix} 
=
\begin{bmatrix} 12\cdot 1/2 + 18\cdot 1/2 \\ 30\cdot 1/2 + 22\cdot 1/2 \end{bmatrix} 
=
\begin{bmatrix} 15 \\ 26 \end{bmatrix}.
$$

- So on average, you and your friend have 15 apples today and 26 apples tomorrow.  
- This idea generalizes to any data measured over time, like GDPs, incomes, or other outcomes.


:::

## What Are We Weighting For? Introducing Weighted Averages

::: {.panel-tabset}

### Simple Averages

You are at a bar with your friend. You earn \$60k/year. Your friend earns \$70k/year. The simple average of your incomes is:

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \begin{bmatrix} 60 & 70 \end{bmatrix} 
\begin{bmatrix} \frac{1}{2} \\[1mm] \frac{1}{2} \end{bmatrix} 
= \frac{1}{2}\cdot 60 + \frac{1}{2}\cdot 70 
= 65 \text{ (k/year)}
$$

Now, suddenly Bill Gates walks in, with a net worth of \$107B/year. Taking the simple average of all three income levels:  

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \frac{1}{3} \\[1mm] \frac{1}{3} \\[1mm] \frac{1}{3} \end{bmatrix}
$$

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= \frac{1}{3}\cdot 60 + \frac{1}{3}\cdot 70 + \frac{1}{3}\cdot 1.07\times 10^{11}
\approx 35.7 \text{ billion/year}
$$

Clearly, treating Bill Gates the same as the others skews the average.

- If the bartender next day bragged about earning 50k that night, we'd say: "It's not that you earned 50k organically; a wealthy dude happened to be there that night. We can't use this as evidence that your business is booming."  

- In other words, if we want a meaningful "bar average," Bill should likely get much less weight than the other patrons.

---

### Weighted Average

Instead of treating Bill as equal to you two, we can give each person a weight \(w_i\) to decide how much they count in the average. This is exactly the fraction we used before:  

$$
\bar{x}_{\text{weighted}} = \sum_{i=1}^{n} w_i x_i, \quad \sum_{i=1}^{n} w_i = 1.
$$

Here, we compute a weighted average where Bill can get a tiny weight, and you and your friend get larger weights, giving a more reasonable "average bar income."

---

### Weighted Average Cont.

Suppose we only care about you and your friend, and ignore Bill Gates (weight = 0):

$$
\mathbf{x} = \begin{bmatrix} 60 & 70 & 1.07\times 10^{11} \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} \mathbf{0.5} \\[1mm] \mathbf{0.5} \\[1mm] \mathbf{0} \end{bmatrix}
$$

Then the weighted average is:

$$
\bar{x} = \mathbf{x} \mathbf{w} 
= 0.5\cdot 60 + 0.5\cdot 70 + 0 \cdot 1.07\times 10^{11}
= 65 \text{ (k/year)}
$$

See how the average is as it was at first when it was just you and your friend? Bill is clearly not representative of a bar or even a country. Giving him as much weight as regular humans does not make sense.

:::

## How to Choose the Weights?

- The issue with the previous examples is that the weights we used were arbitrary. There’s no reason you and your friend must each get 0.5; maybe one of you should get 0.1 and the other 0.9. In real life, weights are usually unknown. To use weighted averages for estimating policy impacts, we need a formal method.

- Suppose we have one unit exposed to terrorism in 1975 (the Basque Country), and a set of 16 other units that are not exposed. We want to estimate the effect of terrorism on GDP per capita.

- Using data on GDPs over the pre- and post-intervention periods, we aim to estimate the counterfactual for the Basque Country as a weighted average of 16 control units. Below we plot the Basque Country's GDP, and the control/donor regions.

```{python}
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Create figure with 2 subplots ---
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# --- Left subplot: observed Basque only ---
axes[0].plot(time, y, color='black', linewidth=2, label='Basque')
axes[0].set_title("Observed Basque GDP")
axes[0].set_xlabel("Year")
axes[0].set_ylabel("GDP")
axes[0].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism')
axes[0].legend()
axes[0].text(time[T0]-3, max(y)*0.95, "Intervention", color='red')

# --- Right subplot: observed Basque + donor units ---
# Plot all donors as thin grey lines
for j in range(Y0.shape[1]):
    axes[1].plot(time, Y0[:, j], color='grey', linewidth=0.8, alpha=0.7)

# Plot Basque on top
axes[1].plot(time, y, color='black', linewidth=2, label='Basque')
axes[1].set_title("Basque vs Donor Units")
axes[1].set_xlabel("Year")
axes[1].axvline(x=time[T0], color='red', linestyle='--', label='Terrorism')
axes[1].text(time[T0]-3, max(y)*0.95, "Intervention", color='red')
axes[1].legend()

plt.tight_layout()
plt.show()
```

## Using Least Squares

But how do we know how much weight a unit should get? One approach is linear regression. In multiple regression, the coefficients returned to us can be interpreted as weights for our control group.

::: {.panel-tabset}

### Notation

- $\mathbf{y}_1 \in \mathbb{R}^{T}$: column vector of outcomes (GDP per capita) for the treated unit (Basque Country) over $T$ time periods  
- $\mathbf{Y}_0 \in \mathbb{R}^{T \times J}$: donor matrix; each column is a control unit, each row is a time period ($J=16$ control units)  
- $\mathbf{w} \in \mathbb{R}^{J}$: vector of weights indicating how much each control unit contributes to the weighted average version of the Basque Country  

*Reminder: a weighted average can be written as*  

$$
\hat{y}_1 = \mathbf{Y}_0 \mathbf{w} = w_1 Y_{01} + w_2 Y_{02} + \dots + w_J Y_{0J}
$$

---

### OLS Problem

- We can choose weights to minimize the pre-intervention difference between Basque and the weighted donors:  

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^J}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2
$$

- In Stata, this is just: `reg y Donor1-DonorJ if year < 1975`, where `y` is Basque GDP and the donors are the untreated units.  

- The OLS coefficients are the weights that minimize the difference between the weighted average and the treated unit in the pre-intervention period.

---

### Interpretation

- Once we have $\mathbf{w}^{\ast}$, we compute the weighted average of controls:

$$
\hat{y}_1 = \mathbf{Y}_0 \mathbf{w}^{\ast} = w_1 Y_{01} + w_2 Y_{02} + \dots + w_J Y_{0J}
$$

- That is: multiply all the values for each control unit by its regression coefficient and sum across donors.  

- This gives the predicted Basque trajectory pre- and post-intervention.

---

### Treatment Effect

- The average treatment effect on the treated (ATT) can be expressed as the mean difference between observed outcomes and the weighted average in the post-intervention period:

$$
\text{ATT} = \frac{1}{T - T_0} \sum_{t=T_0+1}^{T} \left( y_{1t} - \hat{y}_{1t} \right)
$$

- $T_0$: last pre-intervention period  
- $y_{1t}$: observed outcome of the treated unit at time $t$  
- $\hat{y}_{1t}$: synthetic prediction at time $t$  

*Intuition:* ATT measures how much Basque GDP deviates from the OLS prediction after terrorism began.

:::

---

## Least Squares Cont.

::: {.panel-tabset}

### Our Weights

Here are the weights OLS returns to us for the 16 donor units:

```{python}
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# --- Define CVXPY variable for donor weights ---
J = Y0.shape[1]
w = cp.Variable(J)

# --- Define unconstrained OLS problem on pre-intervention ---
objective = cp.Minimize(cp.sum_squares(y[:T0] - Y0[:T0, :] @ w))
problem = cp.Problem(objective)
problem.solve()

# --- Extract weights ---
weights = w.value

# --- Predict outcomes using weights ---
y_pred = Y0 @ weights

# --- Compute ATT (mean post-treatment difference) ---
att = np.mean(y[T0:] - y_pred[T0:])

# --- Get donor names ---
donor_names = prepped_data['donor_names']

# --- Build DataFrame ---
weights_df = pd.DataFrame(weights, index=donor_names, columns=["Weight"])

md_table = weights_df.to_markdown(index=True)

display(Markdown(md_table))
```

Notice that OLS assigns some positive and some negative weights. Negative weights can be hard to interpret substantively, and it’s unclear which donors really matter most.

### Plotting

Here is our prediction using OLS. Unfortunately, while OLS fits very well to the Basque Country before terrorism happened, it has a weird zig-zag-y prediction after the treatment happens. We can see that the prediction line for the Basque country suggests that its economy would have fallen off a cliff, had terrorism not happened, in other words, terrorism was great for business to the tune of 1500 dollars per person... This does not seem like a sensible finding substantively, or a very reasonable prediciton.

```{python}
# --- Plot results ---
plt.figure(figsize=(10,6))
plt.plot(time, y, label="Basque (Observed)", color='black')
plt.plot(time, y_pred, label="Synthetic (OLS)", color='blue', linestyle='--')
plt.axvline(x=time[T0], color='red', linestyle='-', label='Terrorism')
plt.xlabel("Year")
plt.ylabel("GDP")
plt.title(f"Effect of Terrorism in the Basque Country (ATT = {att:.3f})")
plt.legend()
plt.show()
```

:::



## How To Change The Weights

What if we could change the weights somehow? What if we could force he predictions of the model to be within the range of the donor pool units? The plot below plots the area between Baleares, with the highest GDP, and Extremadura, the poorest area in Spain. 

```{python}

import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']


# Compute min and max across donors at each time point
donor_min = Y0.min(axis=1)
donor_max = Y0.max(axis=1)

plt.figure(figsize=(10,6))

# Shade the convex hull (area between min and max)
plt.fill_between(time, donor_min, donor_max, color='lightgrey', alpha=0.4, label="Donor Span")

# Plot Basque GDP
plt.plot(time, y, color='black', linewidth=2, label='Basque (Observed)')
# Intervention line
plt.axvline(x=time[T0], color='red', linestyle='-', linewidth=2, label='Terrorism')

plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque GDP vs Donor Span")
plt.legend()
plt.show()


```

## Extreme Weighted Averages

- Let’s assign all the weight to just one donor over the entire time series:

  1. Max donor: weight = 1 for the donor with the highest GDP over the full period, 0 for others  
  2. Min donor: weight = 1 for the donor with the lowest  GDP over the full period, 0 for others  

- This illustrates the full span of possible predictions within the span of the donor pool.

```{python}
# Identify donors with max and min GDP over the full period
max_idx = Y0.mean(axis=0).argmax()
min_idx = Y0.mean(axis=0).argmin()

# Create extreme weight vectors
w_max = np.zeros(Y0.shape[1])
w_max[max_idx] = 1

w_min = np.zeros(Y0.shape[1])
w_min[min_idx] = 1

# Compute synthetic trajectories
y_max = Y0 @ w_max
y_min = Y0 @ w_min

# Plot
plt.figure(figsize=(10,6))
plt.fill_between(time, y_min, y_max, color='lightgrey', alpha=0.4, label="Convex Hull Extremes")
plt.plot(time, y, color='black', linewidth=2, label='Basque')
plt.plot(time, y_max, color='red', linestyle='--', linewidth=2, label='Baleares')
plt.plot(time, y_min, color='blue', linestyle='--', linewidth=2, label='Extremadura')
plt.axvline(x=time[T0], color='red', linestyle='-', linewidth=2, label='Terrorism')
plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque GDP vs Extreme Weighted Donors")
plt.legend()
plt.show()
```

---


## Example Plot


```{python}

##| echo: false
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import pandas as pd
from mlsynth.utils.datautils import dataprep

# --- Load and preprocess data ---
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0 = Ywide_data[["Cataluna", "Madrid (Comunidad De)"]].values

# --- Weights ---
w_final = np.array([0.85, 0.15])
w_start = np.array([0.1, 0.9])
steps = 50
weights_history = np.array([w_start + (w_final - w_start) * t/(steps-1) for t in range(steps)])

# --- Figure setup ---
fig, ax = plt.subplots(figsize=(8, 4))
line_y, = ax.plot(y, label="Treated (Basque)", color="red", linewidth=2)
line_hatY, = ax.plot(w_start[0]*Y0[:,0] + w_start[1]*Y0[:,1], 
                     label="Weighted combo", color="purple", linestyle="--")
line_cataluna, = ax.plot(Y0[:,0], label="Cataluna", color="green", alpha=0.5)
line_madrid, = ax.plot(Y0[:,1], label="Madrid", color="blue", alpha=0.5)

ax.legend()
ax.set_ylim(y.min() - 1, y.max() + 1)

# --- Animation function ---
def update(frame):
    w_curr = weights_history[frame]
    line_hatY.set_ydata(w_curr[0]*Y0[:,0] + w_curr[1]*Y0[:,1])
    
    if frame == steps - 1:
        line_cataluna.set_visible(False)
        line_madrid.set_visible(False)
        ax.set_title(f"Final SCM: w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}")
    else:
        ax.set_title(f"Frame {frame+1}: w_Cataluna={w_curr[0]:.2f}, w_Madrid={w_curr[1]:.2f}")
    
    return line_hatY, line_cataluna, line_madrid

# --- Animate with repeated final frame ---
extra_frames = 30  # ~3 seconds at 10 fps
all_frames = list(range(steps)) + [steps-1]*extra_frames

ani = FuncAnimation(fig, update, frames=all_frames, blit=False)
ani.save("scm_weighted_combo.gif", writer=PillowWriter(fps=10))

plt.close(fig)


```

![SCM Weighted Combo](scm_weighted_combo.gif)

