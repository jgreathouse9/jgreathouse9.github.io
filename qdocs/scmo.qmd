---
title: "Synthetic Control Methods With Multiple Outcomes"
date: 2025-04-15
categories: [Causal Inference, Econometrics]
---

Sometimes, analysts have multiple different metrics at their disposal with which to estimate synthetic control models. That is, multiple relevant variables may predict a single target/outcome variable that we care about, and [plenty](https://doi.org/10.1016/j.ecosta.2017.08.002) of other papers have commented on this fact before.

However as it turns out, most people who use synthetic controls use only a single outcome in their analyses; perhaps they will adjust their unit weighted by some diagonal matrix, $V$ in most applications, to assist in choosing the unit weights, but this is limited by the number of pretreatment periods you have (if you have more covariates than you have pretreatment periods, you cannot estimate the regression). Recent papers by econometricians have tried to get around this, though. In particular, this blog post covers a few recent [recent](https://doi.org/10.48550/arXiv.2311.16260) [papers](https://doi.org/10.48550/arXiv.2304.02272) which have advocated for this in applied settings. Of course, I use the ```SCMO``` class of ```mlsynth``` to show how this may be used in simulated and applied settings.

# Notation

I adopt the following notational conventions throughout. Scalars are denoted by lowercase italicized letters such as $g$, vectors are denoted by bold lowercase letters such as $\mathbf{v} \in \mathbb{R}^n$, and matrices are denoted by bold uppercase letters such as $\mathbf{A} \in \mathbb{R}^{m \times n}$. Sets are denoted by calligraphic letters, such as $\mathcal{N}$ or $\mathcal{T}$, and the cardinality of a finite set $\mathcal{A}$ is written $|\mathcal{A}|$. For a matrix $\mathbf{A}$, we write $\| \mathbf{A} \|_F^2 = \sum_{i,j} A_{ij}^2$ for the squared Frobenius norm and $\| \mathbf{A} \|_2$ for its spectral norm. For any $n \in \mathbb{N}$, we define the $n$-dimensional probability simplex by

$$
\Delta^{n} = \left\{ \mathbf{w} \in \mathbb{R}^{n+1} \mid w_i \geq 0 \text{ for all } i, \sum_{i=1}^{n+1} w_i = 1 \right\}.
$$

Matrix multiplication is written in the usual way; when dimensions are compatible, products like $\mathbf{A}^\top \mathbf{b}$, $\mathbf{A} \mathbf{B}$, or $\mathbf{b}^\top \mathbf{c}$ denote standard matrix-vector or matrix-matrix products. Elementwise operations (e.g., $\mathbf{a} \odot \mathbf{b}$) are avoided unless explicitly defined.

Let $\mathcal{N} = \{1, 2, \dots, N\}$ index the units in the panel, with unit $1$ denoting the treated unit and $\mathcal{N}_0 = \{2, 3, \dots, N\}$ denoting the set of control units. We write $N_0 = |\mathcal{N}_0| = N - 1$ for its cardinality. Let $\mathcal{T} \subset \mathbb{N}$ be a finite set of time periods with $|\mathcal{T}| = T$. Let $T_0 < T$ denote the final pre-treatment period, and define $\mathcal{T}_1 = \{1, 2, \dots, T_0\}$ and $\mathcal{T}_2 = \{T_0 + 1, \dots, T\}$ as the pre-treatment and post-treatment periods, respectively.

Let $\mathbf{y}_j \in \mathbb{R}^T$ denote the time series for outcome zero (our main outcome of interest) for unit $j \in \mathcal{N}$. Let $\mathbf{Y}_0 \in \mathbb{R}^{T \times N_0}$ denote the matrix that stacks these vectors for all control units $j \in \mathcal{N}_0$. We write $\mathbf{y}_1 = \begin{bmatrix} y_{1,1} & \cdots & y_{1,T} \end{bmatrix}^\top$ for the treated unit’s outcome vector. We partition each $\mathbf{y}_j$ into a pre-treatment vector $\mathbf{y}_j^{\text{pre}} \in \mathbb{R}^{|\mathcal{T}_1|}$ and post-treatment vector $\mathbf{y}_j^{\text{post}} \in \mathbb{R}^{|\mathcal{T}_2|}$, and likewise for $\mathbf{Y}_0$, defining $\mathbf{Y}_0^{\text{pre}} \in \mathbb{R}^{|\mathcal{T}_1| \times N_0}$ and $\mathbf{Y}_0^{\text{post}} \in \mathbb{R}^{|\mathcal{T}_2| \times N_0}$.

Let $K$ denote the number of auxiliary outcomes (excluding the main one). For each outcome $\ell \in \{1, 2, \dots, K\}$, let $\mathbf{y}_j^{(\ell)} \in \mathbb{R}^T$ denote the $\ell$-th outcome vector for unit $j$, and let $\mathbf{Y}_0^{(\ell)} \in \mathbb{R}^{T \times N_0}$ denote the matrix collecting the $\ell$-th outcome across all controls. These are also split into pre- and post-treatment periods in the usual way.

We define the "stacking" operation—denoted by a prime—as the vertical concatenation of multiple outcomes. Specifically, let $\mathbf{Y}_0^{\prime} \in \mathbb{R}^{K |\mathcal{T}_1| \times N_0}$ denote the vertically stacked matrix (and vector):

$$
\mathbf{Y}_0^{\prime} = \begin{bmatrix}
\mathbf{Y}_0^{(1), \text{pre}} \\
\mathbf{Y}_0^{(2), \text{pre}} \\
\vdots \\
\mathbf{Y}_0^{(K), \text{pre}}
\end{bmatrix},
\quad
\mathbf{y}_1^{\prime} = \begin{bmatrix}
\mathbf{y}_1^{(1), \text{pre}} \\
\mathbf{y}_1^{(2), \text{pre}} \\
\vdots \\
\mathbf{y}_1^{(K), \text{pre}}
\end{bmatrix}.
$$

This is simply stacking the pre-intervention period metrics atop one another. If we have two metrics and 19 pretreatment periods, we have a vector of 38 metrics across those two outcomes. This is done both for the matrices too: if we have 10 donors, 19 pretreatment periods, and two metrics (the main outcome and another one) we now have 10 columns and 38 rows for all the metrics across those same 19 pre-treatment periods.

## Standard Synthetic Control

Before introducing the stacked estimator however, I begin by reviewing the standard synthetic control method. Given pre-treatment data for a treated unit and a set of control units, the canonical synthetic control estimator selects weights $\mathbf{w} \in \Delta^{N_0}$ to minimize the pre-treatment discrepancy between the treated unit and a convex combination of the controls:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}}{\operatorname*{argmin}} \|\mathbf{y}_1^{\text{pre}} - \mathbf{Y}_0^{\text{pre}} \mathbf{w} \|_F^2.
$$

This is a constrained least squares problem in which we regress the treated unit’s pre-treatment outcomes onto the control matrix under the constraint that $\mathbf{w}$ lies in the simplex $\Delta^{N_0}$. That is, the synthetic control estimator builds a linear approximation to $\mathbf{y}_1^{\text{pre}}$ using a weighted average of the control outcomes, where the weights are non-negative and sum to one. This corresponds to restricting attention to points in the convex hull of the control units. For the purposes of this blog post, think of this as the solution to a single linear equation. Once the optimal weights $\mathbf{w}^\ast$ are estimated, the counterfactual trajectory for the treated unit in the post-treatment period is estimated by applying the same weights to the control matrix:

$$
\mathbf{y}^{\text{SC}}_1^{\text{SC}} = \mathbf{Y}_0^{\text{post}} \mathbf{w}^\ast.
$$

The estimated treatment effect at each post-treatment time point is then given by the difference between observed and counterfactual outcomes: $\hat{\tau}_{1,t} = y_{1,t} - \hat{y}_{1t}$ for $t \in \mathcal{T}_2$. Of course, the method is predicated on good preatment fit, where the predictions in the pre-treatment period should ideally have a low root mean squared error with respect to the treatment unit's observed values.

## Stacked SCM with Multiple Outcomes

When multiple outcomes are present, there are three choices we can make: do we use the concatenated approachh a demeaning approach, or do we combine them as a model average? The first approach stacks all the outcomes as we've discussed above into a big matrix/big vector. We then apply the standard SCM optimization problem to the stacked data. The objective function for this optimization is:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}}{\operatorname*{argmin}} \|\mathbf{y}_1^{\prime} - \mathbf{Y}_0^{\prime} \mathbf{w} \|_F^2 \quad \forall t \in \mathcal{T}_1.
$$

Here, the vector $\mathbf{y}_1^{\prime}$ is the stacked pre-treatment outcomes of the treated unit, including at least the main target unit and at least one other outcome. Similarly, and $\mathbf{Y}_0^{\prime}$ is the stacked matrix of control outcomes. The weights $\mathbf{w}^\ast$ are found such that the counterfactual outcomes for the treated unit (in the pre-treatment period) are as close as possible to the weighted combination of the control outcomes- all of them. The counterfactual prediction for the treated unit during the post-treatment period is then computed as:

$$
\mathbf{y}_1^{\text{cat}} = \mathbf{Y}_0 \mathbf{w}^\ast.
$$

The second approach involves including an intercept term for each outcome to control for differences in levels across outcomes. Instead of demeaning the outcomes ourselves, we add an intercept to the optimization, which effectively adjusts for any systematic shifts in the data (the estimator from Sun, Ben-Michael, and Feller's paper I cited above). This approach captures baseline trends between the outcomes, should they exist in the outcomes of interest. The objective function in this case is:

$$
\mathbf{w}^\ast = \underset{\mathbf{w} \in \Delta^{N_0}, \beta \in \mathbb{R}}{\operatorname*{argmin}} \|\mathbf{y}_1^{\prime} - \mathbf{Y}_0^{\prime} \mathbf{w} - \beta \|_F^2 \quad \forall t \in \mathcal{T}_1.
$$

Here, $\beta$ represents the intercept term, which accounts for baseline shifts between the treated and control units. The weights $\mathbf{w}^\ast$ are determined by solving this optimization, which adjusts the outcomes for the intercept term while finding the best fit for the counterfactual prediction. The counterfactual for the treated unit is then computed as:

$$
\mathbf{y}_1^{\text{avg}} = \mathbf{Y}_0 \mathbf{w}^\ast + \beta.
$$

When I first read about this apporach, I was confused as to what was going on, until I considered them as the solution to a system of multiple equations. To have a better sense of what's going on, we typically consider our outcomes in data science or econoemtrics to be generated by some process that we never see. A common assumption is that they are generated by a [low-rank model](https://jmlr.org/papers/volume19/17-777/17-777.pdf), which essentially formulates our outcomes as a byproduct of the interaction between unit specific fixed effects that are idiosyncratic to each unit (that are not expected to change over time) and time effects that are common across all units of interest. Well, more than one outcome exists in the world that may plausibly have a similar factor structure to the metric that we do care about. When we include these in our SCM, we now allow the model to better capture the latent factors that influence the main outcome, thereby improving the match of the synthetic control. The optimization framework effectively allows us to match on both the main outcome and the auxiliary factors, capturing the factor loadings that generate the observed data. Thus, stacking the outcomes together or including an intercept term aligns the model with the idea that multiple systems of equations, representing different outcomes or contributing factors, can provide a better counterfactual prediction than simply using one outcome.

## Model Averaging

Sun, Ben-Michael, and Feller also advocate model averaging in this setting. Suppose we are given two distinct estimators of the counterfactual outcome for the treated unit. On one hand, we have $\mathbf{y}_1^{\text{CAT}} \in \mathbb{R}^{T_0}$, which denotes the pre-treatment fit from the concatenated model, and on the other hand, we have $\mathbf{y}_1^{\text{AVG}} \in \mathbb{R}^{T_0}$, the corresponding fit from the demeaned model. As before, we also observe the treated unit’s actual pre-treatment trajectory, $\mathbf{y}_1^{\text{pre}} \in \mathbb{R}^{T_0}$. 

We now stack the two counterfactuals into a single matrix:

$$
\mathbf{Y}^{\text{MA}} = \begin{bmatrix}
\mathbf{y}_1^{\text{CAT}} & \mathbf{y}_1^{\text{AVG}}
\end{bmatrix} \in \mathbb{R}^{T_0 \times 2}.
$$

We define the model-averaged pre-treatment fit as a convex combination of the two predictions:

$$
\mathbf{y}_1^{\text{MA}}(\boldsymbol{\lambda}) = \mathbf{Y}^{\text{MA}} \boldsymbol{\lambda},
$$

where $\boldsymbol{\lambda} \in \Delta^2$ is a **2-dimensional simplex weight vector**:

$$
\Delta^2 = \left\{ \boldsymbol{\lambda} \in \mathbb{R}_{\geq 0}^2 : \| \boldsymbol{\lambda} \|_1 = 1 \right\}.
$$

The optimal weights $\boldsymbol{\lambda}^\ast$ are chosen to minimize the mean squared error between the treated unit’s actual outcomes and the model-averaged prediction:

$$
\boldsymbol{\lambda}^\ast = \underset{\boldsymbol{\lambda} \in \Delta^2}{\operatorname{argmin}} \left\| \mathbf{y}_1^{\text{pre}} - \mathbf{Y}^{\text{MA}} \boldsymbol{\lambda} \right\|_F^2.
$$

The interpretation of the convex hull remains the same as in traditional SCM: for each time point in the pre-treatment period, the model-averaged prediction lies between the minimum and maximum of the two individual estimators. That is,

$$
\mathbf{y}_1^{\text{MA}}(t) \in \left[
\min\left(\mathbf{y}_1^{\text{CAT}}(t), \mathbf{y}_1^{\text{AVG}}(t)\right),
\max\left(\mathbf{y}_1^{\text{CAT}}(t), \mathbf{y}_1^{\text{AVG}}(t)\right)
\right]
\quad \text{for all } t \in \{1, \dots, T_0\}.
$$

Once $\boldsymbol{\lambda}^\ast$ is found, the post-treatment counterfactual is computed analogously:

$$
\mathbf{Y}^{\text{MA, post}} = \begin{bmatrix}
\mathbf{y}_1^{\text{CAT, post}} & \mathbf{y}_1^{\text{AVG, post}}
\end{bmatrix},
\quad
\mathbf{y}_1^{\text{MA, post}} = \mathbf{Y}^{\text{MA, post}} \boldsymbol{\lambda}^\ast.
$$

Essentially, this is a mixture of both models.

## Conformal Prediction via Agnostic Means

Now a final word on infernece. I use conformal prediction intervals to conduct inference here, develoepd in [this paper](https://nppackages.github.io/references/Cattaneo-Feng-Palomba-Titiunik_2025_JSS.pdf). Precisely, I use the agnostic approach (yes, I know [other approaches](https://doi.org/10.1080/01621459.2021.1920957) exist; users of ```mlsynth``` will likely be given the option to choose which flavor of conformal prediction they desire as an option, in the future). Define the vector of residuals as  
$\mathbf{u}_{\text{pre}} = \mathbf{y}_{1,\text{pre}} - \mathbf{y}^{\text{SC}}_{1,\text{pre}}$, or just the pretreatment error betwixt the actual observed values and its counterfactual. Furthermore, let $\hat{\sigma}^2 = \frac{1}{T_0 - 1} \left\| \mathbf{u}_{\text{pre}} - \bar{u} \mathbf{1} \right\|^2$ be the unbiased estimator of the residual variance, where $\bar{u} = \frac{1}{T_0} \sum_{t=1}^{T_0} u_t$ is the mean residual.

We aim to construct prediction intervals for the counterfactual outcomes in the post-treatment period, accounting for uncertainty due to the unobserved post-treatment error term. Let $\mathbf{y}^{\text{SC}}_{1,\text{post}} \in \mathbb{R}^{T_1}$ be the post-treatment SC predictions. Assuming that the out-of-sample error is conditionally sub-Gaussian given the history $\mathscr{H}$  (in English, that large errors are unlikely, given a well-fitting pre-intervention model), we obtain a valid non-asymptotic prediction interval via [concentration](https://arxiv.org/pdf/1910.02884) inequalities: $\delta_\alpha = \sqrt{2 \hat{\sigma}^2 \log(2 / \alpha)}$. The conformal-style prediction intervals are then defined as $\mathbf{l}_{\text{post}} = \mathbf{y}^{\text{SC}}_{1,\text{post}} - \delta_\alpha \mathbf{1}$,   $\mathbf{u}_{\text{post}} = \mathbf{y}^{\text{SC}}_{1,\text{post}} + \delta_\alpha \mathbf{1}$. These bounds provide uniform coverage guarantees under the sub-Gaussian assumption on the prediction error. In the paper, the authors also provide two more methods, and these will liekly be incorporated in the future.
 

# Estimation in Python

## Simulation

To begin, I conduct a simple synthetic study. Suppose we are working at Airbnb, and we wish to see the causal effect of the introduction of [Airbnb Experiences](https://news.airbnb.com/airbnb-partners-with-art-basel-miami-beach/) on [Gross Booking Value](https://www.lodgify.com/encyclopedia/gross-booking-revenue/) (GBV), a metric which is defined as ''the total revenue generated by room or property rentals before any costs or expenses are subtracted''. Well, in theory, Airbnb Expereinces may make the use of the Airbnb more attractive to tourists, since now tourists no longer need use Viator or other booking services to find stuff to do locally. In other words, it makes it an attraction to that city specifically, and we may see an increase in GBV as a result. Well, all sorts of things may be related to GBV, such as local hotel prices, pre-existing level of tourist arrivals, average city-specific booking price, and other relevant metrics. The goal is to see how the GBV would have evolve absent the policy.

I simulate a panel dataset consisting of $N$ units each observed over $T = T_0 + T_1$ time periods, with $K$ distinct outcomes. For each unit, the observed outcome $\mathbf{Y}_{jtk}$ evolves according to an autoregressive process with latent structure for time, place, and seasonality: 

$$
\mathbf{Y}_{jtk} = 
\rho_k \mathbf{Y}_{jt-1k} +
(1 - \rho_k) \left( 
\alpha_{jk} + \beta_{tk} + \boldsymbol{\phi}_j^\top \boldsymbol{\mu}_{tk} + \mathbf{S}_{jt} + \delta_k
\right) + \varepsilon_{jtk}, \quad \text{for } t > 1,
$$

with initial condition

$$
\mathbf{Y}_{j1k} = 
\alpha_{jk} + \beta_{1k} + \boldsymbol{\phi}_j^\top \boldsymbol{\mu}_{1k} + \mathbf{S}_{j1} + \delta_k + \varepsilon_{j1k}.
$$

Here, $\alpha_{jk} \sim \mathcal{N}(0, 1)$ and $\beta_{tk} \sim \mathcal{N}(0, 1)$ represent unit-outcome and time-outcome fixed effects, respectively. Each unit $j$ possesses latent attributes $\boldsymbol{\phi}_j \in \mathbb{R}^r \sim \mathcal{N}(0, \mathbf{I})$, while each time-outcome pair $(tk)$ has associated latent loadings $\boldsymbol{\mu}_{tk} \in \mathbb{R}^r \sim \mathcal{N}(0, \mathbf{I})$. The seasonal component $\mathbf{S}_{jt}$ captures unit-specific periodicity and is defined as $\gamma_j \cos\left( \frac{4\pi(t - \tau_j)}{T_{\text{season}}} \right)$, with $\gamma_j \sim \text{Unif}(0, \bar{\gamma})$ representing the amplitude and $\tau_j \sim \text{Unif}\{0, \dots, T_{\text{season}} - 1\}$ the phase shift. Each outcome $k$ has a baseline shift $\delta_k \sim \text{Unif}(200, 500)$, an autocorrelation parameter $\rho_k \in (0, 1)$, and an idiosyncratic noise component $\varepsilon_{jtk} \sim \mathcal{N}(0, \sigma^2)$. One unit (Iqueque in this draw) is designated as treated—specifically, the unit with the second-largest realization on the first latent factor dimension. It receives an additive treatment effect of $+5$ to GBV during all post-treatment periods.


```{python}

#| fig-align: center
#| echo: false

import numpy as np
import pandas as pd

from mlsynth.mlsynth import SCMO

def simulate(
    N=99, T0=52*3, T1=52, K=4, r=2, sigma=.20,
    max_gamma=0, T_season=12, seed=2000
):
    np.random.seed(seed)
    T = T0 + T1

    # Latent factors
    phi = np.random.normal(0, 1, size=(N, r))           # Market-specific latent factors (loadings)
    mu = np.random.normal(0, 1, size=(T, K, r))          # Time-and-outcome-specific latent factors

    # Fixed effects
    alpha = np.random.normal(0, 1, size=(N, K))          # Market fixed effects
    beta = np.random.normal(0, 1, size=(T, K))           # Time fixed effects

    # Market-specific seasonal parameters
    gamma_i = np.random.uniform(0, max_gamma, size=N)    # amplitude of seasonal effect
    tau_i = np.random.randint(0, T_season, size=N)       # phase shift (peak week)

    # Construct seasonal matrix S (N x T): market-time-specific seasonality
    t_grid = np.arange(T)
    S = np.array([
        gamma_i[i] * np.cos(4 * np.pi * (t_grid - tau_i[i]) / T_season)
        for i in range(N)
    ])

    # Outcome tensor
    Y = np.zeros((N, T, K))

    # Base shift
    baseline_shift = [np.random.randint(200, 500) for _ in range(K)]  # Random base values for each outcome

    # Autocorrelation coefficients for each outcome
    rho = np.array([0.8, 0.6, 0.5, 0.3])  # AR(1) coefficients for each outcome, can we adjusted if we wish.

    for k in range(K):
        latent = phi @ mu[:, k, :].T  # N x T
        base = (
            alpha[:, [k]] +         # N x 1,
            beta[:, k] +            # T,
            latent +                # N x T,
            S +                     # N x T
            baseline_shift[k]       # scalar
        )
        noise = np.random.normal(0, sigma, size=(N, T))

        # First time point initialization
        Y[:, 0, k] = base[:, 0] + noise[:, 0]

        # Autoregressive Factors
        for t in range(1, T):
            Y[:, t, k] = (
                rho[k] * Y[:, t-1, k] +
                (1 - rho[k]) * base[:, t] +
                noise[:, t]
            )

    # Identify treated market: second-highest factor loading
    treated_unit = np.argsort(phi[:, 0])[-2]

    time = np.arange(T)
    post_treatment = (time >= T0)
    treat = np.zeros((N, T), dtype=int)
    treat[treated_unit, post_treatment] = 1

    # Inject treatment effect into Gross Booking Value for treated market
    Y[treated_unit, post_treatment, 0] += 5  # add treatment effect of +5, but can be whatever we like.

    # Construct the dataframe without loops
    markets = np.arange(N)[:, None]           # shape (N, 1)
    weeks = np.arange(T)[None, :]             # shape (1, T)

    market_grid = np.repeat(markets, T, axis=1).flatten()  # shape (N*T,)
    week_grid = np.tile(weeks, (N, 1)).flatten()           # shape (N*T,)


    cities = [
        "São Paulo", "Mexico City", "San Carlos de Bariloche", "Rio de Janeiro", "Ushuaia",
        "Bogotá", "Santiago", "Caracas", "Guayaquil", "Quito",
        "Brasília", "Bocas del Toro", "Asunción", "Cabo San Lucas", "Playa del Carmen",
        "Medellín", "Porto Alegre", "Placencia", "Recife", "Salvador",
        "Zihuatanejo", "San José", "Panama City", "Montevidio", "Tegucigalpa",
        "Foz do Iguaçu", "Maracaibo", "Rosario", "Maracay", "Antofagasta",
        "San Pedro Sula", "San Juan", "Chihuahua", "Cayo District", "Maturín",
        "Buzios", "Puebla", "Mar del Plata", "Arequipa", "Fernando de Noronha", "Guatemala City",
        "Mazatlán", "Mérida", "Córdoba", "Cozumel", "Trujillo",
        "Corozal Town", "Santa Cruz de la Sierra", "San Luis Potosí", "Jalapão", "Potosí",
        "Tucumán", "Neuquén", "La Plata", "Viña del Mar", "Florianópolis", "Lagos de Moreno",
        "La Paz", "Belém", "Venezuela", "Ribeirão Preto", "Valparaíso",
        "Marília", "Campinas", "Vitoria", "Sorocaba", "Santa Fe",
        "San Salvador", "Lima", "Buenos Aires", "Curitiba", "Maceió",
        "Cartagena", "La Ceiba", "Puerto La Cruz", "Olinda", "Monterrey",
        "Ibagué", "Cúcuta", "Playa Venao", "Cancún", "Puerto Escondido", "Chiclayo", "Ambato",
        "Pucallpa", "Santa Marta", "Villavicencio", "Paraná", "Cauca", "San Vicente",
        "Cali", "Tarija", "Manzanillo", "El Alto", "Santiago de Chile", "Cochabamba",
        "Punta del Este", "Iquique",  "Durango", "Puerto Viejo de Talamanca"
    ]

    city_mapping = {i: cities[i] for i in range(N)}

    data = {
        'Market': [city_mapping[market] for market in market_grid],
        'Week': week_grid,
        'Experiences': treat.flatten()
    }

    for k in range(K):
        if k == 0:
            data['Gross Booking Value'] = Y[:, :, k].flatten()
        elif k == 1:
            data['Average Booking Price'] = Y[:, :, k].flatten()
        elif k == 2:
            data['Average Daily Visitors'] = Y[:, :, k].flatten()
        elif k == 3:
            data['Average Cost of Hotel Rooms'] = Y[:, :, k].flatten()

    return pd.DataFrame(data)

# Run simulation
df = simulate(seed=10000, r=3)
config = {
    "df": df,
    "outcome": 'Gross Booking Value',
    "treat": 'Experiences',
    "unitid": 'Market',
    "time": 'Week',
    "display_graphs": True,
    "save": False,
    "counterfactual_color": ["blue"], "addout": list(df.columns[4:]),
    "method": "both"
}

arco = SCMO(config).fit()

```

## Application to Prop 99

We can apply this to Prop 99 too.

```{python}

import pandas as pd # To work with panel data

from IPython.display import display, Markdown # To create the table

from mlsynth.mlsynth import SCMO # The method of interest

url = "https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/refs/heads/master/examples/datasets/smoking_data.csv"

datap99 = pd.read_csv(url)

datap99["Proposition 99"] = ((datap99["state"] == "California") & (datap99["year"] >= 1989)).astype(int)

config = {
    "df": datap99,
    "outcome": "cigsale",
    "treat": "Proposition 99",
    "unitid": "state",
    "time": "year",
    "display_graphs": True,
    "save": False, "method": "both",
    "counterfactual_color": "red", "addout": "retprice"}

p99 = SCMO(config).fit()

```
